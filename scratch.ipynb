{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import astropy.units as u\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from astroquery.jplhorizons import Horizons\n",
    "from scipy.optimize import least_squares, minimize\n",
    "\n",
    "from jorbit import Observations, Particle\n",
    "from jorbit.astrometry.transformations import icrs_to_horizons_ecliptic\n",
    "from jorbit.utils.states import CartesianState, KeplerianState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nights = [Time(\"2025-01-01 07:00\"), Time(\"2025-01-02 07:00\"), Time(\"2025-01-05 07:00\")]\n",
    "\n",
    "times = []\n",
    "for n in nights:\n",
    "    times.extend([n + i * 1 * u.hour for i in range(3)])\n",
    "times = Time(times)\n",
    "\n",
    "\n",
    "obj = Horizons(id=\"274301\", location=\"695@399\", epochs=times.utc.jd)\n",
    "pts = obj.ephemerides(extra_precision=True, quantities=\"1\")\n",
    "\n",
    "coords = SkyCoord(pts[\"RA\"], pts[\"DEC\"], unit=(u.deg, u.deg))\n",
    "times = Time(pts[\"datetime_jd\"], format=\"jd\", scale=\"utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'RA'), Text(0, 0.5, 'DEC')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApoAAAF2CAYAAADOYjibAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIf0lEQVR4nO3deXxU5aH/8W8WCJBA2LKQBTBetoBckEaWKLuIGqm1btCCglIFgtcKRVFvoS5FKnqFErWX1aJQCBJUFFoQgyJXQHK1ehFEATMxgbBOQsg+z+8PfjNlmIEMkGMmw+f9es3rZZ7znGeeM0+PfnvOec4TZIwxAgAAAGpZcF13AAAAAIGJoAkAAABLEDQBAABgCYImAAAALEHQBAAAgCUImgAAALAEQRMAAACWIGgCAADAEqF13YErkcPhUH5+vpo2baqgoKC67g4AAICMMSouLlZcXJyCg2vnWiRBsw7k5+crMTGxrrsBAADgwWazKSEhoVbaImjWgaZNm0o6M5DNmjWr9fZTUlK0c+fOWm/XyraLioqUmJjIbxIAbTOWgdM2Yxk4bTOWgdO2lWPpbNuZU2qDXwXNqqoqrVmzRuvXr9eOHTt04sQJVVZWKj4+XiNHjtSkSZMUERHhU1vFxcWaPXu23n77bR0+fFiS1KtXLz3++OMaOnSo133sdrtmzJihrKwslZaWKjIyUr/61a80ffp0hYWFudU9ePCgrr76akVFRXlt6+WXX9aoUaO8bnPeLm/WrJklJ3xISIgl7VrdtsRvEihtS4xloLQtMZaB0rbEWAZK25J1YympVh/r86ugefToUd1zzz3q2bOnVq5cqW7duqmqqkp//etfNX78eGVlZWnr1q0KDb1wt48fP64bbrhBhw8f1vLlyzV06FCdOnVK06ZN07Bhw7R06VKNGTPGbR+73a7U1FRVVlZq06ZN6tChg3bu3Klbb71Vn376qdavX+/xvYmJiTp48GBt/wyXbdKkSfWybSvV19+kvrZtpfr6m9TXtq1UX3+T+tq2lerrb1Jf265XjB8pKCgwkkxOTo7HtjvvvNNIMps2baqxnfT0dCPJzJ071628qqrKXH311SYiIsIcOnTIbdvkyZONJLN582a38kWLFnlt68CBA6Zdu3Y+Hpk7u91uJBm73X5J+wcifpPAwVgGDsYycDCWgcPKsbSibb96vVGrVq20detW9ejRw2Nbu3btJJ258liTrKwsSdKwYcPcykNCQnTzzTfr1KlTeuutt1zlpaWlWrx4sWJjYzVo0CC3fe6++26FhIRo/vz5F3s4uAhhYWGaMWOGxyMKqH8Yy8DBWAYOxjJw1Lex9Kug2aBBA6Wmpnp9NmDHjh1q1KiRevfuXWM7zmcyvT0/GRsbK0n65JNPXGWfffaZSkpKdO2113rUj4iIUMeOHbVv3z6/vE0eKMLCwjRz5sx6c+Lg/BjLwMFYBg7GMnDUt7H0q6B5LofDoQMHDmjSpEnKycnRkiVLFB8fX+N+0dHRkv4VOM9WWFgoSTpw4ICr7JtvvpGk87btLN+zZ49b+enTp/XYY4+pa9euiomJUVJSkkaNGuXzLLOioiK3T3l5uU/7AQAAXK7y8nKPLFLb/DZorlu3Ts2bN1dSUpI2bNig5cuX69577/Vp31tuuUWS9P7777uVOxwObdy4UZJ06tQpV/nJkyclSeHh4V7bc5afOHHCrfzEiROKjY3V1q1blZ+fr7Vr18pms6lv375aunRpjf1MTExUZGSk6zNr1iyfjg8AAOByzZo1yy2HWPGOb78NmmlpaSoqKlJhYaEeffRRjRw5UrfddptbQDyfZ555RomJiXr++ee1Zs0alZeXq7CwUBMmTNCxY8cknT9U+ioxMVEFBQWaNm2aWrRooZCQEHXv3l3vvPOOIiIiNHHiRK9XVM9ms9lkt9tdn+nTp19WnwAAQGCw2+3Ky8vzui0vL8+nOSs1mT59ulsOsdlsl93mufw2aDpFRUVp8uTJevbZZ7Vu3TqfwlibNm20c+dO3XfffXriiScUHx+v/v37q3nz5nrzzTdddZyaN28uSSopKfHanrO8RYsWrrKQkBC1bt3ao27Lli01ePBglZaW6oMPPrhgP53vwHJ+6svzFgAAwDp2u13Dhw/XgAEDPMKfzWbTgAEDNHz48MsOm2FhYR5ZpLb5fdB0SktLkyStXbvWp/oxMTGaO3euvv32Wx09elR79uzR7NmzXVdEz57406VLF0nSjz/+6LUtZ3nnzp19+u64uDhJUkFBgU/1AQAAnIqLi1VYWKj9+/dr4MCBrrBps9k0cOBA7d+/X4WFhSouLq7jntbMr4Jmdna2MjMzvW5r0qSJJLlufV+q7du3S5LuuusuV1mfPn0UHh6unJwcj/qnTp3St99+qw4dOqh9+/au8qVLl5530k9+fr6kf01KAgAA8FVCQoKys7OVlJTkCpvbtm1zhcykpCRlZ2fX2nrkVvK7oDljxgw5HA6Pbc5JPOe+3igvL0/V1dVuZTt37lTv3r09yktKSvTXv/5VI0aMUM+ePV3ljRs31rhx43To0CF99NFHbvusWrVK1dXVSk9PdytfunSplixZ4tHPkydPKjs7Ww0bNtTw4cN9OGoAAAB3iYmJbmEzNTXVLWRaMXHHCn4VNKUzrxoaP36866pgRUWFVq9erSlTpqhZs2aaM2eOq+5LL72kxMRE3XHHHW5tlJSUaMeOHXr88cdVVlYmSfruu+80YsQItW7dWgsXLvT43meffVZdu3bVww8/rH379kk6E1inT5+uIUOGaOLEiR77LFiwQAsXLlRFRYUk6fvvv9edd96pkydP6sUXX6wX/08DAADUnQtN+gkKCtJrr73mVrZs2bJ6EzIlPwua6enpevXVV2Wz2ZSamqro6Gi1atVKTz/9tEaNGqUvv/xSvXr1ctVv06aNwsPDXasGOV111VUaPXq03nvvPcXGxio+Pl6//OUvNXjwYG3fvt3ri9wjIyO1detWDR8+XEOGDFF0dLRGjhypCRMm6P333/dY5/wvf/mLnn76aS1cuFBXXXWVWrZsqX79+ikiIkIffvihHnnkEWt+JAAAEBBqmvSTmpqqESNGuJWPHj3aktnhVgkyxpi67sSVpqioSJGRkbLb7ZbM8AIAAP4vLy9PAwYM8LglbrPZdP311ys3N1eS1LZtW61YsUKjR4+29Pa5FfnEr65oAgAAXCnON+nn3JC5detW9evXz6Pu+W65+xOCJgAAQB3xNuknNzdXYWFhrpDpvHJ5dt3o6Gg1bdq0jntfM4JmHUpJSVFycrIyMjLquisAAKCOJCYmatmyZW5l7777rj799FOP2+OJiYnasmWLNmzYoMjIyFr5/oyMDCUnJyslJaVW2jsbz2jWAZ7RBADgymG321VcXOz1bTTO5SRHjBih/fv3u8rr4jVGPKMJAABQj/gys7xXr16uST6ffvqp23OY9WmGuTcETQAAAItcaDlJ56Sf8vJytW3bVtnZ2fV20s/5EDQBAAAscqHlJANp0s/58IxmHeAZTQAAriw2m821VrlTUlKS3n33XUVGRp73+c2mTZvW2qSfmvCMJgAAQD3kbWb5smXL1LVr1/MuWZ2QkPCThUyrEDQBAAAsZrPZNHr0aLey+rac5KUgaAIAAFjo7NvmgTiz/EIImgAAABbJy8tzC5mBOLP8QgiadYiVgQAACGxNmzZVdHS0xwvY/WlmOSsDBRhmnQMAcOWoaWWgn3Jm+YVYkU9Ca6UVAAAAeBUZGXneIHm+GeeBglvnAAAAsARBEwAAAJYgaAIAAMASBE0AAABYgqAJAAAASxA0AQAAYAmCJgAAACxB0AQAAIAlCJp1iCUoAQBAXWMJygDDEpQAAMDfWJFPuKIJAAAASxA0AQAAYAmCJgAAACxB0AQAAIAlCJoAAACwBEETAAAAliBoAgAAwBIETQAAAFiCoAkAAABLEDTrEEtQAgCAusYSlAGGJSgBAIC/YQlKAAAA1BsETQAAAFjCr4JmVVWVVq1apbFjx6pr166Ki4tTVFSUevToodmzZ+vUqVM+t1VcXKynn35aXbp0UcuWLdWyZUvdeOON2rRp03n3sdvtevTRR9WuXTtFR0erQ4cOmjlzpsrLy8+7z7Jly5SSkqLo6GjFxsbqrrvu0rfffntRxw0AABCI/CpoHj16VPfcc4++/PJLrVy5Uvn5+SooKNAjjzyiJ598UkOHDlVVVVWN7Rw/flx9+vTR66+/rrlz5+ro0aM6ePCgrr76ag0bNkx//etfPfax2+1KTU3V+vXrtWnTJhUWFmr58uV69dVXlZaW5vV7n376ad1///2aNGmSDh8+rD179qiqqkopKSn65z//WSu/CQAAQL1l/EhBQYGRZHJycjy23XnnnUaS2bRpU43tpKenG0lm7ty5buVVVVXm6quvNhEREebQoUNu2yZPnmwkmc2bN7uVL1q0yGtbn3/+uQkKCjJjxoxxKz958qRp2rSp6dWrl3E4HF77Z7fbjSRjt9trPBYAAICfghX5xK+uaLZq1Upbt25Vjx49PLa1a9dO0pkrjzXJysqSJA0bNsytPCQkRDfffLNOnTqlt956y1VeWlqqxYsXKzY2VoMGDXLb5+6771ZISIjmz5/vVp6RkSFjjEaOHOlWHhkZqZtvvlm7du3Stm3bauwrAABAoPKroNmgQQOlpqYqKCjIY9uOHTvUqFEj9e7du8Z2Dh8+LEmKiory2BYbGytJ+uSTT1xln332mUpKSnTttdd61I+IiFDHjh21b98+HTx40FX+4YcfSpJ69erlsY+zbOPGjTX2FQAAIFD5VdA8l8Ph0IEDBzRp0iTl5ORoyZIlio+Pr3G/6OhoSf8KnGcrLCyUJB04cMBV9s0330jSedt2lu/Zs0eSdPr0aeXm5qphw4Zew+y59c+nqKjI7XOhSUcAAAC1qby83COL1Da/DZrr1q1T8+bNlZSUpA0bNmj58uW69957fdr3lltukSS9//77buUOh8N1lfHsGewnT56UJIWHh3ttz1l+4sQJt/pNmjTxqf75JCYmKjIy0vWZNWvWBesDAADUllmzZrnlkMTExFr/Dr8NmmlpaSoqKlJhYaEeffRRjRw5UrfddptPrzh65plnlJiYqOeff15r1qxReXm5CgsLNWHCBB07dkzS+UPlT8lms8lut7s+06dPr+suAQCAK8T06dPdcojNZqv17/DboOkUFRWlyZMn69lnn9W6det8CmNt2rTRzp07dd999+mJJ55QfHy8+vfvr+bNm+vNN9901XFq3ry5JKmkpMRre87yFi1auNU/ffq0T/XPp1mzZm6fsLCwGo8NAACgNoSFhXlkkdrm90HTKS0tTZK0du1an+rHxMRo7ty5+vbbb3X06FHt2bPH7aXvZ0/86dKliyTpxx9/9NqWs7xz586Sztwyb9u2rSoqKnTkyJEa6wMAAFyJ/CpoZmdnKzMz0+s25/OQzlvfl2r79u2SpLvuustV1qdPH4WHhysnJ8ej/qlTp/Ttt9+qQ4cOat++vat8yJAhkqRdu3Z57OMsu/HGGy+rrwAAAPWZ3wXNGTNmyOFweGxzTuI59/VGeXl5qq6udivbuXOnevfu7VFeUlKiv/71rxoxYoR69uzpKm/cuLHGjRunQ4cO6aOPPnLbZ9WqVaqurlZ6erpb+aRJkxQUFKQVK1a4ldvtdq1fv17XXnut+vXr5+ORAwAABB6/CprSmVcNjR8/Xvn5+ZKkiooKrV69WlOmTFGzZs00Z84cV92XXnpJiYmJuuOOO9zaKCkp0Y4dO/T444+rrKxMkvTdd99pxIgRat26tRYuXOjxvc8++6y6du2qhx9+WPv27ZN0JrBOnz5dQ4YM0cSJE93q9+rVS08++aTefPNNvfHGGzLGyG636/7775ckLV682Ov7QAEAAK4UfhU009PT9eqrr8pmsyk1NVXR0dFq1aqVnn76aY0aNUpffvml2wvS27Rpo/DwcNeqQU5XXXWVRo8erffee0+xsbGKj4/XL3/5Sw0ePFjbt2/3+u7LyMhIbd26VcOHD9eQIUMUHR2tkSNHasKECXr//fcVGhrqsc9zzz2nJUuWaN68eYqJiVHHjh0VEhKinTt36t///d9r/wcCAACoR4KMMaauO3GlKSoqUmRkpOx2uyUzvAAAAC6WFfnEr65oAgAAIHAQNAEAAGAJgmYdSklJUXJysjIyMuq6KwAA4AqVkZGh5ORkpaSk1HrbPKNZB3hGEwAA+Bue0QQAAEC9QdAEAACAJQiaAAAAsARBEwAAAJYgaAIAAMASBE0AAABYgqAJAAAASxA0AQAAYAmCZh1iZSAAAFDXWBkowLAyEAAA8DesDAQAAIB6g6AJAAAASxA0AQAAYAmCJgAAACxB0AQAAIAlCJoAAACwBEETAAAAliBoAgAAwBIETQAAAFiCoFmHWIISAADUNZagDDAsQQkAAPwNS1ACAACg3iBoAgAAwBIETQAAAFiCoAkAAABLEDQBAABgCYImAAAALEHQBAAAgCUImgAAALAEQRMAAACWIGjWIZagBAAAdY0lKAMMS1ACAAB/wxKUAAAAqDcImgAAALCEXwXNqqoqrVq1SmPHjlXXrl0VFxenqKgo9ejRQ7Nnz9apU6d8bstut+sPf/iDrrnmGsXExCguLk6pqal666235HA4vO6zZs0aDR48WC1btlSLFi2UnJysP/7xjyovL/eoe/DgQYWEhCg2NtbrZ/ny5Zf8OwAAAAQE40cKCgqMJNOzZ0/z1VdfGWOMqaysNIsWLTLBwcGmd+/eprKyssZ2SktLTbdu3Uzjxo3Ne++9ZxwOhykvLzfPPPOMkWQmTpzosc8TTzxhJJlp06aZ4uJiU1VVZbKyskxERIQZNmyYqaqqcqt/4MAB065du0s6TrvdbiQZu91+SfsDAADUNivyiV9d0XRatGiRunXrJkkKDQ3VuHHjdMcdd2j79u3asmVLjfuvWbNGX3/9tR566CGlpaUpKChIDRs21H/+53+qR48eeu2111RQUOCqv2vXLr3wwgvq2bOnZs+erYiICIWEhOj222/X1KlT9Y9//EN/+ctfLDteAACAQORXQbNVq1baunWrevTo4bGtXbt2ks7cEq+JzWaTJHXs2NFjW4cOHWSMUW5urqssKytLkjRs2DCP+mlpaZKk119/veYDAAAAgItfBc0GDRooNTVVQUFBHtt27NihRo0aqXfv3jW20717d0nS7t27Pbbt2bNHYWFhbiH08OHDkqSoqCiP+rGxsZKkr776yqeQCwAAgDP8Kmiey+Fw6MCBA5o0aZJycnK0ZMkSxcfH17jfzTffrLFjx2rhwoXKzMxURUWFioqK9NRTT2n37t16+eWX1aJFC1f96OhoSf8KnGcrLCx0/fOBAwfctp0+fVqPPfaYunbtqpiYGCUlJWnUqFHauXOnT8dXVFTk9vE26QgAAMAK5eXlHlmktvlt0Fy3bp2aN2+upKQkbdiwQcuXL9e9997r8/4LFizQzJkzNW7cOEVERKh58+bKzMzUunXrNHHiRLe6t9xyiyRp/fr1Mue8v37Dhg2ufz531vuJEycUGxurrVu3Kj8/X2vXrpXNZlPfvn21dOnSGvuYmJioyMhI12fWrFk+Hx8AAMDlmDVrllsOSUxMrP0vqbVpRRYpLCw08+bNM02aNDFpaWmmuLjYp3369etnWrVqZbKyskxpaamx2+1m3rx5pmHDhubxxx/32OfBBx90zUgvLCw0paWlJjMz00RFRZnIyEgjyeTk5LjqV1VVmSNHjni0c+zYMRMZGWkaN25sDh065LV/zlldNpvN2O1216esrOwifhkAAIBLV1ZW5pZDbDZbrc869/ug6fTSSy8ZSSY9Pb3GumPGjDGSzOLFiz22TZgwwUgyS5YscSt3OBxmwYIFpm/fvqZVq1YmISHB3H777ebzzz83Xbp0MZJMQUGBT339xS9+cd7vN4bXGwEAAP9zxbzeyBvn7O+1a9fWWNd5u3vIkCEe25xla9ascSsPCgrSgw8+qG3btuno0aOy2WzKyspSr1699OOPP6pNmzauiUE1iYuLkyS3VygBAABcafwqaGZnZyszM9PrtiZNmkiSjh07VmM7zmcpvc1ed5b5OoN8z549Kioq0l133eVWvnTp0vNO+snPz5f0r0lGAAAAVyK/C5ozZszwukTkxo0bJcnj9UZ5eXmqrq52K+vTp48keX25+8cff+xWx+mmm25ym/jj9OqrryoiIkJTp051K1+6dKmWLFniUf/kyZPKzs5Ww4YNNXz4cI/tAAAAVwq/CpqS9M0332j8+PGuq4IVFRVavXq1pkyZombNmmnOnDmuui+99JISExN1xx13uLUxe/ZshYeHa9q0adq8ebMcDocqKyv1xhtv6LXXXlPbtm01ZcoUt3327t2radOmaf/+/ZKkkpISzZ49WwsWLNBbb73ldSbWggULtHDhQlVUVEiSvv/+e9155506efKkXnzxRSUkJNTqbwMAAFCfhNZ1B86Wnp6umJgYZWVlKTU1VSUlJSotLVV8fLxGjRqlqVOnqn379q76bdq0UXh4uGvVIKef/exnysnJ0QsvvKBx48bpxIkTcjgcSkhIUHp6uqZPn67WrVu77fPAAw/ogw8+UO/evRUcHKwmTZqof//+ysnJUZcuXTz6+pe//EV/+9vftHDhQs2YMUOlpaVq0KCB+vbtqw8//FCDBg2y5DcCAACoL4KMOefFkbBcUVGRIiMjZbfb1axZs7ruDgAAgCX5xO9unQMAACAwEDQBAABgCYJmHUpJSVFycrIyMjLquisAAOAKlZGRoeTkZKWkpNR62zyjWQd4RhMAAPgbntEEAABAvUHQBAAAgCUImgAAALAEQRMAAACWIGgCAADAEgRNAAAAWIKgCQAAAEsQNAEAAGCJiwqaP/74o3Jzc5Wbm6uKigqvdT755BPxDnjfsDIQAACoa36xMlB+fr7atm3rCpHZ2dm64YYbPOqlpKSorKxMH3zwgRITE2u3twGClYEAAIC/qdOVgd599105HA4NGzZMW7du9RoyJWnq1KkqLS3V0KFDderUqVrpJAAAAOofn4Nmdna2br31Vn3wwQfq27fveevdc8892rlzp5o0aaIlS5bUSicBAABQ//gcNL/88kvNmDFDQUFBNdZt0aKF/vSnP+lvf/vbZXUOAAAA9ZfPQTM/P189e/b0ueFBgwYpNzf3kjoFAACA+s/noBkeHq6QkBCfGw4NDVV1dfUldQoAAAD1n89Bs1GjRiopKfG54ZKSEjVo0OCSOgUAAID6z+eg2bdvX61atcrnhleuXKl+/fpdUqcAAABQ/4X6WvGBBx7Q3XffrZ49e6pHjx4XrJuTk6PHH39cq1evvtz+AQAAoJ7y+Yrm4MGDNXz4cPXp00cTJkzQxo0bVVhYqKqqKlVVVamwsFAbN27Uww8/rH79+mnEiBEaMGCAlX0HAACAH/N5ZSBJKi8v1+jRo7V69erzvubIGKPRo0dr0aJFCg31+YLpFcX55v2OHTsqJCREkyZN0qRJk+q6WwAA4AqUkZGhjIwMVVdX69tvv63VlYEuKmg6ZWVlKSMjQ59++qnKy8slSWFhYerfv78eeeQR3XrrrbXSuUDFEpQAAMDfWJFPLiloOjkcDh07dkxBQUFq1aqVTy9zB0ETAAD4HyvyyWXd2w4ODlZUVFStdAQAAACBxeegefYqP23btr1g3U8++UTOC6X9+/e/xK4BAACgPvM5aLZv316SFBQUpMrKSgUHn3/C+rhx41RVVaXc3FxWBwIAALhC+Rw0Y2JiVFBQ4Pr7448/9qjjvHq5b98+SVLjxo0vt38AAACop3wOmudO9LnvvvskSXl5eUpISFBQUJD2799/wX0AAABw5bjkyUAHDhyQJLVp08b1zwAAAICTzysDnQ9XLQEAAODNZQdNXLqUlBQlJycrIyOjrrsCAACuUBkZGUpOTlZKSkqtt+3zC9vj4uKUn5/vc7kkNWnSRKdPn768HgYgXtgOAAD8TZ2+sP306dNatmyZzs2lpaWlXssl8WojAACAK5jPVzSDg4O9Po9pjLlgOWHTE1c0AQCAv6nTK5rh4eGaOnWqzw0bY/THP/7xkjoFAACA+s/nK5pt2rRxe2G7Lxo3bqzS0lKf61dVVWnNmjVav369duzYoRMnTqiyslLx8fEaOXKkJk2apIiICJ/astvteuWVV7R69WoVFhYqJCREV111lSZOnKiRI0d6XdlozZo1mj9/vr744gsZY9SmTRv9+te/1pQpUxQWFub1e5YtW6Z58+bphx9+UHBwsG644QY9//zz6tix43n7xhVNAADgbyzJJ8ZHGzZs8LWqy3vvvXdR9QsKCowk07NnT/PVV18ZY4yprKw0ixYtMsHBwaZ3796msrKyxnZKS0tNt27dTOPGjc17771nHA6HKS8vN88884yRZCZOnOixzxNPPGEkmWnTppni4mJTVVVlsrKyTEREhBk2bJipqqry2Oepp54ywcHBZsmSJcbhcJgTJ06Y22+/3TRr1sx8+eWX5+2f3W43kozdbr+IXwcAAMA6VuQTn4PmufLz883y5cvNnDlzzEsvvWRWrFhh8vPzL6szzqCZk5Pjse3OO+80ksymTZtqbOett94yksyjjz7qsa1Hjx4mKCjIra+ff/65K+Cea+bMmUaSycjIcCv//PPPTVBQkBkzZoxb+cmTJ03Tpk1Nr169jMPh8No/giYAAPA3VuSTi36PZn5+vu666y4lJCTo17/+taZNm6bf/e53+tWvfqWEhATdfffd+vHHHy/p6mqrVq20detW9ejRw2Nbu3btJJ25JV4Tm80mSV5vX3fo0EHGGOXm5rrKsrKyJEnDhg3zqJ+WliZJev31193KMzIyZIzRyJEj3cojIyN18803a9euXdq2bVuNfQUAAAhUFxU09+/frz59+ujtt9+WMUZt27bVddddp5SUFMXHx8sYo9WrV6tPnz4e6577okGDBkpNTfU6i33Hjh1q1KiRevfuXWM73bt3lyTt3r3bY9uePXsUFhbmFkIPHz4sSYqKivKoHxsbK0n66quv3ELuhx9+KEnq1auXxz7Oso0bN9bYVwAAgEDlc9A0xujee+/VoUOH9MQTT8hms+nAgQP6n//5H3322WfKzc3VwYMHNWXKFB06dEj33HPPZXfO4XDowIEDmjRpknJycrRkyRLFx8fXuN/NN9+ssWPHauHChcrMzFRFRYWKior01FNPaffu3Xr55ZfVokULV/3o6GhJ/wqcZyssLHT9s3NN99OnTys3N1cNGzb0Gk6dfdyzZ88F+1lUVOT2KS8vr/HYAAAAakN5eblHFqltPgfNdevW6YsvvtD777+vP/7xj14DX9u2bfXiiy/qnXfe0Zdffql33333kju2bt06NW/eXElJSdqwYYOWL1+ue++91+f9FyxYoJkzZ2rcuHGKiIhQ8+bNlZmZqXXr1mnixIludW+55RZJ0vr16z1ePL9hwwbXP586dUqSdPLkSUlnVj7yJjw8XJJ04sSJC/YxMTFRkZGRrs+sWbN8Pj4AAIDLMWvWLLcckpiYWOvf4XPQXLlypdLT03XjjTfWWPeWW27RhAkTtGrVqkvuWFpamoqKilRYWKhHH31UI0eO1G233eYKexdy5MgR9e/fXy+++KKWLVumoqIinTx5UpMnT9bPf/5zPfHEE271U1NT9eCDD+rrr79Wenq6jhw5orKyMq1evVr/9V//pcjISEn/CpC1xWazyW63uz7Tp0+v1fYBAADOZ/r06W45xDnHpTb5/ML2zz//XG+//bbPDY8fP1533HHHJXXqbFFRUZo8ebIqKys1ZcoUTZ8+XX/+858vuM/UqVO1bds2LV68WLfffrskqVGjRpo8ebK++eYbzZ49W507d9b999/v2ue///u/1bt3by1evFhdunRR48aN9bOf/Uzr16/X6NGjZbfb1aZNG0lS8+bNJem867iXlJRIktvteW+aNWvGezQBAECdCAsLO+97wmuLz1c0jx8/rq5du/rccLdu3Wq8dXwxnLO/165dW2Nd5+3uIUOGeGxzlq1Zs8atPCgoSA8++KC2bdumo0ePymazKSsrS7169dKPP/6oNm3auCYGNWnSRG3btlVFRYWOHDni8R3OWfedO3f2/QABAAACjM9BMyQk5KIbDw31+YKpJCk7O1uZmZletzmfhzx27FiN7Thvr3ubve4s8+U1SdKZCT1FRUW666673MqdgXXXrl0e+zjLfHnMAAAAIFD5HDS9hbba3ic7O1szZsyQw+Hw2OZ8VdC5rzfKy8tTdXW1W1mfPn0kSVu2bPFo5+OPP3ar43TTTTe5TfxxevXVVxUREeGxzvukSZMUFBSkFStWuJXb7XatX79e1157rfr16+f1OAEAAK4EPl9yPHbsmAYPHnxRjR8/fvyiO/TNN99o/PjxevbZZxUXF6eKigq9++67mjJlipo1a6Y5c+a46r700kuaOnWqRowYoXfeecdVPnv2bA0cOFDTpk1TXFycBg4cqOrqai1fvlyvvfaa2rZtqylTprh97969ezVt2jR17NhRSUlJKikp0fz587VgwQKtXLnSYyZWr1699OSTT2rWrFkaPHiwxowZo6KiItdzn4sXL76kcA4AABAofA6alZWVys7OvqjGLzZopaenKyYmRllZWUpNTVVJSYlKS0sVHx+vUaNGaerUqWrfvr2rfps2bRQeHu5aNcjpZz/7mXJycvTCCy9o3LhxOnHihBwOhxISEpSenq7p06erdevWbvs88MAD+uCDD9S7d28FBwerSZMm6t+/v3JyctSlSxev/X3uuefUsWNHzZ07V7/73e8UFBSkG264QTt37lSnTp0u6tgBAAACTZA598WR5xEdHa2dO3f63LAxRr179/b6EvQrXVFRkSIjI2W325l1DgAA/IIV+cTnK5otW7b0uHJYk5pe7wMAAIDA5fNkoJqWU6ytfQAAABAYfA6aqH0pKSlKTk5WRkZGXXcFAABcoTIyMpScnKyUlJRab9vnZzRRe3hGEwAA+Bsr8glXNAEAAGAJgiYAAAAsQdAEAACAJQiaAAAAsARBEwAAAJYgaAIAAMASBE0AAABYgqAJAAAASxA06xArAwEAgLrGykABhpWBAACAv2FlIAAAANQbBE0AAABYgqAJAAAASxA0AQAAYAmCJgAAACxB0AQAAIAlCJoAAACwBEETAAAAliBoAgAAwBIEzTrEEpQAAKCusQRlgGEJSgAA4G9YghIAAAD1BkETAAAAliBoAgAAwBIETQAAAFiCoAkAAABLEDQBAABgCYImAAAALEHQBAAAgCUImnWIlYEAAEBdY2WgAMPKQAAAwN+wMhAAAADqDYImAAAALOFXQbOqqkqrVq3S2LFj1bVrV8XFxSkqKko9evTQ7NmzderUKZ/bstvt+sMf/qBrrrlGMTExiouLU2pqqt566y05HA6v+7zzzjsaPHiwEhISFBMTo+TkZE2dOlVHjhzxqJudna2GDRsqNjbW6+eTTz655N8BAAAgEPhV0Dx69Kjuueceffnll1q5cqXy8/NVUFCgRx55RE8++aSGDh2qqqqqGtspKyvT9ddfr9mzZ2vWrFk6dOiQDh48qOHDh+vXv/61Jk+e7LHP888/r9tvv10dO3bUnj17dPjwYS1cuFBvvvmmevXqpWPHjnns069fPx06dMjr54YbbqiV3wQAAKC+8qug6bRo0SJ169ZNkhQaGqpx48bpjjvu0Pbt27Vly5Ya91+zZo2+/vprPfTQQ0pLS1NQUJAaNmyo//zP/1SPHj302muvqaCgwFW/oqJCs2bNUlRUlP785z8rIiJC0pkgOX36dNlsNi1YsMCagwUAAAhQfhU0W7Vqpa1bt6pHjx4e29q1ayfpzC3xmthsNklSx44dPbZ16NBBxhjl5ua6yk6cOKGSkhJdddVVatCggUd9Sfrhhx98Pg4AAAD4WdBs0KCBUlNTFRQU5LFtx44datSokXr37l1jO927d5ck7d6922Pbnj17FBYW5hZCo6OjFRMTo++//14VFRVu9b/55htJ0jXXXHNRxwIAAHCl86ugeS6Hw6EDBw5o0qRJysnJ0ZIlSxQfH1/jfjfffLPGjh2rhQsXKjMzUxUVFSoqKtJTTz2l3bt36+WXX1aLFi1c9YOCgrRo0SKVl5froYce0pEjR1RVVaVNmzbphRde0MCBA/XAAw94fE9hYaHGjx+vzp07Kzo6Wp06ddL48eO1d+/eWv0dAAAA6qPQuu7A+axbt06jRo1ScXGxkpKStHz5co0YMcLn/RcsWKBOnTpp3LhxKi8vV1VVlf7t3/5N69at0/Dhwz3q33rrrdqwYYMmTpyo6OhoNWzYUMHBwfrtb3+rP/zhDx631KUzt+h79+6tV155RQ0bNtTOnTv1m9/8Rj179tQ777yjG2+88YJ9LCoqcvs7LCxMYWFhPh8jAADApSovL1d5ebnr73NzSa0wfq6wsNDMmzfPNGnSxKSlpZni4mKf9unXr59p1aqVycrKMqWlpcZut5t58+aZhg0bmscff9xjn2eeecaEhISY3/72t6awsNBUVlaarVu3mo4dO5rrrrvO/PDDD271y8rKzPHjxz3a2bNnjwkJCTFt2rQxZWVlXvtnt9uNJI/PjBkzfPtRAAAALtOMGTO85hG73V5r3+H3QdPppZdeMpJMenp6jXXHjBljJJnFixd7bJswYYKRZJYsWeIq27x5s5Fk+vfv71F/+/btRpIZMGCAz33t2bOnkWQ2b97sdbszaNpsNmO3212f8wVTAACA2lZWVuaWQ2w2W60HTb9+RvNsaWlpkqS1a9fWWHfDhg2SpCFDhnhsc5atWbPGp/rXXXedIiIitGXLFh0/ftynvsbFxUmS2yuUvGnWrJnbh9vmAADgpxIWFuaRRWqbXwXN7OxsZWZmet3WpEkTSfL64vRzOVcQ8jZ73Vl29muSLlRfkoKDgz32eeWVV7Rv3z6v9fPz8yWdmc0OAABwpfK7oDljxgyvS0Ru3LhRkjxeb5SXl6fq6mq3sj59+kiS15e7f/zxx251aqr/xRdfqKioSLGxsa53eUpngmZWVpZH/e+++05ff/21WrVqpX79+nk/UAAAgCuAXwVN6cx7K8ePH++6KlhRUaHVq1drypQpatasmebMmeOq+9JLLykxMVF33HGHWxuzZ89WeHi4pk2bps2bN8vhcKiyslJvvPGGXnvtNbVt21ZTpkxx1R85cqRSU1P14YcfaubMmSopKZEkffXVV7rvvvsUFBSkl19+2XVl0+n555/X2rVrVV1dLWOMvvjiC911110yxui///u/XVdhAQAArkRBxhhT151wOnr0qDIzM5WVlaV9+/appKREpaWlio+P19ChQzV16lS1b9/eVX/58uX6zW9+o3HjxmnevHlubX377bd64YUXtHnzZp04cUIOh0MJCQm65ZZbNH36dLVu3dqtfllZmebOnatVq1Zp3759CgkJUVhYmPr27avHHnvMY+3ynJwcrVy5Uhs3blRBQYHKysoUHh6u/v3763e/+5169ux53uMsKipSZGSk7Ha7Jc9DAAAAXCwr8olfBc0rBUETAAD4Gyvyid/dOgcAAEBgIGgCAADAEgRNAAAAWIKgWYdSUlKUnJysjIyMuu4KAAC4QmVkZCg5OVkpKSm13jaTgeoAk4EAAIC/YTIQAAAA6g2CJgAAACxB0AQAAIAlCJoAAACwBEETAAAAliBoAgAAwBIETQAAAFiCoAkAAABLEDTrECsDAQCAusbKQAGGlYEAAIC/YWUgAAAA1BsETQAAAFiCoAkAAABLEDQBAABgCYImAAAALEHQBAAAgCUImgAAALAEQRMAAACWIGgCAADAEgTNOsQSlAAAoK6xBGWAYQlKAADgb1iCEgAAAPUGQRMAAACWIGgCAADAEgRNAAAAWIKgCQAAAEsQNAEAAGAJgiYAAAAsQdAEAACAJQiadYiVgQAAQF1jZaAAw8pAAADA37AyEAAAAOoNgiZQx+x2u/Ly8rxuy8vLk91u/4l7BABA7fCroFlVVaVVq1Zp7Nix6tq1q+Li4hQVFaUePXpo9uzZOnXqlM9t2e12/eEPf9A111yjmJgYxcXFKTU1VW+99ZYcDofXfd555x0NHjxYCQkJiomJUXJysqZOnaojR454re9wODR37lx169ZN0dHRiouL07hx41RQUHBJx48rj91u1/DhwzVgwADZbDa3bTabTQMGDNDw4cMJmwCA+sn4kYKCAiPJ9OzZ03z11VfGGGMqKyvNokWLTHBwsOndu7eprKyssZ3S0lLTrVs307hxY/Pee+8Zh8NhysvLzTPPPGMkmYkTJ3rs89xzzxlJ5qGHHjLFxcXGGGM+/fRTExMTYxITE83Ro0c99vn1r39tmjRpYj744ANjjDH5+fmmb9++Jj4+3thstvP2z263G0nGbrf79LsgcJw8edLtfxs2m80kJSUZSaZt27bm66+/NsYYk5ub6ypPSkq64P+eAACoDVbkE78Mmjk5OR7b7rzzTiPJbNq0qcZ23nrrLSPJPProox7bevToYYKCgkx+fr6rrLy83ISHh5uoqChTUVHhVv+VV14xksysWbPcyt9++20jyfz+9793K9+3b58JCgoyv/jFL87bP4LmleHcUHny5EnTp08fk5SUZLZv325OnjxpjDkTKtu2bWskmbCwMPP3v//dLWTm5ubW1SEAAK4gVuQTv7p13qpVK23dulU9evTw2NauXTtJ8ukWovMWZMeOHT22dejQQcYY5ebmuspOnDihkpISXXXVVWrQoIFHfUn64Ycf3Mrnz58vSRo5cqRb+b/9278pJSVFa9euPe9zdwh83m6JFxcXq7CwUPv371dqaqoGDRrk8b/n8vJy3XTTTdq/f7+SkpKUnZ2txMTEujgEAAAum18FzQYNGig1NVVBQUEe23bs2KFGjRqpd+/eNbbTvXt3SdLu3bs9tu3Zs0dhYWFuITQ6OloxMTH6/vvvVVFR4Vb/m2++kSRdc801rrKKigpt3bpV4eHh6tSpk8d39OrVS8YYbdq0qca+IjCdHSoHDhwom82mhIQELV++XKGhoaqqqtJXX32ljz76SAMHDlRubq7i4uLc2li2bBkhEwBQr/lV0DyXw+HQgQMHNGnSJOXk5GjJkiWKj4+vcb+bb75ZY8eO1cKFC5WZmamKigoVFRXpqaee0u7du/Xyyy+rRYsWrvpBQUFatGiRysvL9dBDD+nIkSOqqqrSpk2b9MILL2jgwIF64IEHXPW/++47VVZWKi4uzmsodvZxz549tfAroD5KSEhQdna2kpKSXGFz27ZtGjVqlKqqqlxh8xe/+IX279+vtm3bKjQ01K2N0aNHe0wQAgCgPvHboLlu3To1b95cSUlJ2rBhg5YvX657773X5/0XLFigmTNnaty4cYqIiFDz5s2VmZmpdevWaeLEiR71b731Vm3YsEE5OTmKjo5WeHi4brvtNo0fP17/+Mc/FBYW5qp78uRJSVJ4eLjX73aWnzhx4oJ9LCoqcvuUl5f7fHzwf4mJiW5hMzU11XVLfNWqVW51q6qqlJubq6SkJH366aduAZWwCQCwQnl5uUcWqW1+GzTT0tJUVFSkwsJCPfrooxo5cqRuu+02n15xdOTIEfXv318vvviili1bpqKiIp08eVKTJ0/Wz3/+cz3xxBMe+zz77LMaMGCAhgwZosLCQpWUlGjTpk16++23df3117s901lbEhMTFRkZ6frMmjWr1r8DdSsxMVHLli1zK5szZ46mTp3qVpafn6+2bdsqOztb/fr187gayvO+AIDaNmvWLLccYsnjWrU2rchiL730kpFk0tPTa6w7ZswYI8ksXrzYY9uECROMJLNkyRJX2ebNm40k079/f4/627dvN5LMgAEDXGX/93//ZySZDh06eP1+56uSHn/8ca/bnbO6bDabsdvtrk9ZWVmNx4b65ezXFDk/oaGhrhnlf//7301YWJjr9UZnzzB37tunTx/XDHUAAGpLWVmZWw6x2WyBPev8QtLS0iRJa9eurbHuhg0bJElDhgzx2OYsW7NmjU/1r7vuOkVERGjLli06fvy4pDMzyxs0aKD8/HwZL0vF//jjj5Kkzp07X7CfzZo1c/ucfXse9Z/NZtPAgQNdt8uzsrJcz2aGhoZq+fLlGjZsmHbt2qW2bdsqNzfX7eplYmKitmzZog0bNigyMrKOjwYAEGjCwsI8skht86ugmZ2drczMTK/bmjRpIkk6duxYje04b697m6jjLDv7tTIXqi9JwcHBbvs0bNhQ119/vUpKSrR3716P+rt27VJQUJCGDh1aY18RmPLy8txCZnZ2tgYNGqTu3bu7wuaoUaOUl5enrl27auvWrUpKSlJ0dLSaNm3qaichIYGQCQCot/wuaM6YMcPrEpEbN26UJI/XG+Xl5am6utqtrE+fPpKkLVu2eLTz8ccfu9Wpqf4XX3yhoqIixcbGut7lKUnp6emSpBUrVrjV/+6777Rz507dfvvtSkhIOM+RItA1bdpU0dHRbu/CjIyM1ObNm12Tfc4OlVy9BAAEpFq7CV8LZsyYYSSZcePGmR9//NEYc2bVnszMTNOiRQvTrFkz8/nnn7vqz5kzx0gyI0aMcGtn586dJjw83LRp08Z8+OGHprq62lRUVJilS5eahg0bmrZt25rDhw+76ldWVprU1FQjycyYMcOcOnXKGGPMP//5T9O9e3cTFBRkli9f7tHfX/3qV6ZJkyZm/fr1xpgzKxuxBCWczl0Z6Gw2m43nLgEAfsWKfBJkjJeHDOvI0aNHlZmZqaysLO3bt08lJSUqLS1VfHy8hg4dqqlTp6p9+/au+suXL9dvfvMbjRs3TvPmzXNr69tvv9ULL7ygzZs368SJE3I4HEpISNAtt9yi6dOnq3Xr1m71y8rKNHfuXK1atUr79u1TSEiIwsLC1LdvXz322GO64YYbPPrrcDg0b948LViwQEeOHFFoaKhuuukmPf/88x4v3z5bUVGRIiMjZbfbLXkeAgAA4GJZkU/8KmheKQiaAADA31iRT/zqGU0AAAAEDoImAAAALEHQBAAAgCUImnUoJSVFycnJysjIqOuuAACAK1RGRoaSk5OVkpJS620zGagOMBkIAAD4GyYDAQAAoN4gaAIAAMASBM0AZ7fblZeX5/XvvLw8tzXfz/0bAADgchA0A4i3UDl8+HANGDBAO3bsUG5uruvv7du3a8CAARo+fLjsdrtsNpvb3wAAAJcrtK47gNrhDJWFhYXKzs5WYmKiiouLVVhYqP379ys1NVWdOnVSSUmJDh48qOuvv15VVVWSpL1792rkyJHav3+/JKm4uFiRkZF1eTgAACAAcEUzQJwdKgcOHCibzaaEhAQtX75coaGhqqqq0t69e/XUU0+5/g4NDdWcOXNcITMpKUnZ2dlKSEio68MBAAABgNcb1QGrXm9ks9k0cOBAV2hctmyZRo8erf3797vCpdO5fztDZmJiYq31BwAA1B+83ggXlJiYqOzsbCUlJblulztD56pVq9zqzp8/3+3vZcuWETIBAECtImjWIStWBkpMTNSyZcvcyubMmaOpU6e6laWnp7v9PXr0aNlstlrrBwAAqB9YGSjAWLky0Nm3z52ct8mTkpI0Z84c3X333a5nNFetWqWpU6e6PaPJlU0AAK483DrHBZ37jGZWVpbbxJ9XXnlFU6dOdf1dVVWlqVOnasWKFa7b7QMHDnR7RRIAAMClImgGiLy8PLeQmZ2drUGDBql79+6uUPnII4+oefPmSkpK0tatW5WUlKTo6Gh16tTJ9WxndHS0mjZtWteHAwAAAgDv0QwQTZs2VXR0tCS53f7evHmz6z2Z0dHRWrlypYKDg5WQkKAtW7aoadOmioyMVGRkpNvfAAAAl4tnNOuAVc9o2u12FRcXe30PZl5eHiESAACclxX5hCuaAcR5ZdIbXsIOAAB+ajyjCQAAAEsQNAEAAGAJgib8Qnl5uWbOnKny8vK67gouE2MZOBjLwMFYBo76NpZMBqoDVr6wvb7iNwkcjGXgYCwDB2MZOKwcS17YHmCsWIJSUq2391O1baX6+pvU17atVF9/k/ratpXq629SX9u2Un39Tepr27XNyiUoZfCTs9vtRpKx2+2WtN+lSxdL2rWybX6TwGmbsQycthnLwGmbsQyctq0cSyva5vVGdcD8/6cVioqKLGm/urq63rXtbLO+9Zu2PTGWgdM2Yxk4bTOWgdO2lWPpbNPU4lOVPKNZB/Ly8lwr9wAAAPgTm81Wa+/fJmjWAYfDofz8fDVt2lRBQUF13R0AAAAZY1RcXKy4uDgFB9fONB6CJgAAACzBrHMAAABYgqAJAAAASxA04eHrr79Wv379FBQUpIMHD3qts3TpUjVu3FixsbFeP+fud+zYMb322mv6+c9/rg4dOig2NlZt2rRR//79tWLFioua4da+fXu1bNnS6/e2bt1aQUFBevHFFz32++yzzzR8+HBFR0crOjpa/fv31/r16y/mp6l3/H0snfbu3av7779fSUlJrvHp27evnnvuOY+67du3V1RUlNe+pqenX/R31xeBOJYS56U/jmV2drYaNmx43u/+5JNPPPbhvAycsZRq+bystRclod4rLS01Tz75pGnZsqVp3bq1kWQOHDjgte6SJUvMfffd53PbK1asMJLMr371K1NYWGiMOfO+rgkTJhhJZsqUKT631a5dO/PRRx953ZaRkWGCg4PNwYMH3crXr19vQkNDzcSJE83p06dNZWWlee6554wks3jxYp+/u76oL2NpjDHr1q0z4eHh5s9//rM5ffq0McaYffv2mW7dupl27dp51G/Xrt15jyUQBfJYcl7651h+9NFHZsCAAT7XN4bzMpDGsrbPS4ImXB566CHz85//3NhsNjNgwIBaP3FatWplKioq3MqrqqpMbGysadiwoes/TDUZM2aM+fLLL71u6969u7ntttvcyk6dOmXatGljkpKSTGVlpdu2G264wYSHh5v8/Hyfj6U+qC9jmZubayIiIswzzzzjsW3Dhg0eY2nMlfcftEAdS85L/x1LgmbNAnUsrTgvuXUOl+nTp2vt2rW19u6ssw0dOlQfffSRGjRo4FYeEhKi+Ph4VVRUqLS01Ke23njjDXXv3t2jfNu2bfrnP/+piRMnupW//fbbKigo0F133aXQUPc1CkaOHKmSkhItXrz4Io/Iv9WXsZwzZ45Onz7tMWaSdNNNN+ndd9+tlT7XZ4E6lpyXtas2xxI1C9SxtOK8JGjCpV27dpa13bp1a11zzTUe5Xa7XXv37tU111yjli1bXtZ3vP7667r66qt10003uZV/+OGHkqRevXp57OMs27hx42V9t7+pL2OZmZmpDh06qFWrVrXdzYARqGPJeVm7fop/x+JfAnUsrTgvCZq4ZN99951GjhypDh06KCoqSt26ddOjjz6qH3/8scZ9Kyoq9Pnnn+sXv/iFoqKi9Oabb15WX44fP67MzEw9/PDDHi/B/+abbyRJ8fHxHvs5y/bs2XNZ31/f1cVYFhQUqKCgQPHx8fr444+Vlpam+Ph414Pny5YtO+++GRkZSklJUXx8vOLj43XTTTdp1apVPh9vIKsvY8l5WbO6/HdsYWGhxo8fr86dOys6OlqdOnXS+PHjtXfv3vPuw3l5fvVlLC05Ly/qRjuuGL48cxITE2NWr15tysvLzenTp8369etNQkKCadmypfniiy/O2/b06dNNWFiYkWQGDRpk/vnPf152f19++WXTqFEjc+zYMY9tHTp0MJK89unEiRNGkmnYsOFl98Ff+etYfv7550aSadWqlYmNjTXvv/++KS8vN4cOHTIPPPCAkWQmTZrksV+7du3M6NGjXcdjs9nMww8/bCSZBx54wOfvr48CaSw5L/1zLI0581xfRESEWbBggTl16pSpqKgwn376qenatatp3Lix+cc//uGxD+dlYIylFeclQRNe1XTilJSUmKKiIo/yTZs2GUnm2muvvWD71dXVZv/+/WbixIkmODjYPP3005fV306dOp33YWv+g+afY/nJJ58YSUaSWbRokUebnTt3NpLMZ5995ratoKDAa3vXX3+9kWTeeecdn76/PgqkseS89M+xNMaYsrIyc/z4cY/yPXv2mJCQENOmTRtTVlbmto3zMjDGkqCJn0xNJ86FtGjRwkgy+/fv96n+bbfdZiSZ1atXX/R3GWPM5s2bjSSzfft2r9tTUlKMJPPpp596bMvLyzOSTExMzCV9d33gr2OZk5PjCid5eXke26dMmWIk+fwv1blz5xpJZsyYMT7Vr48CaSw5L/1zLGvSs2dPI8ls3rzZp/qclxfmb2NpxXnJM5qodXFxcZLOPLfli7S0NEnS2rVrL+n7Xn/9dfXq1UvXXXed1+1dunSRJK/PwjjLOnfufEnfHeisHMv27du7/rl169Ye22NjYyVJhw8f9um7L7avVxp/G0vOy0v3U/879nK+m/PywvxtLK04LwmauCQzZ87UkSNHvG7Lz8+XJEVHR7vKli5dqp07d3qt36RJE0lnVkO4WIWFhcrKyvL6ShWnIUOGSJJ27drlsc1ZduONN170dweKuhrLFi1aqFu3bpKkQ4cOeWwvLCyUJMXExLjKsrOzlZmZ6XNfrzT1aSw5Ly+sLv8d+8orr2jfvn0+fzfn5YXVp7G05Ly87GuvCEg13QqQZFauXOlRnp2dbSSZLl26eLT3m9/8xmtbY8aMMZLMzJkz3cqrqqq83oY72x//+EfTokWLC7689kp8MfTZ/Hks58+fbySZ+fPnu5U7HA5zzTXXGEnm888/d5XPmDHDJCcnG4fD4dHWoEGDznssgSKQxpLz0n/Hsl27dmb27Nke5fv27TMNGjQwrVq1MiUlJa5yzsvAGUsrzkuCJrzy5cRJSEgwH330kamurjZVVVVmy5Yt5qqrrjLh4eFm27ZtHu0FBwebuXPnmlOnThljziyp9eyzzxpJJjk52Zw8edJtn7S0NCPJvPzyy177UF1dba666irz29/+tsbj+eCDD1xLapWWlpqqqqqAXurubP48llVVVebGG280rVq1Mn//+99NdXW1KSoqMv/xH/9hJJknnnjCrf6MGTOMJJOenu56w8Dx48fNo48+aiSZO++80+t/7AJFII2lMZyX/jqW7dq1M82aNTNZWVmmqqrKOBwO87//+7+mR48eJjQ01Lz99ttu9TkvA2csjan985KgCZft27ebmJgYExMTYxo0aGAkmdatW5uYmBjz2GOPudXNzs42kyZNMtdcc42JiYkxzZo1M+3btzcPPvig+e677zza3rNnj5k5c6bp3bu3iY+PN61btzbNmjUzP/vZz8xzzz1niouLPfaZNGmSiYiIMCtWrPDa3/Xr15ugoCDz7bff+nR8//M//2OGDRtmWrdubVq3bm2uv/5688EHH/i0b31Tn8ayrKzMPP/886Zz586mefPmpnnz5mbgwIFeH3Y/cuSIefXVV82NN95o2rVrZ6KiokxkZKS5/vrrzcKFCwPyP2aBOpZOnJf+N5a7du0y06ZNMz179jSxsbGmefPmJj4+3owcOdLk5OR4tMN5GThj6VSb52WQMcZc3M12AAAAoGZMBgIAAIAlCJoAAACwBEETAAAAliBoAgAAwBIETQAAAFiCoAkAAABLEDQBAABgCYImAAAALEHQBAA/0qlTJ0VFRSkoKEgNGzZUbGysYmNjFR0drfDwcCUnJ+t3v/udjh8/fsF2PvnkEwUFBWnIkCE/Uc8BwBNBEwD8yN69e7Vz505JUr9+/XTo0CEdOnRIhYWFOnLkiO69917NmTNH1113nYqLi8/bzqJFiyRJH330kQ4cOPCT9B0AzkXQBIB6okmTJvr973+v1NRUff/991q6dKnXekVFRcrMzFSPHj1kjNHixYt/2o4CwP9H0ASAeiYlJUWStHv3bq/b//a3v6lLly7605/+JElaunSpqqurf7L+AYATQRMA6hlnaIyKivK6fdGiRXrggQc0ZMgQtWvXTnl5efr73//+U3YRACQRNAGg3tm+fbtCQ0N19913e2z7+uuv9dVXX2nUqFEKDg7W2LFjJf3rmU0A+CkRNAGgnjhy5IimT5+uqqoqffDBB+rWrZtHnUWLFumXv/ylIiMjJUljx45VcHCw3nvvPR05cuSn7jKAKxxBEwD81LZt21yvNwoPD1d0dLRWrlypP/3pT7rxxhs96ldUVOjNN9/UAw884Cpr27athg4dqsrKSr3xxhs/ZfcBgKAJAP7q7NcbHTt2TC+88IIOHDigtLQ0fffddx7133nnHUVGRmrAgAFu5c7gye1zAD81giYA1AONGjXS448/rpEjR6qsrEy///3vPeosWrRIhw8fVps2bVxXQmNjYzV58mSFhIRoz5492rZtWx30HsCViqAJAPXIn/70JzVq1EgrV67U3r17XeU2m01btmzRgQMHXFdBnZ/Dhw/rt7/9rSRp4cKFddV1AFcggiYA1CMJCQn6j//4DzkcDj333HOu8iVLluiWW25R69atve7nnH2+atWqC64oBAC1iaAJAPXME088oZYtW2rFihX67rvvZIzRkiVLdP/99593n+TkZKWkpKikpEQrV6786ToL4IoWZIwxdd0JAMAZnTp10vHjx3X06FE1aNBALVu2VLdu3bRp0ya3ei+//LKmTJmiiIgINW7cWEeOHFFUVJTuvvtuzZ8/363uwYMH1adPHxUXF+v06dNq3LixmjVrph9++EFhYWE/5eEBuMIQNAEAAGAJbp0DAADAEgRNAAAAWIKgCQAAAEsQNAEAAGAJgiYAAAAsQdAEAACAJQiaAAAAsARBEwAAAJYgaAIAAMASBE0AAABYgqAJAAAASxA0AQAAYAmCJgAAACzx/wDK+xHoyevxkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 711.1x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(coords.ra, coords.dec, c=\"k\", marker=\"x\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "ax.set(xlabel=\"RA\", ylabel=\"DEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observations with 9 set(s) of observations"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = Observations(\n",
    "    observed_coordinates=coords,\n",
    "    times=times,\n",
    "    observatories=\"kitt peak\",\n",
    "    astrometric_uncertainties=1 * u.arcsec,\n",
    ")\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-2.00572335,  1.77860137,  0.5197407 ], dtype=float64),\n",
       " Array([-0.00665991, -0.00662871, -0.00203885], dtype=float64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = Horizons(id=\"274301\", location=\"500@0\", epochs=times.tdb.jd[0])\n",
    "vecs = obj.vectors(refplane=\"earth\")\n",
    "true_x0 = jnp.array([vecs[\"x\"], vecs[\"y\"], vecs[\"z\"]]).T[0]\n",
    "true_v0 = jnp.array([vecs[\"vx\"], vecs[\"vy\"], vecs[\"vz\"]]).T[0]\n",
    "true_x0, true_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Particle: 274301 Wikipedia"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0 = Particle(\n",
    "    x=true_x0, v=true_v0, time=times[0], name=\"274301 Wikipedia\", observations=obs\n",
    ")\n",
    "p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartesianState(x=Array([[-2.00572335,  1.77860137,  0.5197407 ]], dtype=float64), v=Array([[-0.00665991, -0.00662871, -0.00203885]], dtype=float64), time=np.float64(2460676.792467407))\n",
      "KeplerianState(semi=Array([2.37859645], dtype=float64), ecc=Array([0.14924503], dtype=float64), inc=Array([6.73363769], dtype=float64), Omega=Array([183.37295038], dtype=float64), omega=Array([140.26385356], dtype=float64), nu=Array([173.65462829], dtype=float64), time=np.float64(2460676.792467407))\n"
     ]
    }
   ],
   "source": [
    "print(p0.cartesian_state)\n",
    "print(p0.keplerian_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           50\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33487D+10    |proj g|=  1.44601D+11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  1.05423D+10    |proj g|=  2.36925D+10\n",
      "\n",
      "At iterate    2    f=  8.37602D+09    |proj g|=  2.05286D+10\n",
      "\n",
      "At iterate    3    f=  3.29099D+09    |proj g|=  1.55430D+10\n",
      "\n",
      "At iterate    4    f=  5.86065D+08    |proj g|=  7.02731D+09\n",
      "\n",
      "At iterate    5    f=  2.00478D+07    |proj g|=  2.13931D+09\n",
      "\n",
      "At iterate    6    f=  2.00002D+06    |proj g|=  8.68628D+08\n",
      "\n",
      "At iterate    7    f=  2.67625D+05    |proj g|=  2.39249D+08\n",
      "\n",
      "At iterate    8    f=  2.03301D+04    |proj g|=  4.63480D+07\n",
      "\n",
      "At iterate    9    f=  3.45317D+03    |proj g|=  4.58154D+05\n",
      "\n",
      "At iterate   10    f=  3.44927D+03    |proj g|=  3.12814D+05\n",
      "\n",
      "At iterate   11    f=  3.44748D+03    |proj g|=  5.20120D+04\n",
      "\n",
      "At iterate   12    f=  3.44743D+03    |proj g|=  2.45289D+04\n",
      "\n",
      "At iterate   13    f=  3.44742D+03    |proj g|=  2.47594D+04\n",
      "\n",
      "At iterate   14    f=  3.44734D+03    |proj g|=  5.53064D+04\n",
      "\n",
      "At iterate   15    f=  3.44717D+03    |proj g|=  1.11200D+05\n",
      "\n",
      "At iterate   16    f=  3.44668D+03    |proj g|=  2.07102D+05\n",
      "\n",
      "At iterate   17    f=  3.44544D+03    |proj g|=  3.58822D+05\n",
      "\n",
      "At iterate   18    f=  3.44215D+03    |proj g|=  6.06123D+05\n",
      "\n",
      "At iterate   19    f=  3.43360D+03    |proj g|=  1.00377D+06\n",
      "\n",
      "At iterate   20    f=  3.41129D+03    |proj g|=  1.64279D+06\n",
      "\n",
      "At iterate   21    f=  3.35370D+03    |proj g|=  2.65420D+06\n",
      "\n",
      "At iterate   22    f=  3.20827D+03    |proj g|=  4.19823D+06\n",
      "\n",
      "At iterate   23    f=  2.86328D+03    |proj g|=  6.32105D+06\n",
      "\n",
      "At iterate   24    f=  2.16852D+03    |proj g|=  8.41573D+06\n",
      "\n",
      "At iterate   25    f=  1.18101D+03    |proj g|=  8.50310D+06\n",
      "\n",
      "At iterate   26    f=  4.29321D+02    |proj g|=  5.15504D+06\n",
      "\n",
      "At iterate   27    f=  1.00091D+02    |proj g|=  6.65740D+06\n",
      "\n",
      "At iterate   28    f=  2.32210D+01    |proj g|=  2.03793D+06\n",
      "\n",
      "At iterate   29    f=  1.69343D+01    |proj g|=  6.54982D+04\n",
      "\n",
      "At iterate   30    f=  1.69137D+01    |proj g|=  4.26324D+04\n",
      "\n",
      "At iterate   31    f=  1.69119D+01    |proj g|=  1.91307D+04\n",
      "\n",
      "At iterate   32    f=  1.69113D+01    |proj g|=  7.49615D+02\n",
      "\n",
      "At iterate   33    f=  1.69113D+01    |proj g|=  8.07790D+01\n",
      "\n",
      "At iterate   34    f=  1.69113D+01    |proj g|=  4.74402D+00\n",
      "\n",
      "At iterate   35    f=  1.69113D+01    |proj g|=  1.93161D+01\n",
      "\n",
      "At iterate   36    f=  1.69113D+01    |proj g|=  4.73395D+01\n",
      "\n",
      "At iterate   37    f=  1.69113D+01    |proj g|=  1.24859D+02\n",
      "\n",
      "At iterate   38    f=  1.69113D+01    |proj g|=  2.26457D+02\n",
      "\n",
      "At iterate   39    f=  1.69113D+01    |proj g|=  4.11029D+02\n",
      "\n",
      "At iterate   40    f=  1.69113D+01    |proj g|=  6.96118D+02\n",
      "\n",
      "At iterate   41    f=  1.69113D+01    |proj g|=  1.16478D+03\n",
      "\n",
      "At iterate   42    f=  1.69113D+01    |proj g|=  1.91883D+03\n",
      "\n",
      "At iterate   43    f=  1.69112D+01    |proj g|=  3.14313D+03\n",
      "\n",
      "At iterate   44    f=  1.69112D+01    |proj g|=  5.11814D+03\n",
      "\n",
      "At iterate   45    f=  1.69111D+01    |proj g|=  8.31913D+03\n",
      "\n",
      "At iterate   46    f=  1.69109D+01    |proj g|=  1.34902D+04\n",
      "\n",
      "At iterate   47    f=  1.69103D+01    |proj g|=  2.18417D+04\n",
      "\n",
      "At iterate   48    f=  1.69088D+01    |proj g|=  3.52869D+04\n",
      "\n",
      "At iterate   49    f=  1.69048D+01    |proj g|=  5.67506D+04\n",
      "\n",
      "At iterate   50    f=  1.68947D+01    |proj g|=  9.02992D+04\n",
      "\n",
      "At iterate   51    f=  1.68693D+01    |proj g|=  1.39779D+05\n",
      "\n",
      "At iterate   52    f=  1.68100D+01    |proj g|=  2.01232D+05\n",
      "\n",
      "At iterate   53    f=  1.66955D+01    |proj g|=  2.34911D+05\n",
      "\n",
      "At iterate   54    f=  1.65876D+01    |proj g|=  1.52197D+05\n",
      "\n",
      "At iterate   55    f=  1.65546D+01    |proj g|=  7.44867D+04\n",
      "\n",
      "At iterate   56    f=  1.65410D+01    |proj g|=  4.79920D+03\n",
      "\n",
      "At iterate   57    f=  1.65409D+01    |proj g|=  5.36609D+02\n",
      "\n",
      "At iterate   58    f=  1.65409D+01    |proj g|=  7.49257D+00\n",
      "\n",
      "At iterate   59    f=  1.65409D+01    |proj g|=  9.06118D-01\n",
      "\n",
      "At iterate   60    f=  1.65409D+01    |proj g|=  8.67703D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     60     64      1     0     0   8.677D-02   1.654D+01\n",
      "  F =   16.540893597687795     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "p1 = p0.max_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-4.53942769e-07, -1.02186033e-06],\n",
       "       [-3.98357920e-07,  1.04323482e-06],\n",
       "       [ 7.87147816e-07,  5.25321435e-07],\n",
       "       [ 2.72539265e-07, -7.25119919e-07],\n",
       "       [-4.97797634e-07,  6.35748338e-07],\n",
       "       [-3.48837977e-07,  4.05752313e-08],\n",
       "       [-2.87035437e-07, -9.92434556e-07],\n",
       "       [ 9.18807844e-07,  6.32994255e-07],\n",
       "       [-5.26095001e-07,  1.24681087e-07]], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.residuals(jnp.concatenate([p1._x, p1._v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.74874229e+10,  3.01094918e+08,  2.76083138e+09,  2.41128669e+08,\n",
       "        2.74309763e+09,  2.76083138e+09], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_initial_keplerian(ra, dec):\n",
    "    phi = ra\n",
    "    theta = jnp.pi / 2 - dec\n",
    "\n",
    "    x = jnp.sin(theta) * jnp.cos(phi)\n",
    "    y = jnp.sin(theta) * jnp.sin(phi)\n",
    "    z = jnp.cos(theta)\n",
    "\n",
    "    x_icrs = jnp.hstack([x, y, z])\n",
    "    x = icrs_to_horizons_ecliptic(x_icrs)\n",
    "\n",
    "    # assume we're observing the thing at its highest excursion from the ecliptic:\n",
    "    inc = jnp.abs(jnp.arcsin(x[2])) / jnp.linalg.norm(x) * 180 / jnp.pi\n",
    "\n",
    "    # its longitude of ascending node is the angle between the x-axis and the projection of the vector onto the xy-plane:\n",
    "    varphi = (jnp.arctan2(x[1], x[0]) * 180 / jnp.pi) % 360\n",
    "    Omega = varphi - 90 if x[2] > 0 else varphi + 90\n",
    "\n",
    "    nu = 90.0 if x[2] > 0 else 270.0\n",
    "    a = 3.0\n",
    "    ecc = 0.0\n",
    "    omega = 0.0\n",
    "\n",
    "    return jnp.array([a, ecc, nu, inc, Omega, omega])\n",
    "\n",
    "def keplerian_objective(x):\n",
    "    k = KeplerianState(\n",
    "            semi=x[0:1],\n",
    "            ecc=x[1:2],\n",
    "            nu=x[2:3],\n",
    "            inc=x[3:4],\n",
    "            Omega=x[4:5],\n",
    "            omega=x[5:6],\n",
    "            time=times[0],\n",
    "        )\n",
    "    c = k.to_cartesian()\n",
    "    x = jnp.concatenate([c.x.flatten(), c.v.flatten()])\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "keplerian_objective(generate_initial_keplerian(p0._observations.ra[0], p0._observations.dec[0]))\n",
    "\n",
    "jax.grad(keplerian_objective)(generate_initial_keplerian(p0._observations.ra[0], p0._observations.dec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(r, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m keplerian_objective(generate_initial_keplerian(p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mra[\u001b[38;5;241m0\u001b[39m], p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mdec[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 18\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeplerian_objective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_initial_keplerian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mra\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m res\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:937\u001b[0m, in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m             tr_solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsmr\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 937\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall_minpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    941\u001b[0m     result \u001b[38;5;241m=\u001b[39m trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,\n\u001b[1;32m    942\u001b[0m                  gtol, max_nfev, x_scale, loss_function, tr_solver,\n\u001b[1;32m    943\u001b[0m                  tr_options\u001b[38;5;241m.\u001b[39mcopy(), verbose)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:63\u001b[0m, in \u001b[0;36mcall_minpack\u001b[0;34m(fun, x0, jac, ftol, xtol, gtol, max_nfev, x_scale, diff_step)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_nfev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# n squared to account for Jacobian evaluations.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         max_nfev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m n \u001b[38;5;241m*\u001b[39m (n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     x, info, status \u001b[38;5;241m=\u001b[39m \u001b[43m_minpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lmdif\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsfcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_nfev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:830\u001b[0m, in \u001b[0;36mleast_squares.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/numpy/_core/shape_base.py:64\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mConvert inputs to arrays with at least one dimension.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def keplerian_objective(x):\n",
    "    k = KeplerianState(\n",
    "            semi=x[0:1],\n",
    "            ecc=x[1:2],\n",
    "            nu=x[2:3],\n",
    "            inc=x[3:4],\n",
    "            Omega=x[4:5],\n",
    "            omega=x[5:6],\n",
    "            time=times[0],\n",
    "        )\n",
    "    c = k.to_cartesian()\n",
    "    x = jnp.concatenate([c.x.flatten(), c.v.flatten()])\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "keplerian_objective(generate_initial_keplerian(p0._observations.ra[0], p0._observations.dec[0]))\n",
    "\n",
    "res = least_squares(\n",
    "    fun=keplerian_objective,\n",
    "    x0=generate_initial_keplerian(p0._observations.ra[0], p0._observations.dec[0]),\n",
    "    verbose=2,\n",
    "    method=\"lm\",\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.0803e+10                                    1.75e+10    \n",
      "       1              2         4.7208e+09      1.61e+10       3.64e+02       3.18e+10    \n",
      "       2              4         7.0212e+08      4.02e+09       1.08e+02       6.61e+09    \n",
      "       3              8         8.2983e+07      6.19e+08       1.18e+00       2.63e+09    \n",
      "       4             10         4.3101e+05      8.26e+07       5.88e-01       1.61e+08    \n",
      "       5             13         4.8339e+04      3.83e+05       7.35e-02       1.54e+07    \n",
      "       6             18         4.4935e+04      3.40e+03       5.74e-04       4.69e+06    \n",
      "       7             20         4.4571e+04      3.65e+02       1.44e-04       3.75e+05    \n",
      "       8             21         4.4562e+04      9.05e+00       1.44e-04       1.80e+06    \n",
      "       9             22         4.4510e+04      5.21e+01       3.59e-05       5.83e+05    \n",
      "      10             23         4.4496e+04      1.34e+01       3.59e-05       4.42e+05    \n",
      "      11             24         4.4484e+04      1.19e+01       3.59e-05       4.05e+05    \n",
      "      12             25         4.4473e+04      1.17e+01       3.59e-05       4.91e+05    \n",
      "      13             26         4.4461e+04      1.16e+01       3.59e-05       4.13e+05    \n",
      "      14             27         4.4450e+04      1.15e+01       3.59e-05       4.94e+05    \n",
      "      15             28         4.4438e+04      1.14e+01       3.59e-05       4.17e+05    \n",
      "      16             29         4.4427e+04      1.13e+01       3.59e-05       4.97e+05    \n",
      "      17             30         4.4416e+04      1.12e+01       3.59e-05       4.20e+05    \n",
      "      18             31         4.4405e+04      1.11e+01       3.59e-05       5.00e+05    \n",
      "      19             32         4.4394e+04      1.10e+01       3.59e-05       4.24e+05    \n",
      "      20             33         4.4383e+04      1.10e+01       3.59e-05       5.03e+05    \n",
      "      21             34         4.4372e+04      1.08e+01       3.59e-05       4.27e+05    \n",
      "      22             35         4.4361e+04      1.08e+01       3.59e-05       5.05e+05    \n",
      "      23             36         4.4350e+04      1.07e+01       3.59e-05       4.30e+05    \n",
      "      24             37         4.4340e+04      1.06e+01       3.59e-05       5.08e+05    \n",
      "      25             38         4.4329e+04      1.05e+01       3.59e-05       4.33e+05    \n",
      "      26             39         4.4319e+04      1.05e+01       3.59e-05       5.11e+05    \n",
      "      27             40         4.4308e+04      1.03e+01       3.59e-05       4.36e+05    \n",
      "      28             41         4.4298e+04      1.03e+01       3.59e-05       5.13e+05    \n",
      "      29             42         4.4288e+04      1.02e+01       3.59e-05       4.39e+05    \n",
      "      30             43         4.4278e+04      1.01e+01       3.59e-05       5.15e+05    \n",
      "      31             44         4.4268e+04      9.99e+00       3.59e-05       4.42e+05    \n",
      "      32             45         4.4258e+04      9.99e+00       3.59e-05       5.18e+05    \n",
      "      33             46         4.4248e+04      9.83e+00       3.59e-05       4.45e+05    \n",
      "      34             47         4.4238e+04      9.84e+00       3.59e-05       5.20e+05    \n",
      "      35             48         4.4228e+04      9.67e+00       3.59e-05       4.48e+05    \n",
      "      36             49         4.4219e+04      9.69e+00       3.59e-05       5.22e+05    \n",
      "      37             50         4.4209e+04      9.52e+00       3.59e-05       4.51e+05    \n",
      "      38             51         4.4200e+04      9.55e+00       3.59e-05       5.25e+05    \n",
      "      39             52         4.4190e+04      9.36e+00       3.59e-05       4.54e+05    \n",
      "      40             53         4.4181e+04      9.40e+00       3.59e-05       5.27e+05    \n",
      "      41             54         4.4172e+04      9.21e+00       3.59e-05       4.57e+05    \n",
      "      42             55         4.4162e+04      9.26e+00       3.59e-05       5.29e+05    \n",
      "      43             56         4.4153e+04      9.06e+00       3.59e-05       4.59e+05    \n",
      "      44             57         4.4144e+04      9.12e+00       3.59e-05       5.31e+05    \n",
      "      45             58         4.4135e+04      8.91e+00       3.59e-05       4.62e+05    \n",
      "      46             59         4.4126e+04      8.99e+00       3.59e-05       5.33e+05    \n",
      "      47             60         4.4118e+04      8.77e+00       3.59e-05       4.64e+05    \n",
      "      48             61         4.4109e+04      8.85e+00       3.59e-05       5.35e+05    \n",
      "      49             62         4.4100e+04      8.62e+00       3.59e-05       4.67e+05    \n",
      "      50             63         4.4091e+04      8.72e+00       3.59e-05       5.37e+05    \n",
      "      51             64         4.4083e+04      8.48e+00       3.59e-05       4.69e+05    \n",
      "      52             65         4.4074e+04      8.59e+00       3.59e-05       5.39e+05    \n",
      "      53             66         4.4066e+04      8.34e+00       3.59e-05       4.72e+05    \n",
      "      54             67         4.4058e+04      8.46e+00       3.59e-05       5.41e+05    \n",
      "      55             68         4.4049e+04      8.21e+00       3.59e-05       4.74e+05    \n",
      "      56             69         4.4041e+04      8.34e+00       3.59e-05       5.42e+05    \n",
      "      57             70         4.4033e+04      8.07e+00       3.59e-05       4.76e+05    \n",
      "      58             71         4.4025e+04      8.21e+00       3.59e-05       5.44e+05    \n",
      "      59             72         4.4017e+04      7.94e+00       3.59e-05       4.79e+05    \n",
      "      60             73         4.4009e+04      8.09e+00       3.59e-05       5.46e+05    \n",
      "      61             74         4.4001e+04      7.81e+00       3.59e-05       4.81e+05    \n",
      "      62             75         4.3993e+04      7.97e+00       3.59e-05       5.47e+05    \n",
      "      63             76         4.3985e+04      7.68e+00       3.59e-05       4.83e+05    \n",
      "      64             77         4.3977e+04      7.86e+00       3.59e-05       5.49e+05    \n",
      "      65             78         4.3970e+04      7.55e+00       3.59e-05       4.85e+05    \n",
      "      66             79         4.3962e+04      7.74e+00       3.59e-05       5.51e+05    \n",
      "      67             80         4.3955e+04      7.43e+00       3.59e-05       4.87e+05    \n",
      "      68             81         4.3947e+04      7.63e+00       3.59e-05       5.52e+05    \n",
      "      69             82         4.3940e+04      7.30e+00       3.59e-05       4.90e+05    \n",
      "      70             83         4.3932e+04      7.52e+00       3.59e-05       5.54e+05    \n",
      "      71             84         4.3925e+04      7.18e+00       3.59e-05       4.92e+05    \n",
      "      72             85         4.3918e+04      7.41e+00       3.59e-05       5.55e+05    \n",
      "      73             86         4.3911e+04      7.06e+00       3.59e-05       4.94e+05    \n",
      "      74             87         4.3903e+04      7.30e+00       3.59e-05       5.57e+05    \n",
      "      75             88         4.3896e+04      6.95e+00       3.59e-05       4.96e+05    \n",
      "      76             89         4.3889e+04      7.19e+00       3.59e-05       5.58e+05    \n",
      "      77             90         4.3882e+04      6.83e+00       3.59e-05       4.98e+05    \n",
      "      78             91         4.3875e+04      7.09e+00       3.59e-05       5.60e+05    \n",
      "      79             92         4.3869e+04      6.71e+00       3.59e-05       4.99e+05    \n",
      "      80             93         4.3862e+04      6.99e+00       3.59e-05       5.61e+05    \n",
      "      81             94         4.3855e+04      6.60e+00       3.59e-05       5.01e+05    \n",
      "      82             95         4.3848e+04      6.88e+00       3.59e-05       5.62e+05    \n",
      "      83             96         4.3842e+04      6.49e+00       3.59e-05       5.03e+05    \n",
      "      84             97         4.3835e+04      6.79e+00       3.59e-05       5.64e+05    \n",
      "      85             98         4.3828e+04      6.38e+00       3.59e-05       5.05e+05    \n",
      "      86             99         4.3822e+04      6.69e+00       3.59e-05       5.65e+05    \n",
      "      87             100        4.3815e+04      6.27e+00       3.59e-05       5.07e+05    \n",
      "      88             101        4.3809e+04      6.59e+00       3.59e-05       5.66e+05    \n",
      "      89             102        4.3803e+04      6.17e+00       3.59e-05       5.09e+05    \n",
      "      90             103        4.3796e+04      6.50e+00       3.59e-05       5.67e+05    \n",
      "      91             104        4.3790e+04      6.06e+00       3.59e-05       5.10e+05    \n",
      "      92             105        4.3784e+04      6.41e+00       3.59e-05       5.69e+05    \n",
      "      93             106        4.3778e+04      5.96e+00       3.59e-05       5.12e+05    \n",
      "      94             107        4.3773e+04      5.08e+00       8.97e-06       2.57e+05    \n",
      "      95             108        4.3767e+04      5.46e+00       1.79e-05       2.60e+05    \n",
      "      96             109        4.3762e+04      5.67e+00       1.79e-05       2.57e+05    \n",
      "      97             110        4.3752e+04      9.92e+00       3.59e-05       3.52e+05    \n",
      "      98             112        4.3746e+04      5.51e+00       1.79e-05       2.54e+05    \n",
      "      99             113        4.3741e+04      5.54e+00       1.79e-05       2.56e+05    \n",
      "      100            114        4.3731e+04      1.00e+01       3.59e-05       2.66e+05    \n",
      "      101            116        4.3725e+04      5.33e+00       1.79e-05       2.53e+05    \n",
      "      102            117        4.3720e+04      5.47e+00       1.79e-05       2.50e+05    \n",
      "      103            118        4.3710e+04      9.28e+00       3.59e-05       3.75e+05    \n",
      "      104            119        4.3706e+04      4.01e+00       3.59e-05       5.59e+05    \n",
      "      105            120        4.3701e+04      5.36e+00       8.97e-06       2.78e+05    \n",
      "      106            121        4.3696e+04      5.24e+00       1.79e-05       2.48e+05    \n",
      "      107            122        4.3691e+04      5.32e+00       1.79e-05       2.44e+05    \n",
      "      108            123        4.3682e+04      8.79e+00       3.59e-05       3.93e+05    \n",
      "      109            124        4.3678e+04      3.50e+00       3.59e-05       5.71e+05    \n",
      "      110            125        4.3673e+04      5.42e+00       8.97e-06       2.89e+05    \n",
      "      111            126        4.3668e+04      5.15e+00       1.79e-05       2.43e+05    \n",
      "      112            127        4.3662e+04      5.18e+00       1.79e-05       2.39e+05    \n",
      "      113            128        4.3654e+04      8.29e+00       3.59e-05       4.11e+05    \n",
      "      114            129        4.3651e+04      3.07e+00       3.59e-05       5.81e+05    \n",
      "      115            130        4.3646e+04      5.47e+00       8.97e-06       2.97e+05    \n",
      "      116            131        4.3641e+04      5.06e+00       1.79e-05       2.38e+05    \n",
      "      117            132        4.3636e+04      5.03e+00       1.79e-05       2.34e+05    \n",
      "      118            133        4.3628e+04      7.77e+00       3.59e-05       4.29e+05    \n",
      "      119            134        4.3625e+04      2.73e+00       3.59e-05       5.88e+05    \n",
      "      120            135        4.3620e+04      5.49e+00       8.97e-06       3.02e+05    \n",
      "      121            136        4.3615e+04      4.96e+00       1.79e-05       2.33e+05    \n",
      "      122            137        4.3610e+04      4.88e+00       1.79e-05       2.29e+05    \n",
      "      123            138        4.3602e+04      7.23e+00       3.59e-05       4.49e+05    \n",
      "      124            139        4.3600e+04      2.48e+00       3.59e-05       5.93e+05    \n",
      "      125            140        4.3595e+04      5.50e+00       8.97e-06       3.05e+05    \n",
      "      126            141        4.3590e+04      4.85e+00       1.79e-05       2.28e+05    \n",
      "      127            142        4.3585e+04      4.73e+00       1.79e-05       2.24e+05    \n",
      "      128            143        4.3578e+04      6.67e+00       3.59e-05       4.69e+05    \n",
      "      129            144        4.3576e+04      2.32e+00       3.59e-05       5.94e+05    \n",
      "      130            145        4.3570e+04      5.48e+00       8.97e-06       3.06e+05    \n",
      "      131            146        4.3566e+04      4.73e+00       1.79e-05       2.23e+05    \n",
      "      132            147        4.3561e+04      4.58e+00       1.79e-05       2.20e+05    \n",
      "      133            148        4.3555e+04      6.07e+00       3.59e-05       4.91e+05    \n",
      "      134            149        4.3553e+04      2.27e+00       3.59e-05       5.93e+05    \n",
      "      135            150        4.3547e+04      5.44e+00       8.97e-06       3.04e+05    \n",
      "      136            151        4.3543e+04      4.60e+00       1.79e-05       2.19e+05    \n",
      "      137            152        4.3538e+04      4.42e+00       1.79e-05       2.15e+05    \n",
      "      138            153        4.3534e+04      4.32e+00       1.79e-05       2.17e+05    \n",
      "      139            154        4.3530e+04      4.31e+00       1.79e-05       2.13e+05    \n",
      "      140            155        4.3525e+04      4.23e+00       1.79e-05       2.15e+05    \n",
      "      141            156        4.3521e+04      4.24e+00       1.79e-05       2.11e+05    \n",
      "      142            157        4.3517e+04      4.16e+00       1.79e-05       2.14e+05    \n",
      "      143            158        4.3513e+04      4.17e+00       1.79e-05       2.09e+05    \n",
      "      144            159        4.3509e+04      4.10e+00       1.79e-05       2.12e+05    \n",
      "      145            160        4.3505e+04      4.10e+00       1.79e-05       2.08e+05    \n",
      "      146            161        4.3501e+04      4.03e+00       1.79e-05       2.13e+05    \n",
      "      147            162        4.3497e+04      4.04e+00       1.79e-05       2.06e+05    \n",
      "      148            163        4.3493e+04      3.97e+00       1.79e-05       2.15e+05    \n",
      "      149            164        4.3489e+04      3.97e+00       1.79e-05       2.04e+05    \n",
      "      150            165        4.3485e+04      3.91e+00       1.79e-05       2.18e+05    \n",
      "      151            166        4.3481e+04      3.91e+00       1.79e-05       2.03e+05    \n",
      "      152            167        4.3477e+04      3.85e+00       1.79e-05       2.20e+05    \n",
      "      153            168        4.3473e+04      3.85e+00       1.79e-05       2.01e+05    \n",
      "      154            169        4.3469e+04      3.79e+00       1.79e-05       2.23e+05    \n",
      "      155            170        4.3466e+04      3.79e+00       1.79e-05       1.99e+05    \n",
      "      156            171        4.3462e+04      3.73e+00       1.79e-05       2.25e+05    \n",
      "      157            172        4.3458e+04      3.73e+00       1.79e-05       1.98e+05    \n",
      "      158            173        4.3455e+04      3.67e+00       1.79e-05       2.27e+05    \n",
      "      159            174        4.3451e+04      3.67e+00       1.79e-05       1.96e+05    \n",
      "      160            175        4.3447e+04      3.61e+00       1.79e-05       2.30e+05    \n",
      "      161            176        4.3444e+04      3.61e+00       1.79e-05       1.94e+05    \n",
      "      162            177        4.3440e+04      3.56e+00       1.79e-05       2.32e+05    \n",
      "      163            178        4.3436e+04      3.56e+00       1.79e-05       1.93e+05    \n",
      "      164            179        4.3433e+04      3.50e+00       1.79e-05       2.34e+05    \n",
      "      165            180        4.3429e+04      3.50e+00       1.79e-05       1.91e+05    \n",
      "      166            181        4.3426e+04      3.45e+00       1.79e-05       2.36e+05    \n",
      "      167            182        4.3423e+04      3.45e+00       1.79e-05       1.90e+05    \n",
      "      168            183        4.3419e+04      3.40e+00       1.79e-05       2.38e+05    \n",
      "      169            184        4.3416e+04      3.39e+00       1.79e-05       1.88e+05    \n",
      "      170            185        4.3412e+04      3.35e+00       1.79e-05       2.40e+05    \n",
      "      171            186        4.3409e+04      3.34e+00       1.79e-05       1.86e+05    \n",
      "      172            187        4.3406e+04      3.30e+00       1.79e-05       2.42e+05    \n",
      "      173            188        4.3403e+04      3.29e+00       1.79e-05       1.85e+05    \n",
      "      174            189        4.3399e+04      3.25e+00       1.79e-05       2.43e+05    \n",
      "      175            190        4.3396e+04      3.24e+00       1.79e-05       1.83e+05    \n",
      "      176            191        4.3393e+04      3.20e+00       1.79e-05       2.45e+05    \n",
      "      177            192        4.3390e+04      3.19e+00       1.79e-05       1.85e+05    \n",
      "      178            193        4.3387e+04      3.15e+00       1.79e-05       2.47e+05    \n",
      "      179            194        4.3383e+04      3.14e+00       1.79e-05       1.87e+05    \n",
      "      180            195        4.3380e+04      3.10e+00       1.79e-05       2.48e+05    \n",
      "      181            196        4.3377e+04      3.09e+00       1.79e-05       1.89e+05    \n",
      "      182            197        4.3374e+04      3.05e+00       1.79e-05       2.50e+05    \n",
      "      183            198        4.3371e+04      3.04e+00       1.79e-05       1.91e+05    \n",
      "      184            199        4.3368e+04      3.01e+00       1.79e-05       2.52e+05    \n",
      "      185            200        4.3365e+04      2.99e+00       1.79e-05       1.93e+05    \n",
      "      186            201        4.3362e+04      2.96e+00       1.79e-05       2.53e+05    \n",
      "      187            202        4.3359e+04      2.94e+00       1.79e-05       1.94e+05    \n",
      "      188            203        4.3356e+04      2.92e+00       1.79e-05       2.55e+05    \n",
      "      189            204        4.3353e+04      2.90e+00       1.79e-05       1.96e+05    \n",
      "      190            205        4.3350e+04      2.88e+00       1.79e-05       2.56e+05    \n",
      "      191            206        4.3348e+04      2.85e+00       1.79e-05       1.98e+05    \n",
      "      192            207        4.3345e+04      2.83e+00       1.79e-05       2.57e+05    \n",
      "      193            208        4.3342e+04      2.81e+00       1.79e-05       2.00e+05    \n",
      "      194            209        4.3339e+04      2.79e+00       1.79e-05       2.59e+05    \n",
      "      195            210        4.3336e+04      2.76e+00       1.79e-05       2.01e+05    \n",
      "      196            211        4.3334e+04      2.75e+00       1.79e-05       2.60e+05    \n",
      "      197            212        4.3331e+04      2.72e+00       1.79e-05       2.03e+05    \n",
      "      198            213        4.3328e+04      2.71e+00       1.79e-05       2.62e+05    \n",
      "      199            214        4.3326e+04      2.68e+00       1.79e-05       2.04e+05    \n",
      "      200            215        4.3323e+04      2.67e+00       1.79e-05       2.63e+05    \n",
      "      201            216        4.3320e+04      2.64e+00       1.79e-05       2.06e+05    \n",
      "      202            217        4.3318e+04      2.63e+00       1.79e-05       2.64e+05    \n",
      "      203            218        4.3315e+04      2.59e+00       1.79e-05       2.08e+05    \n",
      "      204            219        4.3312e+04      2.59e+00       1.79e-05       2.65e+05    \n",
      "      205            220        4.3310e+04      2.55e+00       1.79e-05       2.09e+05    \n",
      "      206            221        4.3307e+04      2.55e+00       1.79e-05       2.66e+05    \n",
      "      207            222        4.3305e+04      2.51e+00       1.79e-05       2.10e+05    \n",
      "      208            223        4.3302e+04      2.51e+00       1.79e-05       2.68e+05    \n",
      "      209            224        4.3300e+04      2.47e+00       1.79e-05       2.12e+05    \n",
      "      210            225        4.3297e+04      2.48e+00       1.79e-05       2.69e+05    \n",
      "      211            226        4.3295e+04      2.43e+00       1.79e-05       2.13e+05    \n",
      "      212            227        4.3293e+04      2.44e+00       1.79e-05       2.70e+05    \n",
      "      213            228        4.3290e+04      2.40e+00       1.79e-05       2.15e+05    \n",
      "      214            229        4.3288e+04      2.41e+00       1.79e-05       2.71e+05    \n",
      "      215            230        4.3285e+04      2.36e+00       1.79e-05       2.16e+05    \n",
      "      216            231        4.3283e+04      2.37e+00       1.79e-05       2.72e+05    \n",
      "      217            232        4.3281e+04      2.32e+00       1.79e-05       2.17e+05    \n",
      "      218            233        4.3278e+04      2.34e+00       1.79e-05       2.73e+05    \n",
      "      219            234        4.3276e+04      2.29e+00       1.79e-05       2.19e+05    \n",
      "      220            235        4.3274e+04      2.30e+00       1.79e-05       2.74e+05    \n",
      "      221            236        4.3271e+04      2.25e+00       1.79e-05       2.20e+05    \n",
      "      222            237        4.3269e+04      2.27e+00       1.79e-05       2.75e+05    \n",
      "      223            238        4.3267e+04      2.21e+00       1.79e-05       2.21e+05    \n",
      "      224            239        4.3265e+04      2.24e+00       1.79e-05       2.76e+05    \n",
      "      225            240        4.3263e+04      2.18e+00       1.79e-05       2.22e+05    \n",
      "      226            241        4.3260e+04      2.20e+00       1.79e-05       2.77e+05    \n",
      "      227            242        4.3258e+04      2.14e+00       1.79e-05       2.24e+05    \n",
      "      228            243        4.3256e+04      2.17e+00       1.79e-05       2.78e+05    \n",
      "      229            244        4.3254e+04      2.11e+00       1.79e-05       2.25e+05    \n",
      "      230            245        4.3252e+04      2.14e+00       1.79e-05       2.79e+05    \n",
      "      231            246        4.3250e+04      2.08e+00       1.79e-05       2.26e+05    \n",
      "      232            247        4.3248e+04      2.11e+00       1.79e-05       2.80e+05    \n",
      "      233            248        4.3246e+04      2.04e+00       1.79e-05       2.27e+05    \n",
      "      234            249        4.3244e+04      2.08e+00       1.79e-05       2.80e+05    \n",
      "      235            250        4.3242e+04      2.01e+00       1.79e-05       2.28e+05    \n",
      "      236            251        4.3239e+04      2.05e+00       1.79e-05       2.81e+05    \n",
      "      237            252        4.3237e+04      1.98e+00       1.79e-05       2.29e+05    \n",
      "      238            253        4.3235e+04      2.02e+00       1.79e-05       2.82e+05    \n",
      "      239            254        4.3234e+04      1.95e+00       1.79e-05       2.31e+05    \n",
      "      240            255        4.3232e+04      1.99e+00       1.79e-05       2.83e+05    \n",
      "      241            256        4.3230e+04      1.92e+00       1.79e-05       2.32e+05    \n",
      "      242            257        4.3228e+04      1.96e+00       1.79e-05       2.84e+05    \n",
      "      243            258        4.3226e+04      1.89e+00       1.79e-05       2.33e+05    \n",
      "      244            259        4.3224e+04      1.94e+00       1.79e-05       2.84e+05    \n",
      "      245            260        4.3222e+04      1.86e+00       1.79e-05       2.34e+05    \n",
      "      246            261        4.3220e+04      1.91e+00       1.79e-05       2.85e+05    \n",
      "      247            262        4.3218e+04      1.83e+00       1.79e-05       2.35e+05    \n",
      "      248            263        4.3216e+04      1.88e+00       1.79e-05       2.86e+05    \n",
      "      249            264        4.3215e+04      1.80e+00       1.79e-05       2.36e+05    \n",
      "      250            265        4.3213e+04      1.86e+00       1.79e-05       2.87e+05    \n",
      "      251            266        4.3211e+04      1.77e+00       1.79e-05       2.37e+05    \n",
      "      252            267        4.3209e+04      1.83e+00       1.79e-05       2.87e+05    \n",
      "      253            268        4.3207e+04      1.74e+00       1.79e-05       2.38e+05    \n",
      "      254            269        4.3206e+04      1.81e+00       1.79e-05       2.88e+05    \n",
      "      255            270        4.3204e+04      1.71e+00       1.79e-05       2.39e+05    \n",
      "      256            271        4.3202e+04      1.78e+00       1.79e-05       2.89e+05    \n",
      "      257            272        4.3200e+04      1.69e+00       1.79e-05       2.40e+05    \n",
      "      258            273        4.3199e+04      1.76e+00       1.79e-05       2.89e+05    \n",
      "      259            274        4.3197e+04      1.66e+00       1.79e-05       2.40e+05    \n",
      "      260            275        4.3195e+04      1.73e+00       1.79e-05       2.90e+05    \n",
      "      261            276        4.3194e+04      1.63e+00       1.79e-05       2.41e+05    \n",
      "      262            277        4.3192e+04      1.71e+00       1.79e-05       2.91e+05    \n",
      "      263            278        4.3190e+04      1.61e+00       1.79e-05       2.42e+05    \n",
      "      264            279        4.3189e+04      1.69e+00       1.79e-05       2.91e+05    \n",
      "      265            280        4.3187e+04      1.58e+00       1.79e-05       2.43e+05    \n",
      "      266            281        4.3185e+04      1.66e+00       1.79e-05       2.92e+05    \n",
      "      267            282        4.3184e+04      1.55e+00       1.79e-05       2.44e+05    \n",
      "      268            283        4.3182e+04      1.64e+00       1.79e-05       2.92e+05    \n",
      "      269            284        4.3181e+04      1.53e+00       1.79e-05       2.45e+05    \n",
      "      270            285        4.3179e+04      1.62e+00       1.79e-05       2.93e+05    \n",
      "      271            286        4.3177e+04      1.50e+00       1.79e-05       2.46e+05    \n",
      "      272            287        4.3176e+04      1.27e+00       4.49e-06       1.24e+05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 287, initial cost 2.0803e+10, final cost 4.3176e+04, first-order optimality 1.24e+05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     message: `xtol` termination condition is satisfied.\n",
       "     success: True\n",
       "      status: 3\n",
       "         fun: [ 1.040e+02  1.017e+02  9.926e+01  4.706e+01  4.468e+01\n",
       "                4.228e+01  1.258e+02  1.282e+02  1.307e+02]\n",
       "           x: [ 2.550e+00  1.663e-01  1.963e+02  1.013e+01  5.282e+02\n",
       "               -2.260e+02]\n",
       "        cost: 43176.221262296895\n",
       "         jac: [[-4.693e+03 -9.579e+03 ...  2.269e+02 -4.938e+02]\n",
       "               [-4.658e+03 -9.505e+03 ...  2.213e+02 -4.996e+02]\n",
       "               ...\n",
       "               [ 4.807e+03  9.557e+03 ... -2.619e+02  4.789e+02]\n",
       "               [ 4.747e+03  9.429e+03 ... -2.525e+02  4.886e+02]]\n",
       "        grad: [ 9.513e+04  1.096e+05 -8.231e+04  1.238e+05 -5.003e+04\n",
       "               -8.194e+04]\n",
       "  optimality: 123754.6547348428\n",
       " active_mask: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
       "                0.000e+00]\n",
       "        nfev: 287\n",
       "        njev: 273"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keplerian_objective(x):\n",
    "    k = KeplerianState(\n",
    "            semi=x[0:1],\n",
    "            ecc=x[1:2],\n",
    "            nu=x[2:3],\n",
    "            inc=x[3:4],\n",
    "            Omega=x[4:5],\n",
    "            omega=x[5:6],\n",
    "            time=times[0],\n",
    "        )\n",
    "    c = k.to_cartesian()\n",
    "    x = jnp.concatenate([c.x.flatten(), c.v.flatten()])\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "res = least_squares(\n",
    "    fun=keplerian_objective,\n",
    "    jac=jax.jacfwd(keplerian_objective),\n",
    "    x0=generate_initial_keplerian(p0._observations.ra[0], p0._observations.dec[0]),\n",
    "    verbose=2,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.55043707e+00,  1.66256623e-01,  1.96267115e+02,  1.01308640e+01,\n",
       "        5.28163642e+02, -2.26048969e+02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeplerianState(semi=Array([2.37859645], dtype=float64), ecc=Array([0.14924503], dtype=float64), inc=Array([6.73363769], dtype=float64), Omega=Array([183.37295038], dtype=float64), omega=Array([140.26385356], dtype=float64), nu=Array([173.65462829], dtype=float64), time=np.float64(2460676.792467407))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.keplerian_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-22.57306906, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "perturbed = copy.deepcopy(p0.cartesian_state)\n",
    "perturbed.x += 1000 * u.km.to(u.au)\n",
    "ll = p0.loglike(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.90962391,  0.69842047],\n",
       "       [-0.90984353,  0.69858176],\n",
       "       [-0.91006152,  0.69873977],\n",
       "       [-0.91479967,  0.70217721],\n",
       "       [-0.9150206 ,  0.70233721],\n",
       "       [-0.91524007,  0.70249453],\n",
       "       [-0.93037155,  0.71334351],\n",
       "       [-0.93059056,  0.71350114],\n",
       "       [-0.93081201,  0.71365595]], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "x0 = jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    "x1 = jnp.concatenate([-perturbed.x.flatten(), perturbed.v.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-0.90962391,  0.69842047],\n",
       "        [-0.90984353,  0.69858176],\n",
       "        [-0.91006152,  0.69873977],\n",
       "        [-0.91479967,  0.70217721],\n",
       "        [-0.9150206 ,  0.70233721],\n",
       "        [-0.91524007,  0.70249453],\n",
       "        [-0.93037155,  0.71334351],\n",
       "        [-0.93059056,  0.71350114],\n",
       "        [-0.93081201,  0.71365595]], dtype=float64),\n",
       " Array([[-91092.82616836,  44300.54791232],\n",
       "        [-91067.48832186,  44293.03170153],\n",
       "        [-91041.82333691,  44285.50912264],\n",
       "        [-90499.66732878,  44118.36499448],\n",
       "        [-90472.91765956,  44110.15553629],\n",
       "        [-90445.84779392,  44101.94104199],\n",
       "        [-88523.61709689,  43472.6618289 ],\n",
       "        [-88492.77544129,  43462.40902278],\n",
       "        [-88461.63551964,  43452.15541053]], dtype=float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(x0), p0.residuals(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3188.938457\n",
      "         Iterations: 439\n",
      "         Function evaluations: 713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-3188.9384567, dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "result = minimize(\n",
    "    fun=f,\n",
    "    x0=x1,\n",
    "    method=\"Nelder-Mead\",\n",
    "    options={\"disp\": True, \"maxiter\": 1000},\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 558, initial cost 4.5159e+10, final cost 2.8194e+03, first-order optimality 7.82e+03.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-2835.97158603, dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tmp(x):\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=x1,\n",
    "    verbose=2,\n",
    "    method=\"lm\",\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         4.5159e+10                                    8.19e+10    \n",
      "       1              3         8.4716e+09      3.67e+10       6.83e-01       1.40e+10    \n",
      "       2              4         8.2194e+09      2.52e+08       1.37e+00       1.90e+10    \n",
      "       3              5         2.6552e+09      5.56e+09       3.41e-01       8.10e+09    \n",
      "       4              6         4.2061e+07      2.61e+09       6.83e-01       1.18e+09    \n",
      "       5              8         1.8053e+07      2.40e+07       5.42e-02       1.66e+09    \n",
      "       6              9         7.1703e+05      1.73e+07       5.42e-02       2.57e+08    \n",
      "       7             12         8.3191e+04      6.34e+05       6.77e-03       1.16e+08    \n",
      "       8             15         2.0467e+04      6.27e+04       8.46e-04       3.10e+07    \n",
      "       9             16         1.3565e+04      6.90e+03       1.69e-03       3.76e+07    \n",
      "      10             18         7.0870e+03      6.48e+03       4.23e-04       2.12e+07    \n",
      "      11             20         5.4135e+03      1.67e+03       1.06e-04       9.87e+06    \n",
      "      12             22         5.0311e+03      3.82e+02       5.29e-05       4.09e+06    \n",
      "      13             24         4.9438e+03      8.73e+01       2.64e-05       1.67e+06    \n",
      "      14             26         4.9224e+03      2.14e+01       1.32e-05       9.50e+05    \n",
      "      15             27         4.9080e+03      1.44e+01       2.64e-05       3.82e+05    \n",
      "      16             30         4.9068e+03      1.22e+00       3.31e-06       2.30e+05    \n",
      "      17             31         4.9052e+03      1.62e+00       6.61e-06       1.65e+05    \n",
      "      18             33         4.9045e+03      6.46e-01       3.31e-06       1.69e+05    \n",
      "      19             34         4.9038e+03      6.85e-01       6.61e-06       2.39e+05    \n",
      "      20             36         4.9034e+03      3.94e-01       1.65e-06       9.39e+04    \n",
      "      21             37         4.9032e+03      2.34e-01       3.31e-06       1.51e+05    \n",
      "      22             38         4.9030e+03      1.61e-01       3.31e-06       1.41e+05    \n",
      "      23             39         4.9029e+03      1.17e-01       3.31e-06       1.64e+05    \n",
      "      24             40         4.9028e+03      1.27e-01       8.26e-07       8.53e+04    \n",
      "      25             41         4.9027e+03      9.16e-02       1.65e-06       4.99e+04    \n",
      "      26             42         4.9026e+03      6.12e-02       1.65e-06       7.36e+04    \n",
      "      27             43         4.9026e+03      4.55e-02       1.65e-06       6.96e+04    \n",
      "      28             44         4.9026e+03      3.13e-02       1.65e-06       8.05e+04    \n",
      "      29             45         4.9025e+03      3.24e-02       4.13e-07       4.20e+04    \n",
      "      30             46         4.9025e+03      2.65e-02       8.26e-07       2.32e+04    \n",
      "      31             47         4.9025e+03      1.94e-02       8.26e-07       3.24e+04    \n",
      "      32             48         4.9025e+03      1.61e-02       8.26e-07       3.22e+04    \n",
      "      33             49         4.9025e+03      1.16e-02       8.26e-07       3.72e+04    \n",
      "      34             50         4.9024e+03      1.11e-02       8.26e-07       3.61e+04    \n",
      "      35             51         4.9024e+03      8.18e-03       8.26e-07       3.86e+04    \n",
      "      36             52         4.9024e+03      8.19e-03       2.07e-07       1.96e+04    \n",
      "      37             53         4.9024e+03      7.91e-03       4.13e-07       1.92e+04    \n",
      "      38             54         4.9024e+03      7.27e-03       4.13e-07       1.44e+04    \n",
      "      39             55         4.9024e+03      6.67e-03       4.13e-07       1.92e+04    \n",
      "      40             56         4.9024e+03      6.31e-03       4.13e-07       1.39e+04    \n",
      "      41             57         4.9024e+03      5.88e-03       4.13e-07       1.92e+04    \n",
      "      42             58         4.9024e+03      5.94e-03       4.13e-07       1.59e+04    \n",
      "      43             59         4.9024e+03      6.11e-03       4.13e-07       1.82e+04    \n",
      "      44             60         4.9024e+03      5.60e-03       4.13e-07       1.48e+04    \n",
      "      45             61         4.9024e+03      5.71e-03       4.13e-07       1.81e+04    \n",
      "      46             62         4.9024e+03      5.55e-03       4.13e-07       1.31e+04    \n",
      "      47             63         4.9024e+03      5.21e-03       4.13e-07       1.77e+04    \n",
      "      48             64         4.9023e+03      5.29e-03       4.13e-07       1.41e+04    \n",
      "      49             65         4.9023e+03      5.44e-03       4.13e-07       1.85e+04    \n",
      "      50             66         4.9023e+03      5.41e-03       4.13e-07       1.47e+04    \n",
      "      51             67         4.9023e+03      5.62e-03       4.13e-07       1.82e+04    \n",
      "      52             68         4.9023e+03      5.48e-03       4.13e-07       1.52e+04    \n",
      "      53             69         4.9023e+03      5.72e-03       4.13e-07       1.76e+04    \n",
      "      54             70         4.9023e+03      5.33e-03       4.13e-07       1.51e+04    \n",
      "      55             71         4.9023e+03      5.68e-03       4.13e-07       1.67e+04    \n",
      "      56             72         4.9023e+03      5.11e-03       4.13e-07       1.36e+04    \n",
      "      57             73         4.9023e+03      5.33e-03       4.13e-07       1.79e+04    \n",
      "      58             74         4.9023e+03      5.37e-03       4.13e-07       1.43e+04    \n",
      "      59             75         4.9023e+03      5.55e-03       4.13e-07       1.81e+04    \n",
      "      60             76         4.9023e+03      5.50e-03       4.13e-07       1.37e+04    \n",
      "      61             77         4.9023e+03      5.33e-03       4.13e-07       1.66e+04    \n",
      "      62             78         4.9023e+03      4.97e-03       4.13e-07       1.48e+04    \n",
      "      63             79         4.9023e+03      5.71e-03       4.13e-07       1.79e+04    \n",
      "      64             80         4.9023e+03      5.31e-03       4.13e-07       1.48e+04    \n",
      "      65             81         4.9023e+03      5.68e-03       4.13e-07       1.74e+04    \n",
      "      66             82         4.9023e+03      5.24e-03       4.13e-07       1.32e+04    \n",
      "      67             83         4.9022e+03      5.20e-03       4.13e-07       1.81e+04    \n",
      "      68             84         4.9022e+03      5.41e-03       4.13e-07       1.38e+04    \n",
      "      69             85         4.9022e+03      5.36e-03       4.13e-07       1.78e+04    \n",
      "      70             86         4.9022e+03      5.27e-03       4.13e-07       1.36e+04    \n",
      "      71             87         4.9022e+03      5.35e-03       4.13e-07       1.79e+04    \n",
      "      72             88         4.9022e+03      5.30e-03       4.13e-07       1.48e+04    \n",
      "      73             89         4.9022e+03      5.67e-03       4.13e-07       1.61e+04    \n",
      "      74             90         4.9022e+03      4.67e-03       4.13e-07       1.46e+04    \n",
      "      75             91         4.9022e+03      5.68e-03       4.13e-07       1.67e+04    \n",
      "      76             92         4.9022e+03      5.18e-03       4.13e-07       1.48e+04    \n",
      "      77             93         4.9022e+03      5.59e-03       4.13e-07       1.77e+04    \n",
      "      78             94         4.9022e+03      5.30e-03       4.13e-07       1.29e+04    \n",
      "      79             95         4.9022e+03      5.14e-03       4.13e-07       1.70e+04    \n",
      "      80             96         4.9022e+03      4.88e-03       4.13e-07       1.42e+04    \n",
      "      81             97         4.9022e+03      5.56e-03       4.13e-07       1.79e+04    \n",
      "      82             98         4.9022e+03      5.37e-03       4.13e-07       1.49e+04    \n",
      "      83             99         4.9022e+03      5.81e-03       4.13e-07       1.57e+04    \n",
      "      84             100        4.9022e+03      4.66e-03       4.13e-07       1.47e+04    \n",
      "      85             101        4.9022e+03      5.68e-03       4.13e-07       1.70e+04    \n",
      "      86             102        4.9021e+03      5.00e-03       4.13e-07       1.36e+04    \n",
      "      87             103        4.9021e+03      5.29e-03       4.13e-07       1.70e+04    \n",
      "      88             104        4.9021e+03      4.97e-03       4.13e-07       1.41e+04    \n",
      "      89             105        4.9021e+03      5.54e-03       4.13e-07       1.77e+04    \n",
      "      90             106        4.9021e+03      5.32e-03       4.13e-07       1.49e+04    \n",
      "      91             107        4.9021e+03      5.67e-03       4.13e-07       1.67e+04    \n",
      "      92             108        4.9021e+03      5.07e-03       4.13e-07       1.37e+04    \n",
      "      93             109        4.9021e+03      5.22e-03       4.13e-07       1.83e+04    \n",
      "      94             110        4.9021e+03      5.35e-03       4.13e-07       1.38e+04    \n",
      "      95             111        4.9021e+03      5.37e-03       4.13e-07       1.81e+04    \n",
      "      96             112        4.9021e+03      5.25e-03       4.13e-07       1.47e+04    \n",
      "      97             113        4.9021e+03      5.66e-03       4.13e-07       1.75e+04    \n",
      "      98             114        4.9021e+03      5.29e-03       4.13e-07       1.49e+04    \n",
      "      99             115        4.9021e+03      5.68e-03       4.13e-07       1.65e+04    \n",
      "      100            116        4.9021e+03      5.01e-03       4.13e-07       1.47e+04    \n",
      "      101            117        4.9021e+03      5.71e-03       4.13e-07       1.82e+04    \n",
      "      102            118        4.9021e+03      5.48e-03       4.13e-07       1.47e+04    \n",
      "      103            119        4.9021e+03      5.60e-03       4.13e-07       1.69e+04    \n",
      "      104            120        4.9020e+03      5.16e-03       4.13e-07       1.50e+04    \n",
      "      105            121        4.9020e+03      5.66e-03       4.13e-07       1.66e+04    \n",
      "      106            122        4.9020e+03      4.76e-03       4.13e-07       1.44e+04    \n",
      "      107            123        4.9020e+03      5.33e-03       4.13e-07       1.67e+04    \n",
      "      108            124        4.9020e+03      4.79e-03       4.13e-07       1.43e+04    \n",
      "      109            125        4.9020e+03      5.49e-03       4.13e-07       1.79e+04    \n",
      "      110            126        4.9020e+03      5.34e-03       4.13e-07       1.46e+04    \n",
      "      111            127        4.9020e+03      5.57e-03       4.13e-07       1.80e+04    \n",
      "      112            128        4.9020e+03      5.42e-03       4.13e-07       1.47e+04    \n",
      "      113            129        4.9020e+03      5.55e-03       4.13e-07       1.80e+04    \n",
      "      114            130        4.9020e+03      5.31e-03       4.13e-07       1.34e+04    \n",
      "      115            131        4.9020e+03      5.27e-03       4.13e-07       1.78e+04    \n",
      "      116            132        4.9020e+03      5.23e-03       4.13e-07       1.38e+04    \n",
      "      117            133        4.9020e+03      5.38e-03       4.13e-07       1.83e+04    \n",
      "      118            134        4.9020e+03      5.50e-03       4.13e-07       1.51e+04    \n",
      "      119            135        4.9020e+03      5.70e-03       4.13e-07       1.66e+04    \n",
      "      120            136        4.9020e+03      5.17e-03       4.13e-07       1.37e+04    \n",
      "      121            137        4.9020e+03      5.32e-03       4.13e-07       1.83e+04    \n",
      "      122            138        4.9020e+03      5.48e-03       4.13e-07       1.49e+04    \n",
      "      123            139        4.9019e+03      5.68e-03       4.13e-07       1.81e+04    \n",
      "      124            140        4.9019e+03      5.50e-03       4.13e-07       1.46e+04    \n",
      "      125            141        4.9019e+03      5.49e-03       4.13e-07       1.81e+04    \n",
      "      126            142        4.9019e+03      5.40e-03       4.13e-07       1.46e+04    \n",
      "      127            143        4.9019e+03      5.50e-03       4.13e-07       1.70e+04    \n",
      "      128            144        4.9019e+03      5.25e-03       4.13e-07       1.42e+04    \n",
      "      129            145        4.9019e+03      5.08e-03       4.13e-07       1.70e+04    \n",
      "      130            146        4.9019e+03      4.99e-03       4.13e-07       1.41e+04    \n",
      "      131            147        4.9019e+03      5.42e-03       4.13e-07       1.79e+04    \n",
      "      132            148        4.9019e+03      5.24e-03       4.13e-07       1.35e+04    \n",
      "      133            149        4.9019e+03      5.33e-03       4.13e-07       1.78e+04    \n",
      "      134            150        4.9019e+03      5.26e-03       4.13e-07       1.34e+04    \n",
      "      135            151        4.9019e+03      5.26e-03       4.13e-07       1.82e+04    \n",
      "      136            152        4.9019e+03      5.29e-03       4.13e-07       1.49e+04    \n",
      "      137            153        4.9019e+03      5.68e-03       4.13e-07       1.65e+04    \n",
      "      138            154        4.9019e+03      4.77e-03       4.13e-07       1.45e+04    \n",
      "      139            155        4.9019e+03      5.64e-03       4.13e-07       1.68e+04    \n",
      "      140            156        4.9019e+03      5.29e-03       4.13e-07       1.48e+04    \n",
      "      141            157        4.9019e+03      5.62e-03       4.13e-07       1.77e+04    \n",
      "      142            158        4.9018e+03      5.38e-03       4.13e-07       1.39e+04    \n",
      "      143            159        4.9018e+03      5.10e-03       4.13e-07       1.70e+04    \n",
      "      144            160        4.9018e+03      4.85e-03       4.13e-07       1.43e+04    \n",
      "      145            161        4.9018e+03      5.63e-03       4.13e-07       1.77e+04    \n",
      "      146            162        4.9018e+03      5.31e-03       4.13e-07       1.36e+04    \n",
      "      147            163        4.9018e+03      5.31e-03       4.13e-07       1.82e+04    \n",
      "      148            164        4.9018e+03      5.26e-03       4.13e-07       1.48e+04    \n",
      "      149            165        4.9018e+03      5.65e-03       4.13e-07       1.62e+04    \n",
      "      150            166        4.9018e+03      4.68e-03       4.13e-07       1.45e+04    \n",
      "      151            167        4.9018e+03      5.58e-03       4.13e-07       1.79e+04    \n",
      "      152            168        4.9018e+03      5.39e-03       4.13e-07       1.39e+04    \n",
      "      153            169        4.9018e+03      5.47e-03       4.13e-07       1.70e+04    \n",
      "      154            170        4.9018e+03      5.14e-03       4.13e-07       1.49e+04    \n",
      "      155            171        4.9018e+03      5.62e-03       4.13e-07       1.74e+04    \n",
      "      156            172        4.9018e+03      5.20e-03       4.13e-07       1.41e+04    \n",
      "      157            173        4.9018e+03      5.45e-03       4.13e-07       1.69e+04    \n",
      "      158            174        4.9018e+03      5.16e-03       4.13e-07       1.36e+04    \n",
      "      159            175        4.9018e+03      5.33e-03       4.13e-07       1.78e+04    \n",
      "      160            176        4.9018e+03      5.26e-03       4.13e-07       1.44e+04    \n",
      "      161            177        4.9017e+03      5.58e-03       4.13e-07       1.55e+04    \n",
      "      162            178        4.9017e+03      4.44e-03       4.13e-07       1.50e+04    \n",
      "      163            179        4.9017e+03      5.43e-03       4.13e-07       1.81e+04    \n",
      "      164            180        4.9017e+03      5.38e-03       4.13e-07       1.46e+04    \n",
      "      165            181        4.9017e+03      5.60e-03       4.13e-07       1.68e+04    \n",
      "      166            182        4.9017e+03      5.09e-03       4.13e-07       1.47e+04    \n",
      "      167            183        4.9017e+03      5.60e-03       4.13e-07       1.74e+04    \n",
      "      168            184        4.9017e+03      5.29e-03       4.13e-07       1.36e+04    \n",
      "      169            185        4.9017e+03      5.48e-03       4.13e-07       1.72e+04    \n",
      "      170            186        4.9017e+03      4.97e-03       4.13e-07       1.45e+04    \n",
      "      171            187        4.9017e+03      5.60e-03       4.13e-07       1.75e+04    \n",
      "      172            188        4.9017e+03      5.37e-03       4.13e-07       1.49e+04    \n",
      "      173            189        4.9017e+03      5.74e-03       4.13e-07       1.57e+04    \n",
      "      174            190        4.9017e+03      4.26e-03       4.13e-07       1.56e+04    \n",
      "      175            191        4.9017e+03      5.24e-03       4.13e-07       1.73e+04    \n",
      "      176            192        4.9017e+03      5.17e-03       4.13e-07       1.36e+04    \n",
      "      177            193        4.9017e+03      5.37e-03       4.13e-07       1.80e+04    \n",
      "      178            194        4.9017e+03      5.34e-03       4.13e-07       1.27e+04    \n",
      "      179            195        4.9017e+03      5.05e-03       4.13e-07       1.78e+04    \n",
      "      180            196        4.9016e+03      5.34e-03       4.13e-07       1.57e+04    \n",
      "      181            197        4.9016e+03      5.83e-03       4.13e-07       1.79e+04    \n",
      "      182            198        4.9016e+03      5.44e-03       4.13e-07       1.39e+04    \n",
      "      183            199        4.9016e+03      5.48e-03       4.13e-07       1.79e+04    \n",
      "      184            200        4.9016e+03      5.28e-03       4.13e-07       1.39e+04    \n",
      "      185            201        4.9016e+03      5.40e-03       4.13e-07       1.84e+04    \n",
      "      186            202        4.9016e+03      5.39e-03       4.13e-07       1.48e+04    \n",
      "      187            203        4.9016e+03      5.68e-03       4.13e-07       1.81e+04    \n",
      "      188            204        4.9016e+03      5.46e-03       4.13e-07       1.47e+04    \n",
      "      189            205        4.9016e+03      5.61e-03       4.13e-07       1.82e+04    \n",
      "      190            206        4.9016e+03      5.46e-03       4.13e-07       1.48e+04    \n",
      "      191            207        4.9016e+03      5.62e-03       4.13e-07       1.65e+04    \n",
      "      192            208        4.9016e+03      5.13e-03       4.13e-07       1.47e+04    \n",
      "      193            209        4.9016e+03      5.57e-03       4.13e-07       1.81e+04    \n",
      "      194            210        4.9016e+03      5.49e-03       4.13e-07       1.33e+04    \n",
      "      195            211        4.9016e+03      5.19e-03       4.13e-07       1.67e+04    \n",
      "      196            212        4.9016e+03      4.79e-03       4.13e-07       1.43e+04    \n",
      "      197            213        4.9016e+03      5.29e-03       4.13e-07       1.82e+04    \n",
      "      198            214        4.9015e+03      5.28e-03       4.13e-07       1.38e+04    \n",
      "      199            215        4.9015e+03      5.39e-03       4.13e-07       1.81e+04    \n",
      "      200            216        4.9015e+03      5.30e-03       4.13e-07       1.50e+04    \n",
      "      201            217        4.9015e+03      5.68e-03       4.13e-07       1.78e+04    \n",
      "      202            218        4.9015e+03      5.30e-03       4.13e-07       1.48e+04    \n",
      "      203            219        4.9015e+03      5.72e-03       4.13e-07       1.66e+04    \n",
      "      204            220        4.9015e+03      4.76e-03       4.13e-07       1.43e+04    \n",
      "      205            221        4.9015e+03      5.36e-03       4.13e-07       1.78e+04    \n",
      "      206            222        4.9015e+03      5.28e-03       4.13e-07       1.27e+04    \n",
      "      207            223        4.9015e+03      5.12e-03       4.13e-07       1.73e+04    \n",
      "      208            224        4.9015e+03      5.16e-03       4.13e-07       1.47e+04    \n",
      "      209            225        4.9015e+03      5.63e-03       4.13e-07       1.68e+04    \n",
      "      210            226        4.9015e+03      4.91e-03       4.13e-07       1.52e+04    \n",
      "      211            227        4.9015e+03      5.78e-03       4.13e-07       1.79e+04    \n",
      "      212            228        4.9015e+03      5.33e-03       4.13e-07       1.50e+04    \n",
      "      213            229        4.9015e+03      5.63e-03       4.13e-07       1.66e+04    \n",
      "      214            230        4.9015e+03      4.89e-03       4.13e-07       1.40e+04    \n",
      "      215            231        4.9015e+03      5.25e-03       4.13e-07       1.66e+04    \n",
      "      216            232        4.9015e+03      5.08e-03       4.13e-07       1.47e+04    \n",
      "      217            233        4.9014e+03      5.60e-03       4.13e-07       1.67e+04    \n",
      "      218            234        4.9014e+03      4.77e-03       4.13e-07       1.45e+04    \n",
      "      219            235        4.9014e+03      5.42e-03       4.13e-07       1.62e+04    \n",
      "      220            236        4.9014e+03      4.76e-03       4.13e-07       1.46e+04    \n",
      "      221            237        4.9014e+03      5.21e-03       4.13e-07       1.62e+04    \n",
      "      222            238        4.9014e+03      4.32e-03       4.13e-07       1.52e+04    \n",
      "      223            239        4.9014e+03      5.41e-03       4.13e-07       1.69e+04    \n",
      "      224            240        4.9014e+03      4.90e-03       4.13e-07       1.44e+04    \n",
      "      225            241        4.9014e+03      5.62e-03       4.13e-07       1.77e+04    \n",
      "      226            242        4.9014e+03      5.25e-03       4.13e-07       1.41e+04    \n",
      "      227            243        4.9014e+03      5.45e-03       4.13e-07       1.80e+04    \n",
      "      228            244        4.9014e+03      5.37e-03       4.13e-07       1.48e+04    \n",
      "      229            245        4.9014e+03      5.60e-03       4.13e-07       1.58e+04    \n",
      "      230            246        4.9014e+03      4.63e-03       4.13e-07       1.48e+04    \n",
      "      231            247        4.9014e+03      5.35e-03       4.13e-07       1.79e+04    \n",
      "      232            248        4.9014e+03      5.39e-03       4.13e-07       1.28e+04    \n",
      "      233            249        4.9014e+03      5.08e-03       4.13e-07       1.71e+04    \n",
      "      234            250        4.9014e+03      4.87e-03       4.13e-07       1.42e+04    \n",
      "      235            251        4.9014e+03      5.42e-03       4.13e-07       1.82e+04    \n",
      "      236            252        4.9013e+03      5.31e-03       4.13e-07       1.48e+04    \n",
      "      237            253        4.9013e+03      5.64e-03       4.13e-07       1.80e+04    \n",
      "      238            254        4.9013e+03      5.43e-03       4.13e-07       1.47e+04    \n",
      "      239            255        4.9013e+03      5.59e-03       4.13e-07       1.70e+04    \n",
      "      240            256        4.9013e+03      4.99e-03       4.13e-07       1.44e+04    \n",
      "      241            257        4.9013e+03      5.54e-03       4.13e-07       1.67e+04    \n",
      "      242            258        4.9013e+03      5.09e-03       4.13e-07       1.43e+04    \n",
      "      243            259        4.9013e+03      5.64e-03       4.13e-07       1.77e+04    \n",
      "      244            260        4.9013e+03      5.33e-03       4.13e-07       1.49e+04    \n",
      "      245            261        4.9013e+03      5.63e-03       4.13e-07       1.74e+04    \n",
      "      246            262        4.9013e+03      5.23e-03       4.13e-07       1.32e+04    \n",
      "      247            263        4.9013e+03      5.22e-03       4.13e-07       1.82e+04    \n",
      "      248            264        4.9013e+03      5.34e-03       4.13e-07       1.50e+04    \n",
      "      249            265        4.9013e+03      5.78e-03       4.13e-07       1.79e+04    \n",
      "      250            266        4.9013e+03      5.43e-03       4.13e-07       1.57e+04    \n",
      "      251            267        4.9013e+03      5.89e-03       4.13e-07       1.82e+04    \n",
      "      252            268        4.9013e+03      5.48e-03       4.13e-07       1.39e+04    \n",
      "      253            269        4.9013e+03      5.36e-03       4.13e-07       1.59e+04    \n",
      "      254            270        4.9013e+03      4.90e-03       4.13e-07       1.49e+04    \n",
      "      255            271        4.9012e+03      5.66e-03       4.13e-07       1.79e+04    \n",
      "      256            272        4.9012e+03      5.31e-03       4.13e-07       1.46e+04    \n",
      "      257            273        4.9012e+03      5.61e-03       4.13e-07       1.60e+04    \n",
      "      258            274        4.9012e+03      4.74e-03       4.13e-07       1.45e+04    \n",
      "      259            275        4.9012e+03      5.30e-03       4.13e-07       1.79e+04    \n",
      "      260            276        4.9012e+03      5.27e-03       4.13e-07       1.40e+04    \n",
      "      261            277        4.9012e+03      5.02e-03       4.13e-07       1.72e+04    \n",
      "      262            278        4.9012e+03      4.96e-03       4.13e-07       1.44e+04    \n",
      "      263            279        4.9012e+03      5.66e-03       4.13e-07       1.80e+04    \n",
      "      264            280        4.9012e+03      5.46e-03       4.13e-07       1.50e+04    \n",
      "      265            281        4.9012e+03      5.83e-03       4.13e-07       1.69e+04    \n",
      "      266            282        4.9012e+03      5.25e-03       4.13e-07       1.44e+04    \n",
      "      267            283        4.9012e+03      5.59e-03       4.13e-07       1.70e+04    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# jac=gradient,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m p0\u001b[38;5;241m.\u001b[39mloglike(result\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:941\u001b[0m, in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m     result \u001b[38;5;241m=\u001b[39m call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\n\u001b[1;32m    938\u001b[0m                           max_nfev, x_scale, diff_step)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 941\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdogbox\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tr_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsmr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularize\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tr_options:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/trf.py:119\u001b[0m, in \u001b[0;36mtrf\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrf\u001b[39m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[1;32m    113\u001b[0m         loss_function, tr_solver, tr_options, verbose):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# For efficiency, it makes sense to run the simplified version of the\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# algorithm when no bounds are imposed. We decided to write the two\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# separate functions. It violates the DRY principle, but the individual\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# functions are kept the most readable.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(lb \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(ub \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrf_no_bounds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trf_bounds(\n\u001b[1;32m    124\u001b[0m             fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[1;32m    125\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/trf.py:536\u001b[0m, in \u001b[0;36mtrf_no_bounds\u001b[0;34m(fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    532\u001b[0m f_true \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    534\u001b[0m cost \u001b[38;5;241m=\u001b[39m cost_new\n\u001b[0;32m--> 536\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mjac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m njev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:898\u001b[0m, in \u001b[0;36mleast_squares.<locals>.jac_wrapped\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjac_wrapped\u001b[39m(x, f):\n\u001b[0;32m--> 898\u001b[0m     J \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac_sparsity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m J\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# J is guaranteed not sparse.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m         J \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(J)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:519\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:592\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    590\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[1;32m    591\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:470\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    468\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 470\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/numpy/_core/shape_base.py:64\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mConvert inputs to arrays with at least one dimension.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    # jac=gradient,\n",
    "    x0=x1,\n",
    "    verbose=2,\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ -415941.92979457,  -739901.15981344,  -648959.72258677,\n",
       "        -721214.59992895, -1277773.22303356, -1121110.99923105],      dtype=float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacfwd(p0.loglike)(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/runpy.py:198\u001b[0m, in \u001b[0;36m_run_module_as_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m mod_spec\u001b[38;5;241m.\u001b[39morigin\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_code(code, main_globals, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod_spec)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/runpy.py:88\u001b[0m, in \u001b[0;36m_run_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m run_globals\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m mod_name,\n\u001b[1;32m     82\u001b[0m                    \u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m fname,\n\u001b[1;32m     83\u001b[0m                    __cached__ \u001b[38;5;241m=\u001b[39m cached,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m                    __package__ \u001b[38;5;241m=\u001b[39m pkg_name,\n\u001b[1;32m     87\u001b[0m                    __spec__ \u001b[38;5;241m=\u001b[39m mod_spec)\n\u001b[0;32m---> 88\u001b[0m exec(code, run_globals)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_globals\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel_launcher.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipykernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[0;32m---> 18\u001b[0m app\u001b[38;5;241m.\u001b[39mlaunch_new_instance()\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/traitlets/config/application.py:1075\u001b[0m, in \u001b[0;36mlaunch_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1074\u001b[0m app\u001b[38;5;241m.\u001b[39minitialize(argv)\n\u001b[0;32m-> 1075\u001b[0m app\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelapp.py:739\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_loop\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/tornado/platform/asyncio.py:205\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masyncio_loop\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:679\u001b[0m, in \u001b[0;36mrun_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:2027\u001b[0m, in \u001b[0;36m_run_once\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2027\u001b[0m         handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m   2028\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/events.py:89\u001b[0m, in \u001b[0;36m_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:545\u001b[0m, in \u001b[0;36mdispatch_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_one()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:534\u001b[0m, in \u001b[0;36mprocess_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m dispatch(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:437\u001b[0m, in \u001b[0;36mdispatch_shell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m--> 437\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/ipkernel.py:362\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_associate_new_top_level_threads_with(parent_header)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mexecute_request(stream, ident, parent)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:778\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(reply_content):\n\u001b[0;32m--> 778\u001b[0m     reply_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# Flush output before sending the reply.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/ipkernel.py:449\u001b[0m, in \u001b[0;36mdo_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 449\u001b[0m     res \u001b[38;5;241m=\u001b[39m shell\u001b[38;5;241m.\u001b[39mrun_cell(\n\u001b[1;32m    450\u001b[0m         code,\n\u001b[1;32m    451\u001b[0m         store_history\u001b[38;5;241m=\u001b[39mstore_history,\n\u001b[1;32m    452\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[1;32m    453\u001b[0m         cell_id\u001b[38;5;241m=\u001b[39mcell_id,\n\u001b[1;32m    454\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/zmqshell.py:549\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun_cell(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3075\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3075\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_cell(\n\u001b[1;32m   3076\u001b[0m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[1;32m   3077\u001b[0m     )\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3130\u001b[0m, in \u001b[0;36m_run_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3130\u001b[0m     result \u001b[38;5;241m=\u001b[39m runner(coro)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3334\u001b[0m, in \u001b[0;36mrun_cell_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3331\u001b[0m interactivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mast_node_interactivity\n\u001b[0;32m-> 3334\u001b[0m has_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_ast_nodes(code_ast\u001b[38;5;241m.\u001b[39mbody, cell_name,\n\u001b[1;32m   3335\u001b[0m        interactivity\u001b[38;5;241m=\u001b[39minteractivity, compiler\u001b[38;5;241m=\u001b[39mcompiler, result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_execution_succeeded \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3517\u001b[0m, in \u001b[0;36mrun_ast_nodes\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3516\u001b[0m     asy \u001b[38;5;241m=\u001b[39m compare(code)\n\u001b[0;32m-> 3517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_code(code, result, async_\u001b[38;5;241m=\u001b[39masy):\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m, in \u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3577\u001b[0m         exec(code_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_global_ns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3579\u001b[0m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      4\u001b[0m perturbed\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39mkm\u001b[38;5;241m.\u001b[39mto(u\u001b[38;5;241m.\u001b[39mau)\n\u001b[0;32m----> 5\u001b[0m ll \u001b[38;5;241m=\u001b[39m p0\u001b[38;5;241m.\u001b[39mloglike(jnp\u001b[38;5;241m.\u001b[39mconcatenate([perturbed\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mflatten(), perturbed\u001b[38;5;241m.\u001b[39mv\u001b[38;5;241m.\u001b[39mflatten()]))\n\u001b[1;32m      6\u001b[0m ll\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:157\u001b[0m, in \u001b[0;36m_loglike\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m state \u001b[38;5;241m=\u001b[39m CartesianState(x\u001b[38;5;241m=\u001b[39mx[:\u001b[38;5;241m3\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m,:], v\u001b[38;5;241m=\u001b[39mx[\u001b[38;5;241m3\u001b[39m:][\u001b[38;5;28;01mNone\u001b[39;00m,:], time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time)\n\u001b[0;32m--> 157\u001b[0m ras, decs \u001b[38;5;241m=\u001b[39m _ephem(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mtimes,\n\u001b[1;32m    159\u001b[0m     state,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integrator,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integrator_state,\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mobserver_positions,\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    166\u001b[0m xis_etas \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(tangent_plane_projection)(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mra, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mdec, ras, decs\n\u001b[1;32m    168\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:290\u001b[0m, in \u001b[0;36m_ephem\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (ra, dec)\n\u001b[0;32m--> 290\u001b[0m _, (ras, decs) \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    291\u001b[0m     scan_func,\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    293\u001b[0m     (positions[:, \u001b[38;5;241m0\u001b[39m, :], velocities[:, \u001b[38;5;241m0\u001b[39m, :], times, observer_positions),\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ras, decs\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:287\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m position, velocity, time, observer_position \u001b[38;5;241m=\u001b[39m scan_over\n\u001b[0;32m--> 287\u001b[0m ra, dec \u001b[38;5;241m=\u001b[39m on_sky(position, velocity, time, observer_position, acc_func)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (ra, dec)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/astrometry/sky_projection.py:103\u001b[0m, in \u001b[0;36mon_sky\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_system_state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m xz, _ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    104\u001b[0m     scan_func,\n\u001b[1;32m    105\u001b[0m     state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m     length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m X \u001b[38;5;241m=\u001b[39m xz \u001b[38;5;241m-\u001b[39m observer_position\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/astrometry/sky_projection.py:93\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m light_travel_time \u001b[38;5;241m=\u001b[39m earth_distance \u001b[38;5;241m*\u001b[39m INV_SPEED_OF_LIGHT\n\u001b[1;32m     92\u001b[0m positions, velocities, final_system_state, final_integrator_state \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 93\u001b[0m     ias15_evolve(\n\u001b[1;32m     94\u001b[0m         state,\n\u001b[1;32m     95\u001b[0m         acc_func,\n\u001b[1;32m     96\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray([state\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m-\u001b[39m light_travel_time]),\n\u001b[1;32m     97\u001b[0m         initial_integrator_state,\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_system_state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1251\u001b[0m, in \u001b[0;36mias15_evolve\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (system_state, integrator_state), (\n\u001b[1;32m   1236\u001b[0m         jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m   1237\u001b[0m             (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         ),\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[1;32m   1250\u001b[0m (final_system_state, final_integrator_state), (positions, velocities) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1251\u001b[0m     jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(scan_func, (initial_system_state, initial_integrator_state), times)\n\u001b[1;32m   1252\u001b[0m )\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m positions, velocities, final_system_state, final_integrator_state\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1232\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1231\u001b[0m final_time \u001b[38;5;241m=\u001b[39m scan_over\n\u001b[0;32m-> 1232\u001b[0m system_state, integrator_state \u001b[38;5;241m=\u001b[39m evolve(\n\u001b[1;32m   1233\u001b[0m     system_state, acceleration_func, final_time, integrator_state\n\u001b[1;32m   1234\u001b[0m )\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (system_state, integrator_state), (\n\u001b[1;32m   1236\u001b[0m     jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m   1237\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     ),\n\u001b[1;32m   1248\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1212\u001b[0m, in \u001b[0;36mevolve\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (step_length \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (iter_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10_000\u001b[39m)\n\u001b[1;32m   1211\u001b[0m final_system_state, final_integrator_state, last_meaningful_dt, iter_num \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1212\u001b[0m     jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mwhile_loop(\n\u001b[1;32m   1213\u001b[0m         cond_func,\n\u001b[1;32m   1214\u001b[0m         step_needed,\n\u001b[1;32m   1215\u001b[0m         (\n\u001b[1;32m   1216\u001b[0m             initial_system_state,\n\u001b[1;32m   1217\u001b[0m             initial_integrator_state,\n\u001b[1;32m   1218\u001b[0m             initial_integrator_state\u001b[38;5;241m.\u001b[39mdt,\n\u001b[1;32m   1219\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1220\u001b[0m         ),\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1224\u001b[0m final_integrator_state\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m last_meaningful_dt\n",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m: ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 72 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/jax/_src/lax/control_flow/loops.py:1657\u001b[0m, in \u001b[0;36m_while_transpose_error\u001b[0;34m(*_, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_while_transpose_error\u001b[39m(\u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1657\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReverse-mode differentiation does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlax.while_loop or lax.fori_loop with dynamic start/stop values. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1659\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry using lax.scan, or using fori_loop with static start/stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop."
     ]
    }
   ],
   "source": [
    "jax.grad(p0.loglike)(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_23957/1719285525.py:47: OptimizeWarning: Unknown solver options: ftol\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 16.556969\n",
      "         Iterations: 4\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cassese/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_minimize.py:726: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "x0 = jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def objective(x):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def gradient(x):\n",
    "    return -jax.jacfwd(p0.loglike)(x)\n",
    "\n",
    "\n",
    "def scale_decorator(func, gradient_func, x0):\n",
    "    # Determine parameter scales based on initial values\n",
    "    param_scales = jnp.where(jnp.abs(x0) < 1e-10, 1.0, jnp.abs(x0))\n",
    "\n",
    "    # Compute gradient at initial point to determine gradient scales\n",
    "    initial_grad = gradient_func(x0)\n",
    "    grad_scales = jnp.where(jnp.abs(initial_grad) < 1e-10, 1.0, jnp.abs(initial_grad))\n",
    "\n",
    "    def scaled_func(x_scaled):\n",
    "        x_unscaled = x_scaled * param_scales\n",
    "        return func(x_unscaled)\n",
    "\n",
    "    def scaled_gradient(x_scaled):\n",
    "        x_unscaled = x_scaled * param_scales\n",
    "        raw_gradient = gradient_func(x_unscaled)\n",
    "        return (raw_gradient * param_scales) / grad_scales\n",
    "\n",
    "    x0_scaled = x0 / param_scales\n",
    "\n",
    "    def unscale_result(result):\n",
    "        \"\"\"Helper to unscale optimization result\"\"\"\n",
    "        unscaled_result = result\n",
    "        unscaled_result.x = result.x * param_scales\n",
    "        unscaled_result.jac = result.jac * grad_scales / param_scales\n",
    "        return unscaled_result\n",
    "\n",
    "    return (\n",
    "        scaled_func,\n",
    "        scaled_gradient,\n",
    "        x0_scaled,\n",
    "        unscale_result,\n",
    "        param_scales,\n",
    "        grad_scales,\n",
    "    )\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "scaled_obj, scaled_grad, x0_scaled, unscale_result, param_scales, grad_scales = (\n",
    "    scale_decorator(objective, gradient, x0)\n",
    ")\n",
    "\n",
    "# Run optimizer with scaled functions\n",
    "result = minimize(\n",
    "    fun=scaled_obj,\n",
    "    x0=x0_scaled,\n",
    "    jac=scaled_grad,\n",
    "    method=\"BFGS\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"ftol\": 1e-8,\n",
    "        \"gtol\": 1e-8,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Unscale the result\n",
    "final_result = unscale_result(result)\n",
    "\n",
    "# result = minimize(\n",
    "#     fun=scaled_obj,\n",
    "#     x0=scaled_x0,\n",
    "#     jac=scaled_grad,\n",
    "#     method='L-BFGS-B',\n",
    "#     options={\"disp\": True, \"ftol\": 1e-8, \"gtol\": 1e-8,},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_23957/1958235744.py:2: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.25731D+01    |proj g|=  2.00572D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  1.88198D+01    |proj g|=  8.44737D-01\n",
      "\n",
      "At iterate    2    f=  1.79691D+01    |proj g|=  4.14018D-01\n",
      "\n",
      "At iterate    3    f=  1.76637D+01    |proj g|=  3.67542D-01\n",
      "\n",
      "At iterate    4    f=  1.68003D+01    |proj g|=  4.31605D-01\n",
      "\n",
      "At iterate    5    f=  1.65988D+01    |proj g|=  2.21770D-01\n",
      "\n",
      "At iterate    6    f=  1.65458D+01    |proj g|=  2.39457D-02\n",
      "\n",
      "At iterate    7    f=  1.65413D+01    |proj g|=  1.41109D-02\n",
      "\n",
      "At iterate    8    f=  1.65409D+01    |proj g|=  2.58912D-03\n",
      "\n",
      "At iterate    9    f=  1.65409D+01    |proj g|=  4.25874D-04\n",
      "\n",
      "At iterate   10    f=  1.65409D+01    |proj g|=  1.07829D-04\n",
      "\n",
      "At iterate   11    f=  1.65409D+01    |proj g|=  1.68624D-05\n",
      "\n",
      "At iterate   12    f=  1.65409D+01    |proj g|=  1.34610D-05\n",
      "\n",
      "At iterate   13    f=  1.65409D+01    |proj g|=  2.09287D-04\n",
      "  ys=-9.005E-14  -gs= 1.468E-13 BFGS update SKIPPED\n",
      "\n",
      "At iterate   14    f=  1.65409D+01    |proj g|=  4.41940D-05\n",
      "\n",
      "At iterate   15    f=  1.65409D+01    |proj g|=  4.18094D-05\n",
      "\n",
      "At iterate   16    f=  1.65409D+01    |proj g|=  1.02679D-05\n",
      "\n",
      "At iterate   17    f=  1.65409D+01    |proj g|=  2.35160D-05\n",
      "\n",
      "At iterate   18    f=  1.65409D+01    |proj g|=  6.18211D-05\n",
      "\n",
      "At iterate   19    f=  1.65409D+01    |proj g|=  6.18211D-05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     19     96      1     1     0   6.182D-05   1.654D+01\n",
      "  F =   16.540901896326066     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning:  more than 10 function and gradient\n",
      "   evaluations in the last line search.  Termination\n",
      "   may possibly be caused by a bad search direction.\n"
     ]
    }
   ],
   "source": [
    "# Run optimizer with scaled functions\n",
    "result = minimize(\n",
    "    fun=scaled_obj,\n",
    "    x0=x0_scaled,\n",
    "    jac=scaled_grad,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")\n",
    "\n",
    "# Unscale the result\n",
    "final_result = unscale_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.97e+07    \n",
      "       1             11         2.1755e+02      3.72e+01       9.34e-06       2.48e+07    \n",
      "       2             12         1.7156e+02      4.60e+01       2.34e-06       1.03e+07    \n",
      "       3             13         1.6787e+02      3.69e+00       4.67e-06       1.13e+07    \n",
      "       4             14         1.5527e+02      1.26e+01       1.17e-06       5.48e+06    \n",
      "       5             15         1.4912e+02      6.16e+00       2.34e-06       4.27e+06    \n",
      "       6             16         1.4661e+02      2.51e+00       2.34e-06       4.82e+06    \n",
      "       7             17         1.4370e+02      2.91e+00       5.84e-07       2.32e+06    \n",
      "       8             18         1.4163e+02      2.07e+00       1.17e-06       1.97e+06    \n",
      "       9             19         1.4023e+02      1.41e+00       1.17e-06       1.84e+06    \n",
      "      10             20         1.3944e+02      7.83e-01       1.17e-06       2.52e+06    \n",
      "      11             21         1.3874e+02      7.02e-01       2.92e-07       1.31e+06    \n",
      "      12             22         1.3817e+02      5.73e-01       5.84e-07       1.05e+06    \n",
      "      13             23         1.3777e+02      4.02e-01       5.84e-07       1.02e+06    \n",
      "      14             24         1.3752e+02      2.51e-01       5.84e-07       1.06e+06    \n",
      "      15             25         1.3737e+02      1.47e-01       5.84e-07       1.27e+06    \n",
      "      16             26         1.3719e+02      1.74e-01       1.46e-07       6.51e+05    \n",
      "      17             27         1.3707e+02      1.20e-01       2.92e-07       4.64e+05    \n",
      "      18             28         1.3700e+02      7.02e-02       2.92e-07       5.67e+05    \n",
      "      19             29         1.3695e+02      5.05e-02       2.92e-07       5.67e+05    \n",
      "      20             30         1.3691e+02      4.17e-02       7.30e-08       2.96e+05    \n",
      "      21             31         1.3688e+02      3.04e-02       1.46e-07       2.53e+05    \n",
      "      22             32         1.3686e+02      2.50e-02       1.46e-07       2.29e+05    \n",
      "      23             33         1.3685e+02      1.10e-02       1.46e-07       2.99e+05    \n",
      "      24             34         1.3683e+02      1.15e-02       3.65e-08       1.61e+05    \n",
      "      25             35         1.3682e+02      1.03e-02       7.30e-08       1.39e+05    \n",
      "      26             36         1.3682e+02      5.96e-03       7.30e-08       1.14e+05    \n",
      "      27             37         1.3681e+02      5.33e-03       7.30e-08       1.36e+05    \n",
      "      28             38         1.3681e+02      1.28e-03       7.30e-08       1.47e+05    \n",
      "      29             39         1.3681e+02      3.02e-03       1.83e-08       7.42e+04    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 2.5477e+02, final cost 1.3681e+02, first-order optimality 7.42e+04.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=scaled_obj,\n",
    "    # jac=scaled_grad,\n",
    "    x0=x0_scaled,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeplerianState(semi=Array([2.37860178], dtype=float64), ecc=Array([0.14924396], dtype=float64), inc=Array([6.73363883], dtype=float64), Omega=Array([183.37296835], dtype=float64), omega=Array([140.26403582], dtype=float64), nu=Array([173.65444973], dtype=float64), time=<Time object: scale='utc' format='jd' value=2460676.791666667>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = unscale_result(result).x\n",
    "c = CartesianState(x=x[:3][None, :], v=x[3:][None, :], time=times[0])\n",
    "c.to_keplerian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.00572658,  1.77860293,  0.51974103, -0.00665991, -0.00662871,\n",
       "       -0.00203885], dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(22.57306906, dtype=float64), Array(16.54132464, dtype=float64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_obj(x0_scaled), objective(\n",
    "    result.x\n",
    ")  # why is least_squares returning something unscaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.88e+07    \n",
      "       1             12         1.8042e+02      7.43e+01       2.60e-06       7.60e+06    \n",
      "       2             13         1.7088e+02      9.54e+00       2.60e-06       9.89e+06    \n",
      "       3             14         1.6386e+02      7.02e+00       2.60e-06       7.29e+06    \n",
      "       4             15         1.5672e+02      7.14e+00       6.51e-07       4.85e+06    \n",
      "       5             16         1.5155e+02      5.17e+00       1.30e-06       3.86e+06    \n",
      "       6             17         1.4793e+02      3.62e+00       1.30e-06       3.63e+06    \n",
      "       7             18         1.4540e+02      2.53e+00       1.30e-06       4.47e+06    \n",
      "       8             19         1.4372e+02      1.68e+00       1.30e-06       3.37e+06    \n",
      "       9             20         1.4201e+02      1.71e+00       3.26e-07       2.39e+06    \n",
      "      10             21         1.4067e+02      1.34e+00       6.51e-07       1.82e+06    \n",
      "      11             22         1.3971e+02      9.54e-01       6.51e-07       1.80e+06    \n",
      "      12             23         1.3901e+02      6.97e-01       6.51e-07       2.15e+06    \n",
      "      13             24         1.3859e+02      4.21e-01       6.51e-07       1.62e+06    \n",
      "      14             25         1.3817e+02      4.26e-01       1.63e-07       1.22e+06    \n",
      "      15             26         1.3781e+02      3.59e-01       3.26e-07       8.85e+05    \n",
      "      16             27         1.3756e+02      2.50e-01       3.26e-07       9.13e+05    \n",
      "      17             28         1.3736e+02      1.97e-01       3.26e-07       1.05e+06    \n",
      "      18             29         1.3726e+02      1.00e-01       3.26e-07       7.97e+05    \n",
      "      19             30         1.3715e+02      1.08e-01       8.14e-08       6.26e+05    \n",
      "      20             31         1.3706e+02      9.69e-02       1.63e-07       4.35e+05    \n",
      "      21             32         1.3699e+02      6.30e-02       1.63e-07       4.67e+05    \n",
      "      22             33         1.3694e+02      5.65e-02       1.63e-07       5.09e+05    \n",
      "      23             34         1.3692e+02      2.10e-02       1.63e-07       4.09e+05    \n",
      "      24             35         1.3689e+02      2.81e-02       4.07e-08       3.24e+05    \n",
      "      25             36         1.3686e+02      2.64e-02       8.14e-08       2.17e+05    \n",
      "      26             37         1.3685e+02      1.44e-02       8.14e-08       2.40e+05    \n",
      "      27             38         1.3683e+02      1.62e-02       8.14e-08       2.47e+05    \n",
      "      28             39         1.3683e+02      2.67e-03       8.14e-08       2.10e+05    \n",
      "      29             40         1.3682e+02      7.42e-03       2.03e-08       1.69e+05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 2.5477e+02, final cost 1.3682e+02, first-order optimality 1.69e+05.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=objective,\n",
    "    # jac=scaled_grad,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.00572091,  1.77860035,  0.51974064, -0.00665998, -0.00662872,\n",
       "       -0.00203887])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.88e+07    \n",
      "       1             12         1.8045e+02      7.43e+01       2.60e-06       7.59e+06    \n",
      "       2             13         1.7114e+02      9.31e+00       2.60e-06       9.91e+06    \n",
      "       3             14         1.6391e+02      7.24e+00       2.60e-06       7.30e+06    \n",
      "       4             15         1.5681e+02      7.09e+00       6.51e-07       4.83e+06    \n",
      "       5             16         1.5172e+02      5.09e+00       1.30e-06       3.86e+06    \n",
      "       6             17         1.4803e+02      3.69e+00       1.30e-06       3.62e+06    \n",
      "       7             18         1.4557e+02      2.46e+00       1.30e-06       4.47e+06    \n",
      "       8             19         1.4379e+02      1.78e+00       1.30e-06       3.38e+06    \n",
      "       9             20         1.4210e+02      1.69e+00       3.26e-07       2.38e+06    \n",
      "      10             21         1.4078e+02      1.31e+00       6.51e-07       1.82e+06    \n",
      "      11             22         1.3979e+02      9.97e-01       6.51e-07       1.79e+06    \n",
      "      12             23         1.3912e+02      6.69e-01       6.51e-07       2.15e+06    \n",
      "      13             24         1.3864e+02      4.74e-01       6.51e-07       1.63e+06    \n",
      "      14             25         1.3822e+02      4.18e-01       1.63e-07       1.21e+06    \n",
      "      15             26         1.3788e+02      3.48e-01       3.26e-07       8.80e+05    \n",
      "      16             27         1.3760e+02      2.74e-01       3.26e-07       9.07e+05    \n",
      "      17             28         1.3742e+02      1.87e-01       3.26e-07       1.04e+06    \n",
      "      18             29         1.3729e+02      1.28e-01       3.26e-07       7.92e+05    \n",
      "      19             30         1.3719e+02      9.48e-02       3.26e-07       1.11e+06    \n",
      "      20             31         1.3709e+02      1.03e-01       8.14e-08       6.11e+05    \n",
      "      21             32         1.3702e+02      7.27e-02       1.63e-07       4.66e+05    \n",
      "      22             33         1.3697e+02      5.22e-02       1.63e-07       5.06e+05    \n",
      "      23             34         1.3693e+02      3.52e-02       1.63e-07       4.03e+05    \n",
      "      24             35         1.3691e+02      2.59e-02       1.63e-07       5.55e+05    \n",
      "      25             36         1.3688e+02      2.58e-02       4.07e-08       3.05e+05    \n",
      "      26             37         1.3686e+02      1.95e-02       8.14e-08       2.39e+05    \n",
      "      27             38         1.3685e+02      1.46e-02       8.14e-08       2.45e+05    \n",
      "      28             39         1.3684e+02      9.80e-03       8.14e-08       2.06e+05    \n",
      "      29             40         1.3683e+02      7.06e-03       8.14e-08       2.76e+05    \n",
      "      30             41         1.3682e+02      6.48e-03       2.03e-08       1.53e+05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 41, initial cost 2.5477e+02, final cost 1.3682e+02, first-order optimality 1.53e+05.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=objective,\n",
    "    jac=gradient,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(16.54218454, dtype=float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.05587732e+09, dtype=float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_obj(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 22.573069057682215, Loss on the last accepted step: 0.0, Step size: 1.0, y: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616324795378832e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.5, y: [ -415943.93551124  -739899.38120539  -648959.20283939  -721214.60658886\n",
      " -1277773.22966227 -1121111.00126991], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061641537689634e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.25, y: [-207972.97061395 -369948.80129867 -324479.341546   -360607.30662438\n",
      " -638886.61814549 -560555.50165438], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616596534629104e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.125, y: [-103987.48816531 -184973.51134531 -162239.41089931 -180303.65664214\n",
      " -319443.3123871  -280277.75184662], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061695882978612e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0625, y: [ -51994.74694099  -92485.86636863  -81119.44557596  -90151.83165102\n",
      " -159721.65950791 -140138.87694274], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0617683345697946e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.03125, y: [-25998.37632882 -46242.04388029 -40559.46291429 -45075.91915547\n",
      " -79860.83306831 -70069.43949079], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0619132127784445e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.015625, y: [-13000.19102274 -23120.13263612 -20279.47158346 -22537.96290769\n",
      " -39930.41984851 -35034.72076482], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0622029036929414e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0078125, y: [ -6501.0983697  -11559.17701403 -10139.47591804 -11268.9847838\n",
      " -19965.21323861 -17517.36140184], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0627822059249875e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00390625, y: [-3251.55204318 -5778.69920299 -5069.47808533 -5634.49572185\n",
      " -9982.60993366 -8758.68172035], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0639392854639366e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.001953125, y: [-1626.77887992 -2888.46029747 -2534.47916897 -2817.25119088\n",
      " -4991.30828119 -4379.3418796 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0662545377147533e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0009765625, y: [ -814.39229829 -1443.34084471 -1266.9797108  -1408.62892539\n",
      " -2495.65745495 -2189.67195923], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0707941370813786e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00048828125, y: [ -408.19900748  -720.78111833  -633.22998171  -704.31779265\n",
      " -1247.83204183 -1094.83699904], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.066223626450892e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.000244140625, y: [-205.10236207 -359.50125514 -316.35511716 -352.16222628 -623.91933527\n",
      " -547.41951895], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1.741263176329129e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0001220703125, y: [-103.55403937 -178.86132354 -157.91768489 -176.08444309 -311.96298199\n",
      " -273.7107789 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1210058266610485.2, Loss on the last accepted step: 22.573069057682215, Step size: 6.103515625e-05, y: [ -52.77987802  -88.54135775  -78.69896875  -88.0455515  -155.98480535\n",
      " -136.85640888], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1557258003248586.5, Loss on the last accepted step: 22.573069057682215, Step size: 3.0517578125e-05, y: [-27.39279734 -43.38137485 -39.08961069 -44.0261057  -77.99571703\n",
      " -68.42922386], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 802058121937971.2, Loss on the last accepted step: 22.573069057682215, Step size: 1.52587890625e-05, y: [-14.699257   -20.8013834  -19.28493165 -22.0163828  -39.00117287\n",
      " -34.21563136], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 248592818236005.0, Loss on the last accepted step: 22.573069057682215, Step size: 7.62939453125e-06, y: [ -8.35248683  -9.51138767  -9.38259214 -11.01152135 -19.50390079\n",
      " -17.10883511], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 69273677478740.26, Loss on the last accepted step: 22.573069057682215, Step size: 3.814697265625e-06, y: [-5.17910175 -3.86638981 -4.43142238 -5.50909063 -9.75526475 -8.55543698], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18329591762755.668, Loss on the last accepted step: 22.573069057682215, Step size: 1.9073486328125e-06, y: [-3.59240921 -1.04389088 -1.9558375  -2.75787527 -4.88094673 -4.27873792], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4717159011385.629, Loss on the last accepted step: 22.573069057682215, Step size: 9.5367431640625e-07, y: [-2.79906293  0.36735859 -0.71804506 -1.38226759 -2.44378772 -2.14038838], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1196347948988.4683, Loss on the last accepted step: 22.573069057682215, Step size: 4.76837158203125e-07, y: [-2.4023898   1.07298332 -0.09914884 -0.69446375 -1.22520822 -1.07121362], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 301099069639.34674, Loss on the last accepted step: 22.573069057682215, Step size: 2.384185791015625e-07, y: [-2.20405323  1.42579568  0.21029927 -0.35056183 -0.61591847 -0.53662624], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 75478377806.22424, Loss on the last accepted step: 22.573069057682215, Step size: 1.1920928955078125e-07, y: [-2.10488495  1.60220187  0.36502333 -0.17861087 -0.31127359 -0.26933254], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18884195537.268745, Loss on the last accepted step: 22.573069057682215, Step size: 5.960464477539063e-08, y: [-2.05530081  1.69040496  0.44238535 -0.09263539 -0.15895115 -0.1356857 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4721389919.197352, Loss on the last accepted step: 22.573069057682215, Step size: 2.9802322387695312e-08, y: [-2.03050873  1.73450651  0.48106637 -0.04964765 -0.08278993 -0.06886228], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1180222493.5438774, Loss on the last accepted step: 22.573069057682215, Step size: 1.4901161193847656e-08, y: [-2.0181127   1.75655728  0.50040687 -0.02815378 -0.04470932 -0.03545056], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 295006572.2998562, Loss on the last accepted step: 22.573069057682215, Step size: 7.450580596923828e-09, y: [-2.01191468  1.76758266  0.51007713 -0.01740684 -0.02566902 -0.01874471], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 73732297.54740189, Loss on the last accepted step: 22.573069057682215, Step size: 3.725290298461914e-09, y: [-2.00881567  1.77309536  0.51491225 -0.01203337 -0.01614887 -0.01039178], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18424280.514657617, Loss on the last accepted step: 22.573069057682215, Step size: 1.862645149230957e-09, y: [-2.00726617  1.7758517   0.51732982 -0.00934664 -0.01138879 -0.00621532], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4601804.604893329, Loss on the last accepted step: 22.573069057682215, Step size: 9.313225746154785e-10, y: [-2.00649142  1.77722988  0.5185386  -0.00800327 -0.00900875 -0.00412709], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1148343.1054827906, Loss on the last accepted step: 22.573069057682215, Step size: 4.656612873077393e-10, y: [-2.00610404  1.77791896  0.51914299 -0.00733159 -0.00781873 -0.00308297], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 286042.3052457798, Loss on the last accepted step: 22.573069057682215, Step size: 2.3283064365386963e-10, y: [-2.00591035  1.77826351  0.51944519 -0.00699575 -0.00722372 -0.00256091], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 70997.56991652744, Loss on the last accepted step: 22.573069057682215, Step size: 1.1641532182693481e-10, y: [-2.00581351  1.77843578  0.51959628 -0.00682783 -0.00692622 -0.00229988], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 17501.387473158673, Loss on the last accepted step: 22.573069057682215, Step size: 5.820766091346741e-11, y: [-2.00576509  1.77852192  0.51967183 -0.00674387 -0.00677747 -0.00216937], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4259.81340728817, Loss on the last accepted step: 22.573069057682215, Step size: 2.9103830456733704e-11, y: [-2.00574087  1.77856498  0.51970961 -0.00670189 -0.00670309 -0.00210411], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1015.6520619330806, Loss on the last accepted step: 22.573069057682215, Step size: 1.4551915228366852e-11, y: [-2.00572877  1.77858652  0.51972849 -0.0066809  -0.0066659  -0.00207148], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 237.72732879440176, Loss on the last accepted step: 22.573069057682215, Step size: 7.275957614183426e-12, y: [-2.00572272  1.77859728  0.51973794 -0.0066704  -0.00664731 -0.00205517], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 59.80389829475062, Loss on the last accepted step: 22.573069057682215, Step size: 3.637978807091713e-12, y: [-2.00571969  1.77860267  0.51974266 -0.00666515 -0.00663801 -0.00204701], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 23.60190956181135, Loss on the last accepted step: 22.573069057682215, Step size: 1.8189894035458565e-12, y: [-2.00571818  1.77860536  0.51974502 -0.00666253 -0.00663336 -0.00204293], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18.69084593908376, Loss on the last accepted step: 22.573069057682215, Step size: 1.0, y: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712423596641361.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.5, y: [-164091.019947   -293283.07549806 -257138.43638475   55793.39307797\n",
      "   97525.82686763   85840.38200621], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712480206610871.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.25, y: [ -82046.51283221 -146640.64844568 -128568.95831927   27896.69320838\n",
      "   48762.9101183    42920.18998266], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712597410940664.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.125, y: [-41024.25927482 -73319.43491949 -64284.21928654  13948.34327358\n",
      "  24381.45174363  21460.09397088], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712863729759803.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0625, y: [-20513.13249612 -36658.82815639 -32141.84977017   6974.16830618\n",
      "  12190.7225563   10730.045965  ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6713652404484542.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.03125, y: [-10257.56910677 -18328.52477484 -16070.66501198   3487.08082248\n",
      "   6095.35796263   5365.02196205], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6717293985420048.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.015625, y: [-5129.78741209 -9163.37308407 -8035.07263289  1743.53708063\n",
      "  3047.67566579  2682.50996058], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6741501953156261.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0078125, y: [-2565.89656476 -4580.79723868 -4017.27644335   871.76520971\n",
      "  1523.83451738  1341.25395984], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6940865313257104.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00390625, y: [-1283.95114109 -2289.50931599 -2008.37834857   435.87927424\n",
      "   761.91394317   670.62595948], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1.0078867560568664e+16, Loss on the last accepted step: 18.69084593908376, Step size: 0.001953125, y: [ -642.97842925 -1143.86535464 -1003.92930119   217.93630651\n",
      "   380.95365607   335.31195929], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3566118524490609.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0009765625, y: [-322.49207334 -571.04337397 -501.70477749  108.96482265  190.47351251\n",
      "  167.6549592 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4121714667306713.5, Loss on the last accepted step: 18.69084593908376, Step size: 0.00048828125, y: [-162.24889538 -284.63238363 -250.59251565   54.47908072   95.23344074\n",
      "   83.82645915], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 5402129976275411.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.000244140625, y: [ -82.1273064  -141.42688846 -125.03638472   27.23620975   47.61340485\n",
      "   41.91220913], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 412858984188575.94, Loss on the last accepted step: 18.69084593908376, Step size: 0.0001220703125, y: [-42.06651191 -69.82414088 -62.25831926  13.61477427  23.80338691\n",
      "  20.95508412], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 81851767735781.38, Loss on the last accepted step: 18.69084593908376, Step size: 6.103515625e-05, y: [-22.03611467 -34.02276709 -30.86928653   6.80405652  11.89837793\n",
      "  10.47652161], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 18050294594262.91, Loss on the last accepted step: 18.69084593908376, Step size: 3.0517578125e-05, y: [-12.02091604 -16.12208019 -15.17477016   3.39869765   5.94587345\n",
      "   5.23724036], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4228206146111.887, Loss on the last accepted step: 18.69084593908376, Step size: 1.52587890625e-05, y: [-7.01331673 -7.17173674 -7.32751198  1.69601822  2.9696212   2.61759973], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1022977564452.0562, Loss on the last accepted step: 18.69084593908376, Step size: 7.62939453125e-06, y: [-4.50951708 -2.69656502 -3.40388289  0.8446785   1.48149508  1.30777942], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 251731407653.32993, Loss on the last accepted step: 18.69084593908376, Step size: 3.814697265625e-06, y: [-3.25761725 -0.45897916 -1.44206834  0.41900864  0.73743202  0.65286926], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 62492595560.648544, Loss on the last accepted step: 18.69084593908376, Step size: 1.9073486328125e-06, y: [-2.63166733  0.65981377 -0.46116107  0.20617371  0.36540049  0.32541419], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15578567914.51824, Loss on the last accepted step: 18.69084593908376, Step size: 9.5367431640625e-07, y: [-2.31869238  1.21921024  0.02929256  0.09975625  0.17938473  0.16168665], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3890104800.188278, Loss on the last accepted step: 18.69084593908376, Step size: 4.76837158203125e-07, y: [-2.1622049   1.49890847  0.27451938  0.04654751  0.08637684  0.07982288], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 972008146.5688134, Loss on the last accepted step: 18.69084593908376, Step size: 2.384185791015625e-07, y: [-2.08396116  1.63875759  0.39713279  0.01994315  0.0398729   0.03889099], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 242925710.08179784, Loss on the last accepted step: 18.69084593908376, Step size: 1.1920928955078125e-07, y: [-2.04483929  1.70868215  0.4584395   0.00664097  0.01662093  0.01842505], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 60713707.81796045, Loss on the last accepted step: 18.69084593908376, Step size: 5.960464477539063e-08, y: [-2.02527836e+00  1.74364443e+00  4.89092849e-01 -1.01262134e-05\n",
      "  4.99494659e-03  8.19207873e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15171976.190410793, Loss on the last accepted step: 18.69084593908376, Step size: 2.9802322387695312e-08, y: [-2.01549789e+00  1.76112557e+00  5.04419525e-01 -3.33567199e-03\n",
      " -8.18046069e-04  3.07559314e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3790066.3595114155, Loss on the last accepted step: 18.69084593908376, Step size: 1.4901161193847656e-08, y: [-2.01060765e+00  1.76986614e+00  5.12082863e-01 -4.99844487e-03\n",
      " -3.72454240e-03  5.17350337e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 946095.484585892, Loss on the last accepted step: 18.69084593908376, Step size: 7.450580596923828e-09, y: [-2.00816254e+00  1.77423642e+00  5.15914532e-01 -5.82983131e-03\n",
      " -5.17779057e-03 -7.61771063e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 235824.76051909928, Loss on the last accepted step: 18.69084593908376, Step size: 3.725290298461914e-09, y: [-2.00693998e+00  1.77642156e+00  5.17830366e-01 -6.24552454e-03\n",
      " -5.90441465e-03 -1.40133176e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 58614.1952424372, Loss on the last accepted step: 18.69084593908376, Step size: 1.862645149230957e-09, y: [-2.00632870e+00  1.77751413e+00  5.18788284e-01 -6.45337115e-03\n",
      " -6.26772669e-03 -1.72111211e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 14489.629241776953, Loss on the last accepted step: 18.69084593908376, Step size: 9.313225746154785e-10, y: [-2.00602306e+00  1.77806042e+00  5.19267242e-01 -6.55729445e-03\n",
      " -6.44938271e-03 -1.88100229e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3547.4651612417106, Loss on the last accepted step: 18.69084593908376, Step size: 4.656612873077393e-10, y: [-2.00587024e+00  1.77833356e+00  5.19506722e-01 -6.60925611e-03\n",
      " -6.54021072e-03 -1.96094738e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 856.4053607312029, Loss on the last accepted step: 18.69084593908376, Step size: 2.3283064365386963e-10, y: [-2.00579383e+00  1.77847013e+00  5.19626461e-01 -6.63523693e-03\n",
      " -6.58562473e-03 -2.00091992e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 205.88007521941597, Loss on the last accepted step: 18.69084593908376, Step size: 1.1641532182693481e-10, y: [-2.00575563  1.77853842  0.51968633 -0.00664823 -0.00660833 -0.00202091], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 54.3684702234115, Loss on the last accepted step: 18.69084593908376, Step size: 5.820766091346741e-11, y: [-2.00573652  1.77857256  0.51971627 -0.00665472 -0.00661969 -0.0020309 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 22.050412017517964, Loss on the last accepted step: 18.69084593908376, Step size: 2.9103830456733704e-11, y: [-2.00572697  1.77858963  0.51973123 -0.00665797 -0.00662536 -0.0020359 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 16.750817717121407, Loss on the last accepted step: 18.69084593908376, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 131523100784635.75, Loss on the last accepted step: 16.750817717121407, Step size: 0.5, y: [ -7.84415605 -10.72565593  18.49561315 -17.99848853 -43.15785489\n",
      "  60.76072742], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 86567744527131.1, Loss on the last accepted step: 16.750817717121407, Step size: 0.25, y: [ -4.92493912  -4.47352888   9.50767594  -9.00257406 -21.58224154\n",
      "  30.37934451], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 43008345261396.5, Loss on the last accepted step: 16.750817717121407, Step size: 0.125, y: [ -3.46533066  -1.34746536   5.01370733  -4.50461683 -10.79443487\n",
      "  15.18865306], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16941324528617.014, Loss on the last accepted step: 16.750817717121407, Step size: 0.0625, y: [-2.73552643  0.21556641  2.76672302 -2.25563821 -5.40053154  7.59330733], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 5567731542010.476, Loss on the last accepted step: 16.750817717121407, Step size: 0.03125, y: [-2.37062431  0.99708229  1.64323087 -1.1311489  -2.70357987  3.79563447], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1621028872905.0005, Loss on the last accepted step: 16.750817717121407, Step size: 0.015625, y: [-2.18817325  1.38784023  1.08148479 -0.56890425 -1.35510403  1.89679804], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 439222250849.9029, Loss on the last accepted step: 16.750817717121407, Step size: 0.0078125, y: [-2.09694772  1.5832192   0.80061176 -0.28778192 -0.68086612  0.94737982], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 114392382418.5628, Loss on the last accepted step: 16.750817717121407, Step size: 0.00390625, y: [-2.05133496  1.68090868  0.66017524 -0.14722076 -0.34374716  0.47267071], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 29180371886.50014, Loss on the last accepted step: 16.750817717121407, Step size: 0.001953125, y: [-2.02852858  1.72975343  0.58995698 -0.07694018 -0.17518768  0.23531616], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7366575042.67023, Loss on the last accepted step: 16.750817717121407, Step size: 0.0009765625, y: [-2.01712539  1.7541758   0.55484785 -0.04179988 -0.09090794  0.11663888], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1850385017.0490952, Loss on the last accepted step: 16.750817717121407, Step size: 0.00048828125, y: [-2.01142379  1.76638698  0.53729328 -0.02422974 -0.04876807  0.05730024], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 463673321.75153077, Loss on the last accepted step: 16.750817717121407, Step size: 0.000244140625, y: [-2.00857299  1.77249258  0.528516   -0.01544467 -0.02769814  0.02763093], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 116051921.96936578, Loss on the last accepted step: 16.750817717121407, Step size: 0.0001220703125, y: [-2.00714759  1.77554537  0.52412736 -0.01105213 -0.01716317  0.01279627], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 29029622.769996047, Loss on the last accepted step: 16.750817717121407, Step size: 6.103515625e-05, y: [-2.0064349   1.77707177  0.52193304 -0.00885586 -0.01189568  0.00537894], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7259492.578199817, Loss on the last accepted step: 16.750817717121407, Step size: 3.0517578125e-05, y: [-2.00607855e+00  1.77783497e+00  5.20835877e-01 -7.75772805e-03\n",
      " -9.26194222e-03  1.67027073e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1815144.6584349484, Loss on the last accepted step: 16.750817717121407, Step size: 1.52587890625e-05, y: [-2.00590037e+00  1.77821657e+00  5.20287297e-01 -7.20866100e-03\n",
      " -7.94507129e-03 -1.84061726e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 453831.0432556629, Loss on the last accepted step: 16.750817717121407, Step size: 7.62939453125e-06, y: [-2.00581128e+00  1.77840737e+00  5.20013007e-01 -6.93412748e-03\n",
      " -7.28663582e-03 -1.11122795e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 113474.34408761517, Loss on the last accepted step: 16.750817717121407, Step size: 3.814697265625e-06, y: [-2.00576674e+00  1.77850277e+00  5.19875862e-01 -6.79686072e-03\n",
      " -6.95741809e-03 -1.57481107e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 28381.6424259518, Loss on the last accepted step: 16.750817717121407, Step size: 1.9073486328125e-06, y: [-2.00574447e+00  1.77855047e+00  5.19807290e-01 -6.72822734e-03\n",
      " -6.79280922e-03 -1.80660262e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7108.030810167587, Loss on the last accepted step: 16.750817717121407, Step size: 9.5367431640625e-07, y: [-2.00573333e+00  1.77857432e+00  5.19773003e-01 -6.69391065e-03\n",
      " -6.71050479e-03 -1.92249840e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1789.5756604726666, Loss on the last accepted step: 16.750817717121407, Step size: 4.76837158203125e-07, y: [-2.00572776e+00  1.77858624e+00  5.19755860e-01 -6.67675230e-03\n",
      " -6.66935257e-03 -1.98044629e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 459.9564923332625, Loss on the last accepted step: 16.750817717121407, Step size: 2.384185791015625e-07, y: [-2.00572498  1.77859221  0.51974729 -0.00666817 -0.00664878 -0.00200942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 127.55159883155568, Loss on the last accepted step: 16.750817717121407, Step size: 1.1920928955078125e-07, y: [-2.00572359  1.77859519  0.519743   -0.00666388 -0.00663849 -0.00202391], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44.450647194213346, Loss on the last accepted step: 16.750817717121407, Step size: 5.960464477539063e-08, y: [-2.00572289  1.77859668  0.51974086 -0.00666174 -0.00663334 -0.00203115], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23.675586679764415, Loss on the last accepted step: 16.750817717121407, Step size: 2.9802322387695312e-08, y: [-2.00572254  1.77859742  0.51973979 -0.00666067 -0.00663077 -0.00203477], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18.481915022049037, Loss on the last accepted step: 16.750817717121407, Step size: 1.4901161193847656e-08, y: [-2.00572237  1.7785978   0.51973925 -0.00666013 -0.00662949 -0.00203658], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.183544363163342, Loss on the last accepted step: 16.750817717121407, Step size: 7.450580596923828e-09, y: [-2.00572228  1.77859798  0.51973899 -0.00665986 -0.00662884 -0.00203749], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.85897569213404, Loss on the last accepted step: 16.750817717121407, Step size: 3.725290298461914e-09, y: [-2.00572224  1.77859808  0.51973885 -0.00665973 -0.00662852 -0.00203794], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.777845317274387, Loss on the last accepted step: 16.750817717121407, Step size: 1.862645149230957e-09, y: [-2.00572222  1.77859812  0.51973878 -0.00665966 -0.00662836 -0.00203817], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.757568678489104, Loss on the last accepted step: 16.750817717121407, Step size: 9.313225746154785e-10, y: [-2.00572221  1.77859815  0.51973875 -0.00665963 -0.00662828 -0.00203828], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.752502486003525, Loss on the last accepted step: 16.750817717121407, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859816  0.51973873 -0.00665961 -0.00662824 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751237423462015, Loss on the last accepted step: 16.750817717121407, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859816  0.51973873 -0.0066596  -0.00662822 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75092190092924, Loss on the last accepted step: 16.750817717121407, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750843391542276, Loss on the last accepted step: 16.750817717121407, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.7508239500109, Loss on the last accepted step: 16.750817717121407, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750819182552362, Loss on the last accepted step: 16.750817717121407, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750818037101286, Loss on the last accepted step: 16.750817717121407, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817773858405, Loss on the last accepted step: 16.750817717121407, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75081771961047, Loss on the last accepted step: 16.750817717121407, Step size: 1.8189894035458565e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817711967763, Loss on the last accepted step: 16.750817717121407, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18363996951.55463, Loss on the last accepted step: 16.750817711967763, Step size: 0.5, y: [-2.48294375  2.96607241 -0.53189293 -1.00310846  0.18991806  0.41706305], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 6756271546.493962, Loss on the last accepted step: 16.750817711967763, Step size: 0.25, y: [-2.24433297  2.37233529 -0.0060771  -0.50488403  0.09164493  0.20751233], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2146876769.4546375, Loss on the last accepted step: 16.750817711967763, Step size: 0.125, y: [-2.12502759  2.07546673  0.25683081 -0.25577181  0.04250837  0.10273697], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 617037836.1087826, Loss on the last accepted step: 16.750817711967763, Step size: 0.0625, y: [-2.06537489  1.92703245  0.38828476 -0.1312157   0.01794008  0.05034929], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 166608057.54618162, Loss on the last accepted step: 16.750817711967763, Step size: 0.03125, y: [-2.03554854  1.85281531  0.45401174 -0.06893765  0.00565594  0.02415545], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 43391001.33768293, Loss on the last accepted step: 16.750817711967763, Step size: 0.015625, y: [-2.02063537e+00  1.81570674e+00  4.86875228e-01 -3.77986209e-02\n",
      " -4.86129721e-04  1.10585260e-02], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 11079760.041092493, Loss on the last accepted step: 16.750817711967763, Step size: 0.0078125, y: [-2.01317878  1.79715245  0.50330697 -0.02222911 -0.00355717  0.00451007], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2799962.4532937957, Loss on the last accepted step: 16.750817711967763, Step size: 0.00390625, y: [-2.00945049e+00  1.78787531e+00  5.11522845e-01 -1.44443507e-02\n",
      " -5.09268276e-03  1.23583595e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 703820.9772255587, Loss on the last accepted step: 16.750817711967763, Step size: 0.001953125, y: [-2.00758634e+00  1.78323674e+00  5.15630781e-01 -1.05519724e-02\n",
      " -5.86044160e-03 -4.01279058e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 176449.30590998745, Loss on the last accepted step: 16.750817711967763, Step size: 0.0009765625, y: [-2.00665427e+00  1.78091746e+00  5.17684749e-01 -8.60578317e-03\n",
      " -6.24432101e-03 -1.21983656e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44185.34626821333, Loss on the last accepted step: 16.750817711967763, Step size: 0.00048828125, y: [-2.00618823e+00  1.77975781e+00  5.18711733e-01 -7.63268858e-03\n",
      " -6.43626072e-03 -1.62911532e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 11066.473009255686, Loss on the last accepted step: 16.750817711967763, Step size: 0.000244140625, y: [-2.00595521e+00  1.77917799e+00  5.19225225e-01 -7.14614129e-03\n",
      " -6.53223058e-03 -1.83375469e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2780.1287031015477, Loss on the last accepted step: 16.750817711967763, Step size: 0.0001220703125, y: [-2.00583871e+00  1.77888808e+00  5.19481971e-01 -6.90286764e-03\n",
      " -6.58021551e-03 -1.93607438e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 707.7135717177605, Loss on the last accepted step: 16.750817711967763, Step size: 6.103515625e-05, y: [-2.00578045e+00  1.77874313e+00  5.19610344e-01 -6.78123081e-03\n",
      " -6.60420797e-03 -1.98723422e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 189.50619648032514, Loss on the last accepted step: 16.750817711967763, Step size: 3.0517578125e-05, y: [-2.00575132  1.77867065  0.51967453 -0.00672041 -0.0066162  -0.00201281], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 59.9414478351904, Loss on the last accepted step: 16.750817711967763, Step size: 1.52587890625e-05, y: [-2.00573676  1.77863441  0.51970662 -0.00669    -0.0066222  -0.0020256 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 27.548673603864408, Loss on the last accepted step: 16.750817711967763, Step size: 7.62939453125e-06, y: [-2.00572948  1.77861629  0.51972267 -0.0066748  -0.0066252  -0.002032  ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.450294040702488, Loss on the last accepted step: 16.750817711967763, Step size: 3.814697265625e-06, y: [-2.00572584  1.77860723  0.51973069 -0.0066672  -0.0066267  -0.0020352 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.425682053821195, Loss on the last accepted step: 16.750817711967763, Step size: 1.9073486328125e-06, y: [-2.00572402  1.7786027   0.51973471 -0.0066634  -0.00662745 -0.0020368 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.91953015673455, Loss on the last accepted step: 16.750817711967763, Step size: 9.5367431640625e-07, y: [-2.00572311  1.77860043  0.51973671 -0.00666149 -0.00662783 -0.00203759], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.79299390837364, Loss on the last accepted step: 16.750817711967763, Step size: 4.76837158203125e-07, y: [-2.00572265  1.7785993   0.51973771 -0.00666054 -0.00662801 -0.00203799], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.76136082076422, Loss on the last accepted step: 16.750817711967763, Step size: 2.384185791015625e-07, y: [-2.00572242  1.77859874  0.51973822 -0.00666007 -0.00662811 -0.00203819], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75345308797457, Loss on the last accepted step: 16.750817711967763, Step size: 1.1920928955078125e-07, y: [-2.00572231  1.77859845  0.51973847 -0.00665983 -0.00662815 -0.00203829], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751476336636447, Loss on the last accepted step: 16.750817711967763, Step size: 5.960464477539063e-08, y: [-2.00572225  1.77859831  0.51973859 -0.00665971 -0.00662818 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75098238989647, Loss on the last accepted step: 16.750817711967763, Step size: 2.9802322387695312e-08, y: [-2.00572222  1.77859824  0.51973865 -0.00665965 -0.00662819 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750858816951485, Loss on the last accepted step: 16.750817711967763, Step size: 1.4901161193847656e-08, y: [-2.00572221  1.77859821  0.51973869 -0.00665962 -0.00662819 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75082792186553, Loss on the last accepted step: 16.750817711967763, Step size: 7.450580596923828e-09, y: [-2.0057222   1.77859819  0.5197387  -0.00665961 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750820333891976, Loss on the last accepted step: 16.750817711967763, Step size: 3.725290298461914e-09, y: [-2.0057222   1.77859818  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75081838074599, Loss on the last accepted step: 16.750817711967763, Step size: 1.862645149230957e-09, y: [-2.0057222   1.77859817  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817882985412, Loss on the last accepted step: 16.750817711967763, Step size: 9.313225746154785e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817757323993, Loss on the last accepted step: 16.750817711967763, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817724620212, Loss on the last accepted step: 16.750817711967763, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817715789637, Loss on the last accepted step: 16.750817711967763, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817713246494, Loss on the last accepted step: 16.750817711967763, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712338936, Loss on the last accepted step: 16.750817711967763, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712170505, Loss on the last accepted step: 16.750817711967763, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712047162, Loss on the last accepted step: 16.750817711967763, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712051596, Loss on the last accepted step: 16.750817711967763, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817711896932, Loss on the last accepted step: 16.750817711967763, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 122247.18229677227, Loss on the last accepted step: 16.750817711896932, Step size: 0.5, y: [-5.55581302  3.52387244  0.7868994  -4.04584331  2.01188892  0.30655285], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 72213.8255023129, Loss on the last accepted step: 16.750817711896932, Step size: 0.25, y: [-3.78076761  2.65123531  0.65331906 -2.02625145  1.00263036  0.15225723], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 35763.522801532694, Loss on the last accepted step: 16.750817711896932, Step size: 0.125, y: [-2.8932449   2.21491674  0.58652889 -1.01645552  0.49800108  0.07510942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 14598.339188521817, Loss on the last accepted step: 16.750817711896932, Step size: 0.0625, y: [-2.44948355  1.99675745  0.5531338  -0.51155756  0.24568644  0.03653551], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 5018.307460901265, Loss on the last accepted step: 16.750817711896932, Step size: 0.03125, y: [-2.22760287  1.88767781  0.53643626 -0.25910858  0.11952912  0.01724856], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1523.2909212981372, Loss on the last accepted step: 16.750817711896932, Step size: 0.015625, y: [-2.11666253  1.83313799  0.52808749 -0.13288409  0.05645046  0.00760508], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 433.97210686816834, Loss on the last accepted step: 16.750817711896932, Step size: 0.0078125, y: [-2.06119237  1.80586808  0.5239131  -0.06977184  0.02491113  0.00278334], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 126.76368929532103, Loss on the last accepted step: 16.750817711896932, Step size: 0.00390625, y: [-2.03345728e+00  1.79223312e+00  5.21825910e-01 -3.82157168e-02\n",
      "  9.14146457e-03  3.72475019e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44.977445191646794, Loss on the last accepted step: 16.750817711896932, Step size: 0.001953125, y: [-2.01958974e+00  1.78541565e+00  5.20782314e-01 -2.24376554e-02\n",
      "  1.25663207e-03 -8.32959524e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23.880306578305532, Loss on the last accepted step: 16.750817711896932, Step size: 0.0009765625, y: [-2.01265597e+00  1.78200691e+00  5.20260515e-01 -1.45486247e-02\n",
      " -2.68578418e-03 -1.43567680e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18.531962204523392, Loss on the last accepted step: 16.750817711896932, Step size: 0.00048828125, y: [-2.00918908e+00  1.78030254e+00  5.19999616e-01 -1.06041093e-02\n",
      " -4.65699231e-03 -1.73703543e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.19068430677233, Loss on the last accepted step: 16.750817711896932, Step size: 0.000244140625, y: [-2.00745564e+00  1.77945035e+00  5.19869167e-01 -8.63185167e-03\n",
      " -5.64259637e-03 -1.88771475e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.857466963444338, Loss on the last accepted step: 16.750817711896932, Step size: 0.0001220703125, y: [-2.00658892e+00  1.77902426e+00  5.19803942e-01 -7.64572283e-03\n",
      " -6.13539840e-03 -1.96305441e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.77574520488109, Loss on the last accepted step: 16.750817711896932, Step size: 6.103515625e-05, y: [-2.00615556e+00  1.77881122e+00  5.19771330e-01 -7.15265841e-03\n",
      " -6.38179942e-03 -2.00072424e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75617269907901, Loss on the last accepted step: 16.750817711896932, Step size: 3.0517578125e-05, y: [-2.00593888  1.77870469  0.51975502 -0.00690613 -0.006505   -0.00201956], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75171681690975, Loss on the last accepted step: 16.750817711896932, Step size: 1.52587890625e-05, y: [-2.00583054  1.77865143  0.51974687 -0.00678286 -0.0065666  -0.00202898], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750822485870852, Loss on the last accepted step: 16.750817711896932, Step size: 7.62939453125e-06, y: [-2.00577637  1.7786248   0.51974279 -0.00672123 -0.0065974  -0.00203369], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75070899419128, Loss on the last accepted step: 16.750817711896932, Step size: 1.0, y: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 224.62754792706178, Loss on the last accepted step: 16.75070899419128, Step size: 0.5, y: [-6.26535506  3.88206434  0.8417004   0.2307619  -0.12013257 -0.01951041], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 100.82484958016879, Loss on the last accepted step: 16.75070899419128, Step size: 0.25, y: [-4.13555217  2.83033791  0.68072058  0.11203575 -0.06337268 -0.01077322], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 49.75759425604515, Loss on the last accepted step: 16.75070899419128, Step size: 0.125, y: [-3.07065072  2.3044747   0.60023067  0.05267267 -0.03499274 -0.00640463], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 29.10252300326593, Loss on the last accepted step: 16.75070899419128, Step size: 0.0625, y: [-2.5382      2.04154309  0.55998571  0.02299113 -0.02080277 -0.00422034], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 20.615770620524625, Loss on the last accepted step: 16.75070899419128, Step size: 0.03125, y: [-2.27197464  1.91007729  0.53986323  0.00815036 -0.01370779 -0.00312819], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 17.62360062464468, Loss on the last accepted step: 16.75070899419128, Step size: 0.015625, y: [-2.13886196e+00  1.84434439e+00  5.29801994e-01  7.29974286e-04\n",
      " -1.01602931e-02 -2.58211375e-03], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 16.810958430674155, Loss on the last accepted step: 16.75070899419128, Step size: 0.0078125, y: [-2.07230562  1.81147794  0.52477137 -0.00298022 -0.00838655 -0.00230908], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 16.667565468019887, Loss on the last accepted step: 16.75070899419128, Step size: 1.0, y: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 60.345487249090766, Loss on the last accepted step: 16.667565468019887, Step size: 0.5, y: [-1.43658772e+00  1.49758209e+00  4.76739940e-01 -3.76857997e-02\n",
      "  8.19311683e-03  2.37237244e-04], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 19.808296337302217, Loss on the last accepted step: 16.667565468019887, Step size: 0.25, y: [-1.73780759e+00  1.64631340e+00  4.99498002e-01 -2.12605570e-02\n",
      "  3.46721645e-04 -9.67660486e-04], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 17.146001891809924, Loss on the last accepted step: 16.667565468019887, Step size: 0.125, y: [-1.88841752e+00  1.72067906e+00  5.10877034e-01 -1.30479357e-02\n",
      " -3.57647595e-03 -1.57010935e-03], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 16.714983595230724, Loss on the last accepted step: 16.667565468019887, Step size: 0.0625, y: [-1.96372249e+00  1.75786188e+00  5.16566549e-01 -8.94162499e-03\n",
      " -5.53807474e-03 -1.87133378e-03], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 16.64579281615707, Loss on the last accepted step: 16.667565468019887, Step size: 1.0, y: [-2.00137497  1.7764533   0.51941131 -0.00688847 -0.00651887 -0.00202195], y on the last accepted step: [-2.00137497  1.7764533   0.51941131 -0.00688847 -0.00651887 -0.00202195]\n",
      "Loss on this step: 16.548062851891693, Loss on the last accepted step: 16.64579281615707, Step size: 1.0, y: [-1.9996816   1.77561838  0.51928441 -0.00699265 -0.0064699  -0.00201447], y on the last accepted step: [-1.9996816   1.77561838  0.51928441 -0.00699265 -0.0064699  -0.00201447]\n",
      "Loss on this step: 16.540977129124727, Loss on the last accepted step: 16.548062851891693, Step size: 1.0, y: [-2.00742049  1.77943937  0.51986893 -0.00656737 -0.00667292 -0.00204564], y on the last accepted step: [-2.00742049  1.77943937  0.51986893 -0.00656737 -0.00667292 -0.00204564]\n",
      "Loss on this step: 16.54089465193331, Loss on the last accepted step: 16.540977129124727, Step size: 1.0, y: [-2.00558462  1.77853286  0.51973021 -0.0066675  -0.00662509 -0.0020383 ], y on the last accepted step: [-2.00558462  1.77853286  0.51973021 -0.0066675  -0.00662509 -0.0020383 ]\n",
      "Loss on this step: 16.540893598080693, Loss on the last accepted step: 16.54089465193331, Step size: 1.0, y: [-2.00572347  1.77860143  0.51974071 -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572347  1.77860143  0.51974071 -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.5408935976883, Loss on the last accepted step: 16.540893598080693, Step size: 1.0, y: [-2.00572342  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572342  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597688108, Loss on the last accepted step: 16.5408935976883, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597688004, Loss on the last accepted step: 16.540893597688108, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687983, Loss on the last accepted step: 16.540893597688004, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687983, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.5, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.25, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.0625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.54089359768798, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.5, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.25, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.0625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.03125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.015625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0078125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00390625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.001953125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0009765625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00048828125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.000244140625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0001220703125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 6.103515625e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.0517578125e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.52587890625e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 7.62939453125e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.814697265625e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.9073486328125e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 9.5367431640625e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 4.76837158203125e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.384185791015625e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.1920928955078125e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 5.960464477539063e-08, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.9802322387695312e-08, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n"
     ]
    }
   ],
   "source": [
    "import optimistix\n",
    "\n",
    "\n",
    "# def func(x, args):\n",
    "#     c = CartesianState(x=x[:3][None,:], v=x[3:][None,:], time=times[0].tdb.jd)\n",
    "#     return -p0.loglike(c)\n",
    "def func(x, args):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "solver = optimistix.BFGS(\n",
    "    rtol=1e-12,\n",
    "    atol=1e-12,\n",
    "    verbose=frozenset({\"step_size\", \"loss\", \"y\"}),\n",
    "    use_inverse=False,\n",
    ")\n",
    "res = optimistix.minimise(\n",
    "    fn=func,\n",
    "    solver=solver,\n",
    "    y0=jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 22.573069057682215, Loss on the last accepted step: 0.0, Step size: 1.0, y: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616324795378832e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.5, y: [ -415943.93551124  -739899.38120539  -648959.20283939  -721214.60658886\n",
      " -1277773.22966227 -1121111.00126991], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061641537689634e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.25, y: [-207972.97061395 -369948.80129867 -324479.341546   -360607.30662438\n",
      " -638886.61814549 -560555.50165438], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616596534629104e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.125, y: [-103987.48816531 -184973.51134531 -162239.41089931 -180303.65664214\n",
      " -319443.3123871  -280277.75184662], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061695882978612e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0625, y: [ -51994.74694099  -92485.86636863  -81119.44557596  -90151.83165102\n",
      " -159721.65950791 -140138.87694274], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0617683345697946e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.03125, y: [-25998.37632882 -46242.04388029 -40559.46291429 -45075.91915547\n",
      " -79860.83306831 -70069.43949079], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0619132127784445e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.015625, y: [-13000.19102274 -23120.13263612 -20279.47158346 -22537.96290769\n",
      " -39930.41984851 -35034.72076482], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0622029036929414e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0078125, y: [ -6501.0983697  -11559.17701403 -10139.47591804 -11268.9847838\n",
      " -19965.21323861 -17517.36140184], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0627822059249875e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00390625, y: [-3251.55204318 -5778.69920299 -5069.47808533 -5634.49572185\n",
      " -9982.60993366 -8758.68172035], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0639392854639366e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.001953125, y: [-1626.77887992 -2888.46029747 -2534.47916897 -2817.25119088\n",
      " -4991.30828119 -4379.3418796 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0662545377147533e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0009765625, y: [ -814.39229829 -1443.34084471 -1266.9797108  -1408.62892539\n",
      " -2495.65745495 -2189.67195923], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0707941370813786e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00048828125, y: [ -408.19900748  -720.78111833  -633.22998171  -704.31779265\n",
      " -1247.83204183 -1094.83699904], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.066223626450892e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.000244140625, y: [-205.10236207 -359.50125514 -316.35511716 -352.16222628 -623.91933527\n",
      " -547.41951895], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1.741263176329129e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0001220703125, y: [-103.55403937 -178.86132354 -157.91768489 -176.08444309 -311.96298199\n",
      " -273.7107789 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1210058266610485.2, Loss on the last accepted step: 22.573069057682215, Step size: 6.103515625e-05, y: [ -52.77987802  -88.54135775  -78.69896875  -88.0455515  -155.98480535\n",
      " -136.85640888], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1557258003248586.5, Loss on the last accepted step: 22.573069057682215, Step size: 3.0517578125e-05, y: [-27.39279734 -43.38137485 -39.08961069 -44.0261057  -77.99571703\n",
      " -68.42922386], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 802058121937971.2, Loss on the last accepted step: 22.573069057682215, Step size: 1.52587890625e-05, y: [-14.699257   -20.8013834  -19.28493165 -22.0163828  -39.00117287\n",
      " -34.21563136], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 248592818236005.0, Loss on the last accepted step: 22.573069057682215, Step size: 7.62939453125e-06, y: [ -8.35248683  -9.51138767  -9.38259214 -11.01152135 -19.50390079\n",
      " -17.10883511], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 69273677478740.26, Loss on the last accepted step: 22.573069057682215, Step size: 3.814697265625e-06, y: [-5.17910175 -3.86638981 -4.43142238 -5.50909063 -9.75526475 -8.55543698], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18329591762755.668, Loss on the last accepted step: 22.573069057682215, Step size: 1.9073486328125e-06, y: [-3.59240921 -1.04389088 -1.9558375  -2.75787527 -4.88094673 -4.27873792], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4717159011385.629, Loss on the last accepted step: 22.573069057682215, Step size: 9.5367431640625e-07, y: [-2.79906293  0.36735859 -0.71804506 -1.38226759 -2.44378772 -2.14038838], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1196347948988.4683, Loss on the last accepted step: 22.573069057682215, Step size: 4.76837158203125e-07, y: [-2.4023898   1.07298332 -0.09914884 -0.69446375 -1.22520822 -1.07121362], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 301099069639.34674, Loss on the last accepted step: 22.573069057682215, Step size: 2.384185791015625e-07, y: [-2.20405323  1.42579568  0.21029927 -0.35056183 -0.61591847 -0.53662624], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 75478377806.22424, Loss on the last accepted step: 22.573069057682215, Step size: 1.1920928955078125e-07, y: [-2.10488495  1.60220187  0.36502333 -0.17861087 -0.31127359 -0.26933254], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18884195537.268745, Loss on the last accepted step: 22.573069057682215, Step size: 5.960464477539063e-08, y: [-2.05530081  1.69040496  0.44238535 -0.09263539 -0.15895115 -0.1356857 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4721389919.197352, Loss on the last accepted step: 22.573069057682215, Step size: 2.9802322387695312e-08, y: [-2.03050873  1.73450651  0.48106637 -0.04964765 -0.08278993 -0.06886228], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1180222493.5438774, Loss on the last accepted step: 22.573069057682215, Step size: 1.4901161193847656e-08, y: [-2.0181127   1.75655728  0.50040687 -0.02815378 -0.04470932 -0.03545056], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 295006572.2998562, Loss on the last accepted step: 22.573069057682215, Step size: 7.450580596923828e-09, y: [-2.01191468  1.76758266  0.51007713 -0.01740684 -0.02566902 -0.01874471], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 73732297.54740189, Loss on the last accepted step: 22.573069057682215, Step size: 3.725290298461914e-09, y: [-2.00881567  1.77309536  0.51491225 -0.01203337 -0.01614887 -0.01039178], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18424280.514657617, Loss on the last accepted step: 22.573069057682215, Step size: 1.862645149230957e-09, y: [-2.00726617  1.7758517   0.51732982 -0.00934664 -0.01138879 -0.00621532], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4601804.604893329, Loss on the last accepted step: 22.573069057682215, Step size: 9.313225746154785e-10, y: [-2.00649142  1.77722988  0.5185386  -0.00800327 -0.00900875 -0.00412709], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1148343.1054827906, Loss on the last accepted step: 22.573069057682215, Step size: 4.656612873077393e-10, y: [-2.00610404  1.77791896  0.51914299 -0.00733159 -0.00781873 -0.00308297], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 286042.3052457798, Loss on the last accepted step: 22.573069057682215, Step size: 2.3283064365386963e-10, y: [-2.00591035  1.77826351  0.51944519 -0.00699575 -0.00722372 -0.00256091], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 70997.56991652744, Loss on the last accepted step: 22.573069057682215, Step size: 1.1641532182693481e-10, y: [-2.00581351  1.77843578  0.51959628 -0.00682783 -0.00692622 -0.00229988], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 17501.387473158673, Loss on the last accepted step: 22.573069057682215, Step size: 5.820766091346741e-11, y: [-2.00576509  1.77852192  0.51967183 -0.00674387 -0.00677747 -0.00216937], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4259.81340728817, Loss on the last accepted step: 22.573069057682215, Step size: 2.9103830456733704e-11, y: [-2.00574087  1.77856498  0.51970961 -0.00670189 -0.00670309 -0.00210411], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1015.6520619330806, Loss on the last accepted step: 22.573069057682215, Step size: 1.4551915228366852e-11, y: [-2.00572877  1.77858652  0.51972849 -0.0066809  -0.0066659  -0.00207148], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 237.72732879440176, Loss on the last accepted step: 22.573069057682215, Step size: 7.275957614183426e-12, y: [-2.00572272  1.77859728  0.51973794 -0.0066704  -0.00664731 -0.00205517], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 59.80389829475062, Loss on the last accepted step: 22.573069057682215, Step size: 3.637978807091713e-12, y: [-2.00571969  1.77860267  0.51974266 -0.00666515 -0.00663801 -0.00204701], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 23.60190956181135, Loss on the last accepted step: 22.573069057682215, Step size: 1.8189894035458565e-12, y: [-2.00571818  1.77860536  0.51974502 -0.00666253 -0.00663336 -0.00204293], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18.69084593908376, Loss on the last accepted step: 22.573069057682215, Step size: 1.0, y: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656163577790677.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.5, y: [-164090.84447962 -293279.46017563 -257137.284282     55795.69437065\n",
      "   97524.25599093   85838.89068364], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656219432660415.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.25, y: [ -82046.42509852 -146638.84078446 -128568.3822679    27897.84385472\n",
      "   48762.12467995   42919.44432137], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656335079169682.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.125, y: [-41024.21540797 -73318.53108888 -64283.93126085  13948.91859675\n",
      "  24381.05902445  21459.72114024], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656597900918513.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0625, y: [-20513.1105627  -36658.37624109 -32141.70575732   6974.45596777\n",
      "  12190.52619671  10729.85954967], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6657376519418923.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.03125, y: [-10257.55814006 -18328.29881719 -16070.59300556   3487.22465327\n",
      "   6095.25978283   5364.92875439], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6660973268893504.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.015625, y: [-5129.78192874 -9163.26010524 -8035.03662968  1743.60899603\n",
      "  3047.6265759   2682.46335675], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6684888181703251.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0078125, y: [-2565.89382308 -4580.74074927 -4017.25844174   871.80116741\n",
      "  1523.80997243  1341.23065793], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6881824413960419.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00390625, y: [-1283.94977025 -2289.48107128 -2008.36934777   435.89725309\n",
      "   761.9016707    670.61430852], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 9977300397153084.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.001953125, y: [ -642.97774384 -1143.85123229 -1003.92480078   217.94529594\n",
      "   380.94751983   335.30613381], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3545060847167142.5, Loss on the last accepted step: 18.69084593908376, Step size: 0.0009765625, y: [-322.49173063 -571.03631279 -501.70252729  108.96931736  190.47044439\n",
      "  167.65204646], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4102477456645055.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00048828125, y: [-162.24872402 -284.62885304 -250.59139055   54.48132807   95.23190668\n",
      "   83.82500278], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 5559598981456660.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.000244140625, y: [ -82.12722072 -141.42512317 -125.03582217   27.23733343   47.61263782\n",
      "   41.91148095], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 412895572872776.2, Loss on the last accepted step: 18.69084593908376, Step size: 0.0001220703125, y: [-42.06646907 -69.82325823 -62.25803799  13.6153361   23.80300339\n",
      "  20.95472003], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 81851111819179.84, Loss on the last accepted step: 18.69084593908376, Step size: 6.103515625e-05, y: [-22.03609325 -34.02232576 -30.86914589   6.80433744  11.89818618\n",
      "  10.47633957], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 18050209225659.055, Loss on the last accepted step: 18.69084593908376, Step size: 3.0517578125e-05, y: [-12.02090533 -16.12185953 -15.17469985   3.39883811   5.94577757\n",
      "   5.23714934], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4228174375397.174, Loss on the last accepted step: 18.69084593908376, Step size: 1.52587890625e-05, y: [-7.01331138 -7.17162641 -7.32747682  1.69608845  2.96957326  2.61755422], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1022966504134.654, Loss on the last accepted step: 18.69084593908376, Step size: 7.62939453125e-06, y: [-4.5095144  -2.69650985 -3.40386531  0.84471361  1.48147111  1.30775666], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 251728138122.1346, Loss on the last accepted step: 18.69084593908376, Step size: 3.814697265625e-06, y: [-3.25761591 -0.45895157 -1.44205955  0.4190262   0.73742004  0.65285789], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 62491708224.42246, Loss on the last accepted step: 18.69084593908376, Step size: 1.9073486328125e-06, y: [-2.63166666  0.65982757 -0.46115668  0.20618249  0.3653945   0.3254085 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15578336950.830612, Loss on the last accepted step: 18.69084593908376, Step size: 9.5367431640625e-07, y: [-2.31869204  1.21921714  0.02929476  0.09976064  0.17938173  0.1616838 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3890045900.1679, Loss on the last accepted step: 18.69084593908376, Step size: 4.76837158203125e-07, y: [-2.16220473  1.49891192  0.27452048  0.04654971  0.08637535  0.07982145], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 971993276.5072057, Loss on the last accepted step: 18.69084593908376, Step size: 2.384185791015625e-07, y: [-2.08396108  1.63875931  0.39713334  0.01994425  0.03987215  0.03889028], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 242921974.5233047, Loss on the last accepted step: 18.69084593908376, Step size: 1.1920928955078125e-07, y: [-2.04483925  1.70868301  0.45843977  0.00664151  0.01662056  0.01842469], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 60712771.75720936, Loss on the last accepted step: 18.69084593908376, Step size: 5.960464477539063e-08, y: [-2.02527833e+00  1.74364486e+00  4.89092986e-01 -9.85187798e-06\n",
      "  4.99475933e-03  8.19190096e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15171741.93509069, Loss on the last accepted step: 18.69084593908376, Step size: 2.9802322387695312e-08, y: [-2.01549788e+00  1.76112578e+00  5.04419593e-01 -3.33553482e-03\n",
      " -8.18139701e-04  3.07550425e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3790007.78187058, Loss on the last accepted step: 18.69084593908376, Step size: 1.4901161193847656e-08, y: [-2.01060765e+00  1.76986624e+00  5.12082897e-01 -4.99837629e-03\n",
      " -3.72458922e-03  5.17305892e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 946080.8469193219, Loss on the last accepted step: 18.69084593908376, Step size: 7.450580596923828e-09, y: [-2.00816253e+00  1.77423647e+00  5.15914549e-01 -5.82979702e-03\n",
      " -5.17781397e-03 -7.61793285e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 235821.10599754317, Loss on the last accepted step: 18.69084593908376, Step size: 3.725290298461914e-09, y: [-2.00693998e+00  1.77642159e+00  5.17830375e-01 -6.24550739e-03\n",
      " -5.90442635e-03 -1.40134287e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 58613.28431119843, Loss on the last accepted step: 18.69084593908376, Step size: 1.862645149230957e-09, y: [-2.00632870e+00  1.77751415e+00  5.18788288e-01 -6.45336257e-03\n",
      " -6.26773254e-03 -1.72111767e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 14489.40284052151, Loss on the last accepted step: 18.69084593908376, Step size: 9.313225746154785e-10, y: [-2.00602306e+00  1.77806043e+00  5.19267244e-01 -6.55729017e-03\n",
      " -6.44938564e-03 -1.88100507e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3547.4092573621597, Loss on the last accepted step: 18.69084593908376, Step size: 4.656612873077393e-10, y: [-2.00587024e+00  1.77833357e+00  5.19506723e-01 -6.60925396e-03\n",
      " -6.54021218e-03 -1.96094876e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 856.3917254968645, Loss on the last accepted step: 18.69084593908376, Step size: 2.3283064365386963e-10, y: [-2.00579383e+00  1.77847014e+00  5.19626462e-01 -6.63523586e-03\n",
      " -6.58562546e-03 -2.00092061e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 205.8768389403899, Loss on the last accepted step: 18.69084593908376, Step size: 1.1641532182693481e-10, y: [-2.00575563  1.77853842  0.51968633 -0.00664823 -0.00660833 -0.00202091], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 54.367747074627246, Loss on the last accepted step: 18.69084593908376, Step size: 5.820766091346741e-11, y: [-2.00573652  1.77857256  0.51971627 -0.00665472 -0.00661969 -0.0020309 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 22.050274190831995, Loss on the last accepted step: 18.69084593908376, Step size: 2.9103830456733704e-11, y: [-2.00572697  1.77858963  0.51973123 -0.00665797 -0.00662536 -0.0020359 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 16.75080474090859, Loss on the last accepted step: 18.69084593908376, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 169644603169826.8, Loss on the last accepted step: 16.75080474090859, Step size: 0.5, y: [ -9.05464439 -13.81993504  22.79009627 -21.1900384  -51.30125272\n",
      "  72.09817681], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 118164505879627.69, Loss on the last accepted step: 16.75080474090859, Step size: 0.25, y: [ -5.53018329  -6.02066844  11.65491749 -10.598349   -25.65394046\n",
      "  36.04806921], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 59610682135552.16, Loss on the last accepted step: 16.75080474090859, Step size: 0.125, y: [ -3.76795274  -2.12103513   6.08732811  -5.3025043  -12.83028433\n",
      "  18.02301541], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23706295099936.734, Loss on the last accepted step: 16.75080474090859, Step size: 0.0625, y: [-2.88683747 -0.17121848  3.30353341 -2.65458194 -6.41845627  9.01048851], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7839821118969.897, Loss on the last accepted step: 16.75080474090859, Step size: 0.03125, y: [-2.44627983  0.80368984  1.91163606 -1.33062077 -3.21254223  4.50422506], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2291126953191.8936, Loss on the last accepted step: 16.75080474090859, Step size: 0.015625, y: [-2.22600101  1.29114401  1.21568739 -0.66864018 -1.60958522  2.25109333], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 622104027015.105, Loss on the last accepted step: 16.75080474090859, Step size: 0.0078125, y: [-2.11586161  1.53487109  0.86771305 -0.33764989 -0.80810671  1.12452747], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 162209252307.20477, Loss on the last accepted step: 16.75080474090859, Step size: 0.00390625, y: [-2.0607919   1.65673463  0.69372589 -0.17215474 -0.40736745  0.56124454], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 41401575908.518974, Loss on the last accepted step: 16.75080474090859, Step size: 0.001953125, y: [-2.03325705  1.7176664   0.6067323  -0.08940717 -0.20699783  0.27960307], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10454068085.194855, Loss on the last accepted step: 16.75080474090859, Step size: 0.0009765625, y: [-2.01948962  1.74813228  0.56323551 -0.04803338 -0.10681301  0.13878234], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2626075854.0582895, Loss on the last accepted step: 16.75080474090859, Step size: 0.00048828125, y: [-2.01260591  1.76336523  0.54148711 -0.02734649 -0.05672061  0.06837197], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 658055204.7769172, Loss on the last accepted step: 16.75080474090859, Step size: 0.000244140625, y: [-2.00916405  1.7709817   0.53061292 -0.01700304 -0.0316744   0.03316679], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 164703543.9706092, Loss on the last accepted step: 16.75080474090859, Step size: 0.0001220703125, y: [-2.00744312  1.77478993  0.52517582 -0.01183132 -0.0191513   0.0155642 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 41199466.63779612, Loss on the last accepted step: 16.75080474090859, Step size: 6.103515625e-05, y: [-2.00658266  1.77669405  0.52245727 -0.00924546 -0.01288975  0.0067629 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10302815.7094152, Loss on the last accepted step: 16.75080474090859, Step size: 3.0517578125e-05, y: [-2.00615243  1.77764611  0.52109799 -0.00795252 -0.00975898  0.00236225], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2576082.8629744216, Loss on the last accepted step: 16.75080474090859, Step size: 1.52587890625e-05, y: [-2.00593731e+00  1.77812214e+00  5.20418355e-01 -7.30605931e-03\n",
      " -8.19358811e-03  1.61929728e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 644078.9871490183, Loss on the last accepted step: 16.75080474090859, Step size: 7.62939453125e-06, y: [-2.00582975e+00  1.77836016e+00  5.20078536e-01 -6.98282660e-03\n",
      " -7.41089426e-03 -9.38232247e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 161037.99518129436, Loss on the last accepted step: 16.75080474090859, Step size: 3.814697265625e-06, y: [-2.00577597e+00  1.77847916e+00  5.19908627e-01 -6.82121024e-03\n",
      " -7.01954733e-03 -1.48831323e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 40272.75941962693, Loss on the last accepted step: 16.75080474090859, Step size: 1.9073486328125e-06, y: [-2.00574909e+00  1.77853867e+00  5.19823672e-01 -6.74040207e-03\n",
      " -6.82387387e-03 -1.76335373e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10080.833671238892, Loss on the last accepted step: 16.75080474090859, Step size: 9.5367431640625e-07, y: [-2.00573564e+00  1.77856842e+00  5.19781195e-01 -6.69999798e-03\n",
      " -6.72603713e-03 -1.90087398e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2532.778357383092, Loss on the last accepted step: 16.75080474090859, Step size: 4.76837158203125e-07, y: [-2.00572892e+00  1.77858329e+00  5.19759956e-01 -6.67979593e-03\n",
      " -6.67711877e-03 -1.96963410e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 645.7569269823059, Loss on the last accepted step: 16.75080474090859, Step size: 2.384185791015625e-07, y: [-2.00572556e+00  1.77859073e+00  5.19749337e-01 -6.66969491e-03\n",
      " -6.65265958e-03 -2.00401416e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 174.00142949487125, Loss on the last accepted step: 16.75080474090859, Step size: 1.1920928955078125e-07, y: [-2.00572388  1.77859445  0.51974403 -0.00666464 -0.00664043 -0.0020212 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 56.062942112056135, Loss on the last accepted step: 16.75080474090859, Step size: 5.960464477539063e-08, y: [-2.00572304  1.77859631  0.51974137 -0.00666212 -0.00663432 -0.0020298 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26.57857151241832, Loss on the last accepted step: 16.75080474090859, Step size: 2.9802322387695312e-08, y: [-2.00572262  1.77859724  0.51974004 -0.00666086 -0.00663126 -0.0020341 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.20761179275799, Loss on the last accepted step: 16.75080474090859, Step size: 1.4901161193847656e-08, y: [-2.00572241  1.7785977   0.51973938 -0.00666023 -0.00662973 -0.00203625], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.364938889191162, Loss on the last accepted step: 16.75080474090859, Step size: 7.450580596923828e-09, y: [-2.0057223   1.77859794  0.51973905 -0.00665991 -0.00662896 -0.00203732], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.90430463201307, Loss on the last accepted step: 16.75080474090859, Step size: 3.725290298461914e-09, y: [-2.00572225  1.77859805  0.51973888 -0.00665975 -0.00662858 -0.00203786], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.789162834089133, Loss on the last accepted step: 16.75080474090859, Step size: 1.862645149230957e-09, y: [-2.00572222  1.77859811  0.5197388  -0.00665967 -0.00662839 -0.00203813], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.76038583457887, Loss on the last accepted step: 16.75080474090859, Step size: 9.313225746154785e-10, y: [-2.00572221  1.77859814  0.51973876 -0.00665963 -0.0066283  -0.00203826], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.753195796672802, Loss on the last accepted step: 16.75080474090859, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859816  0.51973874 -0.00665961 -0.00662825 -0.00203833], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751400396022706, Loss on the last accepted step: 16.75080474090859, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859816  0.51973873 -0.0066596  -0.00662822 -0.00203836], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750952600210827, Loss on the last accepted step: 16.75080474090859, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75084117851343, Loss on the last accepted step: 16.75080474090859, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750813586596493, Loss on the last accepted step: 16.75080474090859, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750806820486787, Loss on the last accepted step: 16.75080474090859, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750805194843167, Loss on the last accepted step: 16.75080474090859, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804821409293, Loss on the last accepted step: 16.75080474090859, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804744509423, Loss on the last accepted step: 16.75080474090859, Step size: 1.8189894035458565e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473354687, Loss on the last accepted step: 16.75080474090859, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 3922990983.8954067, Loss on the last accepted step: 16.75080473354687, Step size: 0.5, y: [-2.6501976   2.53635291  0.06497501 -0.94988438  0.32484086  0.22920446], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1445432804.8535957, Loss on the last accepted step: 16.75080473354687, Step size: 0.25, y: [-2.3279599   2.15747554  0.29235687 -0.47827199  0.15910633  0.11358303], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 459569595.31041497, Loss on the last accepted step: 16.75080473354687, Step size: 0.125, y: [-2.16684105  1.96803685  0.40604779 -0.24246579  0.07623906  0.05577232], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 132130088.26122552, Loss on the last accepted step: 16.75080473354687, Step size: 0.0625, y: [-2.08628162  1.87331751  0.46289325 -0.12456269  0.03480543  0.02686696], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 35684556.50337921, Loss on the last accepted step: 16.75080473354687, Step size: 0.03125, y: [-2.04600191  1.82595784  0.49131599 -0.06561114  0.01408862  0.01241428], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9294833.672423251, Loss on the last accepted step: 16.75080473354687, Step size: 0.015625, y: [-2.02586205  1.80227801  0.50552735 -0.03613537  0.00373021  0.00518795], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2373592.5596803604, Loss on the last accepted step: 16.75080473354687, Step size: 0.0078125, y: [-2.01579212e+00  1.79043809e+00  5.12633034e-01 -2.13974812e-02\n",
      " -1.44899648e-03  1.57477548e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 599863.1821710416, Loss on the last accepted step: 16.75080473354687, Step size: 0.00390625, y: [-2.01075716e+00  1.78451813e+00  5.16185876e-01 -1.40285376e-02\n",
      " -4.03859849e-03 -2.31809306e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 150799.3217221546, Loss on the last accepted step: 16.75080473354687, Step size: 0.001953125, y: [-2.00823968e+00  1.78155815e+00  5.17962297e-01 -1.03440658e-02\n",
      " -5.33339949e-03 -1.13510170e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 37815.93761301712, Loss on the last accepted step: 16.75080473354687, Step size: 0.0009765625, y: [-2.00698094e+00  1.78007816e+00  5.18850507e-01 -8.50182984e-03\n",
      " -5.98079999e-03 -1.58674789e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9479.54819185883, Loss on the last accepted step: 16.75080473354687, Step size: 0.00048828125, y: [-2.00635157e+00  1.77933816e+00  5.19294612e-01 -7.58071188e-03\n",
      " -6.30450024e-03 -1.81257099e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2384.078443023442, Loss on the last accepted step: 16.75080473354687, Step size: 0.000244140625, y: [-2.00603688e+00  1.77896817e+00  5.19516665e-01 -7.12015291e-03\n",
      " -6.46635037e-03 -1.92548254e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 608.7862317101427, Loss on the last accepted step: 16.75080473354687, Step size: 0.0001220703125, y: [-2.00587954e+00  1.77878317e+00  5.19627691e-01 -6.88987342e-03\n",
      " -6.54727543e-03 -1.98193832e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 164.78499399043434, Loss on the last accepted step: 16.75080473354687, Step size: 6.103515625e-05, y: [-2.00580087  1.77869067  0.5196832  -0.00677473 -0.00658774 -0.00201017], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 53.7624579919134, Loss on the last accepted step: 16.75080473354687, Step size: 3.0517578125e-05, y: [-2.00576153  1.77864442  0.51971096 -0.00671716 -0.00660797 -0.00202428], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26.004075326244468, Loss on the last accepted step: 16.75080473354687, Step size: 1.52587890625e-05, y: [-2.00574186  1.77862129  0.51972484 -0.00668838 -0.00661808 -0.00203134], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.064151777016612, Loss on the last accepted step: 16.75080473354687, Step size: 7.62939453125e-06, y: [-2.00573203  1.77860973  0.51973178 -0.00667399 -0.00662314 -0.00203487], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.32913748775177, Loss on the last accepted step: 16.75080473354687, Step size: 3.814697265625e-06, y: [-2.00572711  1.77860395  0.51973525 -0.00666679 -0.00662567 -0.00203663], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.895383545115955, Loss on the last accepted step: 16.75080473354687, Step size: 1.9073486328125e-06, y: [-2.00572465  1.77860106  0.51973698 -0.00666319 -0.00662694 -0.00203751], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.786947073359862, Loss on the last accepted step: 16.75080473354687, Step size: 9.5367431640625e-07, y: [-2.00572343  1.77859962  0.51973785 -0.00666139 -0.00662757 -0.00203795], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75983917349073, Loss on the last accepted step: 16.75080473354687, Step size: 4.76837158203125e-07, y: [-2.00572281  1.77859889  0.51973828 -0.00666049 -0.00662788 -0.00203817], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.753062803541717, Loss on the last accepted step: 16.75080473354687, Step size: 2.384185791015625e-07, y: [-2.0057225   1.77859853  0.5197385  -0.00666004 -0.00662804 -0.00203828], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751369052565106, Loss on the last accepted step: 16.75080473354687, Step size: 1.1920928955078125e-07, y: [-2.00572235  1.77859835  0.51973861 -0.00665982 -0.00662812 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75094569434059, Loss on the last accepted step: 16.75080473354687, Step size: 5.960464477539063e-08, y: [-2.00572227  1.77859826  0.51973866 -0.00665971 -0.00662816 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750840046601585, Loss on the last accepted step: 16.75080473354687, Step size: 2.9802322387695312e-08, y: [-2.00572223  1.77859821  0.51973869 -0.00665965 -0.00662818 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750813522324403, Loss on the last accepted step: 16.75080473354687, Step size: 1.4901161193847656e-08, y: [-2.00572222  1.77859819  0.5197387  -0.00665962 -0.00662819 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750806876778192, Loss on the last accepted step: 16.75080473354687, Step size: 7.450580596923828e-09, y: [-2.00572221  1.77859818  0.51973871 -0.00665961 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080534286355, Loss on the last accepted step: 16.75080473354687, Step size: 3.725290298461914e-09, y: [-2.0057222   1.77859818  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804902737123, Loss on the last accepted step: 16.75080473354687, Step size: 1.862645149230957e-09, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080478121115, Loss on the last accepted step: 16.75080473354687, Step size: 9.313225746154785e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804748710618, Loss on the last accepted step: 16.75080473354687, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804738944353, Loss on the last accepted step: 16.75080473354687, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473570971, Loss on the last accepted step: 16.75080473354687, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804734426474, Loss on the last accepted step: 16.75080473354687, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804733889535, Loss on the last accepted step: 16.75080473354687, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473373192, Loss on the last accepted step: 16.75080473354687, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804733601424, Loss on the last accepted step: 16.75080473354687, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473353612, Loss on the last accepted step: 16.75080473354687, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 161642.6237495841, Loss on the last accepted step: 16.75080473353612, Step size: 0.5, y: [-7.46735314  4.46343793  0.93068091 -6.22053221  3.0986233   0.47273257], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 103163.30612481372, Loss on the last accepted step: 16.75080473353612, Step size: 0.25, y: [-4.73653767  3.12101805  0.72520981 -3.1135959   1.54599755  0.23534709], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 57050.3179006556, Loss on the last accepted step: 16.75080473353612, Step size: 0.125, y: [-3.37112993  2.44980811  0.62247427 -1.56012775  0.76968467  0.11665435], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26155.623737700727, Loss on the last accepted step: 16.75080473353612, Step size: 0.0625, y: [-2.68842606  2.11420314  0.57110649 -0.78339367  0.38152824  0.05730798], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9935.890076840564, Loss on the last accepted step: 16.75080473353612, Step size: 0.03125, y: [-2.34707413  1.94640065  0.5454226  -0.39502663  0.18745002  0.02763479], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 3233.2974048432666, Loss on the last accepted step: 16.75080473353612, Step size: 0.015625, y: [-2.17639816  1.86249941  0.53258066 -0.20084311  0.09041091  0.0127982 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 950.2824371766724, Loss on the last accepted step: 16.75080473353612, Step size: 0.0078125, y: [-2.09106018  1.82054879  0.52615969 -0.10375135  0.04189135  0.0053799 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 269.7200085662807, Loss on the last accepted step: 16.75080473353612, Step size: 0.00390625, y: [-2.04839119e+00  1.79957348e+00  5.22949203e-01 -5.52054738e-02\n",
      "  1.76315768e-02  1.67075410e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 82.68537503572642, Loss on the last accepted step: 16.75080473353612, Step size: 0.001953125, y: [-2.02705669e+00  1.78908583e+00  5.21343960e-01 -3.09325338e-02\n",
      "  5.50168817e-03 -1.83819993e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 33.57653878709546, Loss on the last accepted step: 16.75080473353612, Step size: 0.0009765625, y: [-2.01638944e+00  1.78384200e+00  5.20541339e-01 -1.87960639e-02\n",
      " -5.63256159e-04 -1.11110704e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 20.994485466267555, Loss on the last accepted step: 16.75080473353612, Step size: 0.00048828125, y: [-2.01105582e+00  1.78122008e+00  5.20140028e-01 -1.27278289e-02\n",
      " -3.59572833e-03 -1.57475057e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.81306965053772, Loss on the last accepted step: 16.75080473353612, Step size: 0.000244140625, y: [-2.00838901e+00  1.77990913e+00  5.19939373e-01 -9.69371142e-03\n",
      " -5.11196441e-03 -1.80657233e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.014847160561676, Loss on the last accepted step: 16.75080473353612, Step size: 0.0001220703125, y: [-2.00705560e+00  1.77925365e+00  5.19839045e-01 -8.17665268e-03\n",
      " -5.87008245e-03 -1.92248321e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.8157768381781, Loss on the last accepted step: 16.75080473353612, Step size: 6.103515625e-05, y: [-2.00638890e+00  1.77892591e+00  5.19788881e-01 -7.41812331e-03\n",
      " -6.24914147e-03 -1.98043865e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.766493885795505, Loss on the last accepted step: 16.75080473353612, Step size: 3.0517578125e-05, y: [-2.00605555  1.77876204  0.5197638  -0.00703886 -0.00643867 -0.00200942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.754445648520665, Loss on the last accepted step: 16.75080473353612, Step size: 1.52587890625e-05, y: [-2.00588887  1.7786801   0.51975126 -0.00684923 -0.00653344 -0.00202391], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751573784889075, Loss on the last accepted step: 16.75080473353612, Step size: 7.62939453125e-06, y: [-2.00580553  1.77863914  0.51974499 -0.00675441 -0.00658082 -0.00203115], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750926239653406, Loss on the last accepted step: 16.75080473353612, Step size: 3.814697265625e-06, y: [-2.00576386  1.77861865  0.51974185 -0.006707   -0.00660451 -0.00203477], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75079990814405, Loss on the last accepted step: 16.75080473353612, Step size: 1.9073486328125e-06, y: [-2.00574303  1.77860841  0.51974028 -0.0066833  -0.00661635 -0.00203658], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75078588270273, Loss on the last accepted step: 16.75080473353612, Step size: 1.0, y: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 107.80584232938871, Loss on the last accepted step: 16.75078588270273, Step size: 0.5, y: [-3.78810545  2.65871305  0.65442978  0.09113483 -0.05335951 -0.00921925], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 38.35965783947903, Loss on the last accepted step: 16.75078588270273, Step size: 0.25, y: [-2.89691903  2.21865817  0.58708464  0.04223169 -0.02999089 -0.00562837], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 21.467829625356504, Loss on the last accepted step: 16.75078588270273, Step size: 0.125, y: [-2.45132582  1.99863073  0.55341207  0.01778012 -0.01830659 -0.00383293], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 17.774846328932956, Loss on the last accepted step: 16.75078588270273, Step size: 0.0625, y: [-2.22852922  1.88861701  0.53657579  0.00555434 -0.01246443 -0.00293521], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.92940743062247, Loss on the last accepted step: 16.75078588270273, Step size: 0.03125, y: [-2.11713092e+00  1.83361015e+00  5.28157644e-01 -5.58553654e-04\n",
      " -9.54335464e-03 -2.48634845e-03], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.748791298705314, Loss on the last accepted step: 16.75078588270273, Step size: 0.015625, y: [-2.06143176  1.80610672  0.52394857 -0.003615   -0.00808282 -0.00226192], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.724982128951844, Loss on the last accepted step: 16.75078588270273, Step size: 1.0, y: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ], y on the last accepted step: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ]\n",
      "Loss on this step: 17.146241008157222, Loss on the last accepted step: 16.724982128951844, Step size: 0.5, y: [-1.94698309e+00  1.74959975e+00  5.15304570e-01 -9.84998454e-03\n",
      " -5.10605390e-03 -1.80450435e-03], y on the last accepted step: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ]\n",
      "Loss on this step: 16.605234340886252, Loss on the last accepted step: 16.724982128951844, Step size: 1.0, y: [-1.99028264e+00  1.77097738e+00  5.18574303e-01 -7.49660371e-03\n",
      " -6.22930042e-03 -1.97710393e-03], y on the last accepted step: [-1.99028264e+00  1.77097738e+00  5.18574303e-01 -7.49660371e-03\n",
      " -6.22930042e-03 -1.97710393e-03]\n",
      "Loss on this step: 16.542555673968565, Loss on the last accepted step: 16.605234340886252, Step size: 1.0, y: [-2.00576117  1.77862013  0.51974359 -0.00665737 -0.00662993 -0.00203912], y on the last accepted step: [-2.00576117  1.77862013  0.51974359 -0.00665737 -0.00662993 -0.00203912]\n",
      "Loss on this step: 16.540904042046847, Loss on the last accepted step: 16.542555673968565, Step size: 1.0, y: [-2.0061081   1.77879133  0.51976976 -0.00663889 -0.00663875 -0.00204039], y on the last accepted step: [-2.0061081   1.77879133  0.51976976 -0.00663889 -0.00663875 -0.00204039]\n",
      "Loss on this step: 16.540893651014116, Loss on the last accepted step: 16.540904042046847, Step size: 1.0, y: [-2.00569725  1.77858848  0.51973873 -0.00666135 -0.00662802 -0.00203875], y on the last accepted step: [-2.00569725  1.77858848  0.51973873 -0.00666135 -0.00662802 -0.00203875]\n",
      "Loss on this step: 16.540893597693966, Loss on the last accepted step: 16.540893651014116, Step size: 1.0, y: [-2.00572371  1.77860155  0.51974072 -0.00665989 -0.00662872 -0.00203885], y on the last accepted step: [-2.00572371  1.77860155  0.51974072 -0.00665989 -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768823, Loss on the last accepted step: 16.540893597693966, Step size: 1.0, y: [-2.00572341  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572341  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768805, Loss on the last accepted step: 16.54089359768823, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687987, Loss on the last accepted step: 16.54089359768805, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687987, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.54089359768798, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.5, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.25, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.03125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.015625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0078125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00390625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.001953125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0009765625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00048828125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.000244140625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0001220703125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 6.103515625e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.0517578125e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.52587890625e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 7.62939453125e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.814697265625e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.9073486328125e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 9.5367431640625e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 4.76837158203125e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.384185791015625e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.1920928955078125e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 5.960464477539063e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.9802322387695312e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.4901161193847656e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n"
     ]
    }
   ],
   "source": [
    "def func(x, args):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "solver = optimistix.BFGS(\n",
    "    rtol=1e-12,\n",
    "    atol=1e-12,\n",
    "    verbose=frozenset({\"step_size\", \"loss\", \"y\"}),\n",
    "    use_inverse=True,\n",
    ")\n",
    "res = optimistix.minimise(\n",
    "    fn=func,\n",
    "    solver=solver,\n",
    "    y0=jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 711.1x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Omegas = jnp.linspace(0, 2 * jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "incs = jnp.arccos(jnp.linspace(-1, 1, 11))[:-1] * 180 / jnp.pi\n",
    "# nus = jnp.linspace(0, 2 * jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "# nus = jnp.array([jnp.pi/4, 7*jnp.pi/4]) * 180 / jnp.pi\n",
    "# Omegas = jnp.array([0.0]) * 180 / jnp.pi\n",
    "incs = jnp.linspace(0, jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "\n",
    "arrs = []\n",
    "for Omega in Omegas:\n",
    "    for inc in incs:\n",
    "        for _nu in range(10):\n",
    "            arrs.append(\n",
    "                jnp.array([5.0, 0.0, np.random.uniform(0, 360), inc, Omega, 0.0])\n",
    "            )\n",
    "arrs = jnp.array(arrs)\n",
    "\n",
    "k = KeplerianState(\n",
    "    semi=arrs[:, 0],\n",
    "    ecc=arrs[:, 1],\n",
    "    nu=arrs[:, 2],\n",
    "    inc=arrs[:, 3],\n",
    "    Omega=arrs[:, 4],\n",
    "    omega=arrs[:, 5],\n",
    "    time=times[0],\n",
    ")\n",
    "c = k.to_cartesian()\n",
    "x = icrs_to_horizons_ecliptic(c.x)\n",
    "v = icrs_to_horizons_ecliptic(c.v)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(x[:, 0], x[:, 1], x[:, 2])\n",
    "ax.set(aspect=\"equal\")\n",
    "\n",
    "# phi = Omegas * jnp.pi / 180\n",
    "# theta = incs * jnp.pi / 180\n",
    "# x = jnp.outer(jnp.sin(theta), jnp.cos(phi))\n",
    "# y = jnp.outer(jnp.sin(theta), jnp.sin(phi))\n",
    "# z = jnp.outer(jnp.cos(theta), jnp.ones_like(phi))\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# ax.scatter(x, y, z, alpha=0.5)\n",
    "# ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.hstack([x, v])\n",
    "r = jax.vmap(p0.loglike)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = jax.vmap(p0.loglike)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-10947237.04957968, dtype=float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:29, 33.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "j = np.zeros(len(r))\n",
    "for i, c in tqdm(enumerate(x)):\n",
    "    j[i] = p0.loglike(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 0.08682409,  0.49240388, -0.8660254 ]], dtype=float64),\n",
       " Array([ 0.08682409,  0.49240388, -0.8660254 ], dtype=float64))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phi = p0._observations.ra[0]\n",
    "# theta = jnp.pi/2 - p0._observations.dec[0]\n",
    "\n",
    "i = 2\n",
    "j = 3\n",
    "phi = jnp.linspace(0, 2 * jnp.pi, 10)[i]\n",
    "theta = jnp.pi / 2 - jnp.linspace(-jnp.pi, jnp.pi, 10)[j]\n",
    "\n",
    "\n",
    "x = jnp.sin(theta) * jnp.cos(phi)\n",
    "y = jnp.sin(theta) * jnp.sin(phi)\n",
    "z = jnp.cos(theta)\n",
    "\n",
    "\n",
    "x_icrs = jnp.hstack([x, y, z])\n",
    "x = icrs_to_horizons_ecliptic(x_icrs)  # * 3.0\n",
    "\n",
    "\n",
    "# assume we're observing the thing at its highest excursion from the ecliptic:\n",
    "inc = jnp.array([jnp.abs(jnp.arcsin(x[2])) / jnp.linalg.norm(x) * 180 / jnp.pi])\n",
    "\n",
    "# its longitude of ascending node is the angle between the x-axis and the projection of the vector onto the xy-plane:\n",
    "varphi = (jnp.arctan2(x[1], x[0]) * 180 / jnp.pi) % 360\n",
    "Omega = jnp.array([varphi]) - 90 if x[2] > 0 else jnp.array([varphi]) + 90\n",
    "\n",
    "nu = jnp.array([90.0]) if x[2] > 0 else jnp.array([270.0])\n",
    "a = jnp.array([3.0])\n",
    "ecc = jnp.array([0.0])\n",
    "omega = jnp.array([0.0])\n",
    "\n",
    "k = KeplerianState(\n",
    "    semi=a, ecc=ecc, nu=nu, inc=inc, Omega=Omega, omega=omega, time=times[0]\n",
    ")\n",
    "c = k.to_cartesian()\n",
    "c.x / 3.0, x_icrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.68373692,  1.32528782,  0.20289981, -0.00432348, -0.00821017,\n",
       "       -0.00355955], dtype=float64)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_initial_guess(ra, dec):\n",
    "    phi = ra\n",
    "    theta = jnp.pi / 2 - dec\n",
    "\n",
    "    x = jnp.sin(theta) * jnp.cos(phi)\n",
    "    y = jnp.sin(theta) * jnp.sin(phi)\n",
    "    z = jnp.cos(theta)\n",
    "\n",
    "    x_icrs = jnp.hstack([x, y, z])\n",
    "    x = icrs_to_horizons_ecliptic(x_icrs)\n",
    "\n",
    "    # assume we're observing the thing at its highest excursion from the ecliptic:\n",
    "    inc = jnp.array([jnp.abs(jnp.arcsin(x[2])) / jnp.linalg.norm(x) * 180 / jnp.pi])\n",
    "\n",
    "    # its longitude of ascending node is the angle between the x-axis and the projection of the vector onto the xy-plane:\n",
    "    varphi = (jnp.arctan2(x[1], x[0]) * 180 / jnp.pi) % 360\n",
    "    Omega = jnp.array([varphi]) - 90 if x[2] > 0 else jnp.array([varphi]) + 90\n",
    "\n",
    "    nu = jnp.array([90.0]) if x[2] > 0 else jnp.array([270.0])\n",
    "    a = jnp.array([3.0])\n",
    "    ecc = jnp.array([0.0])\n",
    "    omega = jnp.array([0.0])\n",
    "\n",
    "    k = KeplerianState(\n",
    "        semi=a, ecc=ecc, nu=nu, inc=inc, Omega=Omega, omega=omega, time=times[0]\n",
    "    )\n",
    "    c = k.to_cartesian()\n",
    "\n",
    "    return jnp.concatenate([c.x.flatten(), c.v.flatten()])\n",
    "\n",
    "\n",
    "generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 267, initial cost 2.0803e+10, final cost 2.0829e+01, first-order optimality 2.81e+05.\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.0829e+01                                    1.51e+03    \n",
      "       1             13         2.0829e+01      1.08e-05       2.01e-08       2.17e+03    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 13, initial cost 2.0829e+01, final cost 2.0829e+01, first-order optimality 2.17e+03.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-37.37021373, dtype=float64)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "\n",
    "def tmp(x):\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    "    method=\"lm\",\n",
    ")\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=result.x,\n",
    "    verbose=2,\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-2.68373692,  1.32528782,  0.20289981, -0.00432348, -0.00821017,\n",
       "        -0.00355955], dtype=float64),\n",
       " Array([-2.00571666,  1.77860805,  0.51974738], dtype=float64))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0, perturbed.x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 60350.47697017, -29348.02941075],\n",
       "        [ 60368.24178112, -29360.07765625],\n",
       "        [ 60386.01812898, -29372.11240986],\n",
       "        [ 60771.72808604, -29636.19788502],\n",
       "        [ 60789.75940909, -29648.39425987],\n",
       "        [ 60807.80023077, -29660.57633796],\n",
       "        [ 62072.27698179, -30521.56535972],\n",
       "        [ 62091.07246654, -30534.19213625],\n",
       "        [ 62109.87083723, -30546.80199541]], dtype=float64),\n",
       " Array([[-0.90962391,  0.69842047],\n",
       "        [-0.90984353,  0.69858176],\n",
       "        [-0.91006152,  0.69873977],\n",
       "        [-0.91479967,  0.70217721],\n",
       "        [-0.9150206 ,  0.70233721],\n",
       "        [-0.91524007,  0.70249453],\n",
       "        [-0.93037155,  0.71334351],\n",
       "        [-0.93059056,  0.71350114],\n",
       "        [-0.93081201,  0.71365595]], dtype=float64))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(x0), p0.residuals(\n",
    "    jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_32615/2677624643.py:3: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.08026D+10    |proj g|=  8.59172D+10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.87312D+09    |proj g|=  1.34391D+10\n",
      "\n",
      "At iterate    2    f=  5.63142D+09    |proj g|=  1.38511D+10\n",
      "\n",
      "At iterate    3    f=  1.72328D+09    |proj g|=  9.57667D+09\n",
      "\n",
      "At iterate    4    f=  2.10574D+08    |proj g|=  4.36231D+09\n",
      "\n",
      "At iterate    5    f=  6.43936D+06    |proj g|=  1.21037D+09\n",
      "\n",
      "At iterate    6    f=  5.98118D+05    |proj g|=  4.08164D+08\n",
      "\n",
      "At iterate    7    f=  3.49487D+04    |proj g|=  3.54526D+07\n",
      "\n",
      "At iterate    8    f=  2.50987D+03    |proj g|=  4.57711D+06\n",
      "\n",
      "At iterate    9    f=  2.08509D+03    |proj g|=  2.21285D+06\n",
      "\n",
      "At iterate   10    f=  2.07084D+03    |proj g|=  1.35289D+05\n",
      "\n",
      "At iterate   11    f=  2.07047D+03    |proj g|=  1.19648D+05\n",
      "\n",
      "At iterate   12    f=  2.07010D+03    |proj g|=  2.10923D+04\n",
      "\n",
      "At iterate   13    f=  2.07008D+03    |proj g|=  1.55474D+04\n",
      "\n",
      "At iterate   14    f=  2.07006D+03    |proj g|=  1.51741D+04\n",
      "\n",
      "At iterate   15    f=  2.06996D+03    |proj g|=  4.51527D+04\n",
      "\n",
      "At iterate   16    f=  2.06976D+03    |proj g|=  8.84213D+04\n",
      "\n",
      "At iterate   17    f=  2.06918D+03    |proj g|=  1.64690D+05\n",
      "\n",
      "At iterate   18    f=  2.06771D+03    |proj g|=  2.84107D+05\n",
      "\n",
      "At iterate   19    f=  2.06381D+03    |proj g|=  4.79257D+05\n",
      "\n",
      "At iterate   20    f=  2.05370D+03    |proj g|=  7.91539D+05\n",
      "\n",
      "At iterate   21    f=  2.02746D+03    |proj g|=  1.28926D+06\n",
      "\n",
      "At iterate   22    f=  1.96060D+03    |proj g|=  2.05884D+06\n",
      "\n",
      "At iterate   23    f=  1.79733D+03    |proj g|=  3.16348D+06\n",
      "\n",
      "At iterate   24    f=  1.44096D+03    |proj g|=  4.43769D+06\n",
      "\n",
      "At iterate   25    f=  8.52306D+02    |proj g|=  5.04203D+06\n",
      "\n",
      "At iterate   26    f=  2.95519D+02    |proj g|=  3.75197D+06\n",
      "\n",
      "At iterate   27    f=  5.45593D+01    |proj g|=  9.65256D+05\n",
      "\n",
      "At iterate   28    f=  2.22937D+01    |proj g|=  3.57105D+05\n",
      "\n",
      "At iterate   29    f=  2.17264D+01    |proj g|=  2.96716D+05\n",
      "\n",
      "At iterate   30    f=  2.16002D+01    |proj g|=  2.69197D+04\n",
      "\n",
      "At iterate   31    f=  2.15979D+01    |proj g|=  7.06181D+02\n",
      "\n",
      "At iterate   32    f=  2.15978D+01    |proj g|=  4.77760D+02\n",
      "\n",
      "At iterate   33    f=  2.15978D+01    |proj g|=  2.74173D+02\n",
      "\n",
      "At iterate   34    f=  2.15978D+01    |proj g|=  1.15808D+01\n",
      "\n",
      "At iterate   35    f=  2.15978D+01    |proj g|=  1.15833D+01\n",
      "\n",
      "At iterate   36    f=  2.15978D+01    |proj g|=  1.54316D+01\n",
      "\n",
      "At iterate   37    f=  2.15978D+01    |proj g|=  2.16900D+01\n",
      "\n",
      "At iterate   38    f=  2.15978D+01    |proj g|=  7.25578D+01\n",
      "\n",
      "At iterate   39    f=  2.15978D+01    |proj g|=  1.14733D+02\n",
      "\n",
      "At iterate   40    f=  2.15978D+01    |proj g|=  2.07047D+02\n",
      "\n",
      "At iterate   41    f=  2.15978D+01    |proj g|=  3.38652D+02\n",
      "\n",
      "At iterate   42    f=  2.15978D+01    |proj g|=  5.62889D+02\n",
      "\n",
      "At iterate   43    f=  2.15978D+01    |proj g|=  9.17398D+02\n",
      "\n",
      "At iterate   44    f=  2.15978D+01    |proj g|=  1.49366D+03\n",
      "\n",
      "At iterate   45    f=  2.15978D+01    |proj g|=  2.42218D+03\n",
      "\n",
      "At iterate   46    f=  2.15977D+01    |proj g|=  3.92222D+03\n",
      "\n",
      "At iterate   47    f=  2.15975D+01    |proj g|=  6.34786D+03\n",
      "\n",
      "At iterate   48    f=  2.15970D+01    |proj g|=  1.02691D+04\n",
      "\n",
      "At iterate   49    f=  2.15956D+01    |proj g|=  1.66084D+04\n",
      "\n",
      "At iterate   50    f=  2.15919D+01    |proj g|=  2.68573D+04\n",
      "\n",
      "At iterate   51    f=  2.15824D+01    |proj g|=  4.34159D+04\n",
      "\n",
      "At iterate   52    f=  2.15575D+01    |proj g|=  7.01218D+04\n",
      "\n",
      "At iterate   53    f=  2.14931D+01    |proj g|=  1.12944D+05\n",
      "\n",
      "At iterate   54    f=  2.13286D+01    |proj g|=  1.80334D+05\n",
      "\n",
      "At iterate   55    f=  2.09274D+01    |proj g|=  2.79451D+05\n",
      "\n",
      "At iterate   56    f=  2.00617D+01    |proj g|=  3.88293D+05\n",
      "\n",
      "At iterate   57    f=  1.86946D+01    |proj g|=  4.48843D+05\n",
      "\n",
      "At iterate   58    f=  1.70426D+01    |proj g|=  3.63513D+05\n",
      "\n",
      "At iterate   59    f=  1.67319D+01    |proj g|=  3.68765D+05\n",
      "\n",
      "At iterate   60    f=  1.65455D+01    |proj g|=  2.42421D+04\n",
      "\n",
      "At iterate   61    f=  1.65417D+01    |proj g|=  1.41932D+04\n",
      "\n",
      "At iterate   62    f=  1.65410D+01    |proj g|=  6.09246D+03\n",
      "\n",
      "At iterate   63    f=  1.65409D+01    |proj g|=  3.66181D+03\n",
      "\n",
      "At iterate   64    f=  1.65409D+01    |proj g|=  6.79236D+02\n",
      "\n",
      "At iterate   65    f=  1.65409D+01    |proj g|=  5.96880D+02\n",
      "\n",
      "At iterate   66    f=  1.65409D+01    |proj g|=  6.23824D+02\n",
      "\n",
      "At iterate   67    f=  1.65409D+01    |proj g|=  6.48640D+01\n",
      "\n",
      "At iterate   68    f=  1.65409D+01    |proj g|=  5.70842D+01\n",
      "\n",
      "At iterate   69    f=  1.65409D+01    |proj g|=  1.51113D+01\n",
      "\n",
      "At iterate   70    f=  1.65409D+01    |proj g|=  1.70456D+01\n",
      "\n",
      "At iterate   71    f=  1.65409D+01    |proj g|=  8.19826D+00\n",
      "\n",
      "At iterate   72    f=  1.65409D+01    |proj g|=  3.06289D-01\n",
      "\n",
      "At iterate   73    f=  1.65409D+01    |proj g|=  5.44969D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     73     85      1     0     0   5.450D-02   1.654D+01\n",
      "  F =   16.540893597687987     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "result = minimize(\n",
    "    fun=lambda x: -p0.loglike(x),\n",
    "    x0=x0,\n",
    "    jac=lambda x: -jax.grad(p0.loglike)(x),\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_32615/2864640065.py:3: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.08026D+10    |proj g|=  8.59169D+10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.79468D+09    |proj g|=  1.64232D+10\n",
      "\n",
      "At iterate    2    f=  2.49217D+09    |proj g|=  8.37510D+09\n",
      "\n",
      "At iterate    3    f=  1.49127D+08    |proj g|=  3.43455D+09\n",
      "\n",
      "At iterate    4    f=  3.86313D+07    |proj g|=  3.16180D+09\n",
      "\n",
      "At iterate    5    f=  6.95561D+06    |proj g|=  1.00851D+09\n",
      "\n",
      "At iterate    6    f=  2.78564D+04    |proj g|=  6.61831D+07\n",
      "\n",
      "At iterate    7    f=  9.26132D+03    |proj g|=  1.68038D+07\n",
      "\n",
      "At iterate    8    f=  6.74918D+03    |proj g|=  1.10810D+07\n",
      "\n",
      "At iterate    9    f=  4.78945D+03    |proj g|=  7.49951D+06\n",
      "\n",
      "At iterate   10    f=  3.72583D+03    |proj g|=  4.72473D+06\n",
      "\n",
      "At iterate   11    f=  3.56047D+03    |proj g|=  5.15348D+05\n",
      "\n",
      "At iterate   12    f=  3.55960D+03    |proj g|=  7.46747D+04\n",
      "\n",
      "At iterate   13    f=  3.55958D+03    |proj g|=  1.17105D+05\n",
      "\n",
      "At iterate   14    f=  3.55956D+03    |proj g|=  6.12807D+04\n",
      "\n",
      "At iterate   15    f=  3.55953D+03    |proj g|=  1.14185D+05\n",
      "\n",
      "At iterate   16    f=  3.55953D+03    |proj g|=  2.18137D+04\n",
      "\n",
      "At iterate   17    f=  3.55953D+03    |proj g|=  2.14381D+04\n",
      "\n",
      "At iterate   18    f=  3.55952D+03    |proj g|=  2.61977D+04\n",
      "\n",
      "At iterate   19    f=  3.55948D+03    |proj g|=  5.91543D+04\n",
      "\n",
      "At iterate   20    f=  3.55941D+03    |proj g|=  7.07542D+04\n",
      "\n",
      "At iterate   21    f=  3.55875D+03    |proj g|=  1.28844D+05\n",
      "\n",
      "At iterate   22    f=  3.55772D+03    |proj g|=  2.47628D+05\n",
      "\n",
      "At iterate   23    f=  3.55658D+03    |proj g|=  4.25565D+05\n",
      "\n",
      "At iterate   24    f=  3.55543D+03    |proj g|=  3.63503D+05\n",
      "\n",
      "At iterate   25    f=  3.55543D+03    |proj g|=  2.79430D+05\n",
      "\n",
      "At iterate   26    f=  3.55476D+03    |proj g|=  2.91142D+05\n",
      "\n",
      "At iterate   27    f=  3.55419D+03    |proj g|=  2.42803D+05\n",
      "\n",
      "At iterate   28    f=  3.55395D+03    |proj g|=  1.24703D+05\n",
      "\n",
      "At iterate   29    f=  3.55393D+03    |proj g|=  1.02369D+05\n",
      "\n",
      "At iterate   30    f=  3.55390D+03    |proj g|=  1.57473D+05\n",
      "\n",
      "At iterate   31    f=  3.55362D+03    |proj g|=  1.53739D+05\n",
      "\n",
      "At iterate   32    f=  3.55346D+03    |proj g|=  1.14800D+05\n",
      "\n",
      "At iterate   33    f=  3.55343D+03    |proj g|=  1.34013D+05\n",
      "\n",
      "At iterate   34    f=  3.55338D+03    |proj g|=  6.07272D+04\n",
      "\n",
      "At iterate   35    f=  3.55338D+03    |proj g|=  1.22618D+05\n",
      "\n",
      "At iterate   36    f=  3.55336D+03    |proj g|=  8.07065D+04\n",
      "\n",
      "At iterate   37    f=  3.55336D+03    |proj g|=  2.17412D+04\n",
      "\n",
      "At iterate   38    f=  3.55336D+03    |proj g|=  2.06002D+04\n",
      "\n",
      "At iterate   39    f=  3.55336D+03    |proj g|=  2.37179D+04\n",
      "\n",
      "At iterate   40    f=  3.55336D+03    |proj g|=  8.70714D+04\n",
      "\n",
      "At iterate   41    f=  3.55336D+03    |proj g|=  8.61711D+04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x0 \u001b[38;5;241m=\u001b[39m generate_initial_guess(p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mra[\u001b[38;5;241m0\u001b[39m], p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mdec[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# jac=lambda x: -jax.grad(p0.loglike)(x),\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max line search steps (default is 20)\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxcor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase memory storage (default is 10)\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Make function value convergence more lenient\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gradient convergence criterion\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max function evaluations\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max iterations\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Step size for finite difference (if needed)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mline_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrong_wolfe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use strong Wolfe conditions\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:344\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:306\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:47\u001b[0m, in \u001b[0;36m_wrapper_grad.<locals>.wrapped1\u001b[0;34m(x, f0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped1\u001b[39m(x, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:519\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:592\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    590\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[1;32m    591\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:470\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    468\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 470\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:24\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user-provided objective function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return a scalar value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "result = minimize(\n",
    "    fun=lambda x: -p0.loglike(x),\n",
    "    x0=x0,\n",
    "    # jac=lambda x: -jax.grad(p0.loglike)(x),\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jorbit",
   "language": "python",
   "name": "jorbit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
