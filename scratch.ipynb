{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import astropy.units as u\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from astroquery.jplhorizons import Horizons\n",
    "\n",
    "from jorbit import Observations, Particle\n",
    "from jorbit.utils.states import CartesianState, KeplerianState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nights = [Time(\"2025-01-01 07:00\"), Time(\"2025-01-02 07:00\"), Time(\"2025-01-05 07:00\")]\n",
    "\n",
    "times = []\n",
    "for n in nights:\n",
    "    times.extend([n + i * 1 * u.hour for i in range(3)])\n",
    "times = Time(times)\n",
    "\n",
    "\n",
    "obj = Horizons(id=\"274301\", location=\"695@399\", epochs=times.utc.jd)\n",
    "pts = obj.ephemerides(extra_precision=True, quantities=\"1\")\n",
    "\n",
    "coords = SkyCoord(pts[\"RA\"], pts[\"DEC\"], unit=(u.deg, u.deg))\n",
    "times = Time(pts[\"datetime_jd\"], format=\"jd\", scale=\"utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'RA'), Text(0, 0.5, 'DEC')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 711.1x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(coords.ra, coords.dec, c=\"k\", marker=\"x\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])\n",
    "ax.set(xlabel=\"RA\", ylabel=\"DEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observations with 9 set(s) of observations"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = Observations(\n",
    "    observed_coordinates=coords,\n",
    "    times=times,\n",
    "    observatories=\"kitt peak\",\n",
    "    astrometric_uncertainties=1 * u.arcsec,\n",
    ")\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-2.00572335,  1.77860137,  0.5197407 ], dtype=float64),\n",
       " Array([-0.00665991, -0.00662871, -0.00203885], dtype=float64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = Horizons(id=\"274301\", location=\"500@0\", epochs=times.tdb.jd[0])\n",
    "vecs = obj.vectors(refplane=\"earth\")\n",
    "true_x0 = jnp.array([vecs[\"x\"], vecs[\"y\"], vecs[\"z\"]]).T[0]\n",
    "true_v0 = jnp.array([vecs[\"vx\"], vecs[\"vy\"], vecs[\"vz\"]]).T[0]\n",
    "true_x0, true_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Particle: 274301 Wikipedia"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0 = Particle(\n",
    "    x=true_x0, v=true_v0, time=times[0], name=\"274301 Wikipedia\", observations=obs\n",
    ")\n",
    "p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartesianState(x=Array([[-2.00572335,  1.77860137,  0.5197407 ]], dtype=float64), v=Array([[-0.00665991, -0.00662871, -0.00203885]], dtype=float64), time=np.float64(2460676.792467407))\n",
      "KeplerianState(semi=Array([2.37859645], dtype=float64), ecc=Array([0.14924503], dtype=float64), inc=Array([6.73363769], dtype=float64), Omega=Array([183.37295038], dtype=float64), omega=Array([140.26385356], dtype=float64), nu=Array([173.65462829], dtype=float64), time=np.float64(2460676.792467407))\n"
     ]
    }
   ],
   "source": [
    "print(p0.cartesian_state)\n",
    "print(p0.keplerian_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-22.57306906, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "perturbed = copy.deepcopy(p0.cartesian_state)\n",
    "perturbed.x += 1000 * u.km.to(u.au)\n",
    "ll = p0.loglike(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.90962391,  0.69842047],\n",
       "       [-0.90984353,  0.69858176],\n",
       "       [-0.91006152,  0.69873977],\n",
       "       [-0.91479967,  0.70217721],\n",
       "       [-0.9150206 ,  0.70233721],\n",
       "       [-0.91524007,  0.70249453],\n",
       "       [-0.93037155,  0.71334351],\n",
       "       [-0.93059056,  0.71350114],\n",
       "       [-0.93081201,  0.71365595]], dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares, minimize\n",
    "\n",
    "x0 = jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    "x1 = jnp.concatenate([-perturbed.x.flatten(), perturbed.v.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-0.90962391,  0.69842047],\n",
       "        [-0.90984353,  0.69858176],\n",
       "        [-0.91006152,  0.69873977],\n",
       "        [-0.91479967,  0.70217721],\n",
       "        [-0.9150206 ,  0.70233721],\n",
       "        [-0.91524007,  0.70249453],\n",
       "        [-0.93037155,  0.71334351],\n",
       "        [-0.93059056,  0.71350114],\n",
       "        [-0.93081201,  0.71365595]], dtype=float64),\n",
       " Array([[-91092.82616836,  44300.54791232],\n",
       "        [-91067.48832186,  44293.03170153],\n",
       "        [-91041.82333691,  44285.50912264],\n",
       "        [-90499.66732878,  44118.36499448],\n",
       "        [-90472.91765956,  44110.15553629],\n",
       "        [-90445.84779392,  44101.94104199],\n",
       "        [-88523.61709689,  43472.6618289 ],\n",
       "        [-88492.77544129,  43462.40902278],\n",
       "        [-88461.63551964,  43452.15541053]], dtype=float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(x0), p0.residuals(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3188.938457\n",
      "         Iterations: 439\n",
      "         Function evaluations: 713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-3188.9384567, dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "result = minimize(\n",
    "    fun=f,\n",
    "    x0=x1,\n",
    "    method=\"Nelder-Mead\",\n",
    "    options={\"disp\": True, \"maxiter\": 1000},\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 558, initial cost 4.5159e+10, final cost 2.8194e+03, first-order optimality 7.82e+03.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-2835.97158603, dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tmp(x):\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=x1,\n",
    "    verbose=2,\n",
    "    method=\"lm\",\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         4.5159e+10                                    8.19e+10    \n",
      "       1              3         8.4716e+09      3.67e+10       6.83e-01       1.40e+10    \n",
      "       2              4         8.2194e+09      2.52e+08       1.37e+00       1.90e+10    \n",
      "       3              5         2.6552e+09      5.56e+09       3.41e-01       8.10e+09    \n",
      "       4              6         4.2061e+07      2.61e+09       6.83e-01       1.18e+09    \n",
      "       5              8         1.8053e+07      2.40e+07       5.42e-02       1.66e+09    \n",
      "       6              9         7.1703e+05      1.73e+07       5.42e-02       2.57e+08    \n",
      "       7             12         8.3191e+04      6.34e+05       6.77e-03       1.16e+08    \n",
      "       8             15         2.0467e+04      6.27e+04       8.46e-04       3.10e+07    \n",
      "       9             16         1.3565e+04      6.90e+03       1.69e-03       3.76e+07    \n",
      "      10             18         7.0870e+03      6.48e+03       4.23e-04       2.12e+07    \n",
      "      11             20         5.4135e+03      1.67e+03       1.06e-04       9.87e+06    \n",
      "      12             22         5.0311e+03      3.82e+02       5.29e-05       4.09e+06    \n",
      "      13             24         4.9438e+03      8.73e+01       2.64e-05       1.67e+06    \n",
      "      14             26         4.9224e+03      2.14e+01       1.32e-05       9.50e+05    \n",
      "      15             27         4.9080e+03      1.44e+01       2.64e-05       3.82e+05    \n",
      "      16             30         4.9068e+03      1.22e+00       3.31e-06       2.30e+05    \n",
      "      17             31         4.9052e+03      1.62e+00       6.61e-06       1.65e+05    \n",
      "      18             33         4.9045e+03      6.46e-01       3.31e-06       1.69e+05    \n",
      "      19             34         4.9038e+03      6.85e-01       6.61e-06       2.39e+05    \n",
      "      20             36         4.9034e+03      3.94e-01       1.65e-06       9.39e+04    \n",
      "      21             37         4.9032e+03      2.34e-01       3.31e-06       1.51e+05    \n",
      "      22             38         4.9030e+03      1.61e-01       3.31e-06       1.41e+05    \n",
      "      23             39         4.9029e+03      1.17e-01       3.31e-06       1.64e+05    \n",
      "      24             40         4.9028e+03      1.27e-01       8.26e-07       8.53e+04    \n",
      "      25             41         4.9027e+03      9.16e-02       1.65e-06       4.99e+04    \n",
      "      26             42         4.9026e+03      6.12e-02       1.65e-06       7.36e+04    \n",
      "      27             43         4.9026e+03      4.55e-02       1.65e-06       6.96e+04    \n",
      "      28             44         4.9026e+03      3.13e-02       1.65e-06       8.05e+04    \n",
      "      29             45         4.9025e+03      3.24e-02       4.13e-07       4.20e+04    \n",
      "      30             46         4.9025e+03      2.65e-02       8.26e-07       2.32e+04    \n",
      "      31             47         4.9025e+03      1.94e-02       8.26e-07       3.24e+04    \n",
      "      32             48         4.9025e+03      1.61e-02       8.26e-07       3.22e+04    \n",
      "      33             49         4.9025e+03      1.16e-02       8.26e-07       3.72e+04    \n",
      "      34             50         4.9024e+03      1.11e-02       8.26e-07       3.61e+04    \n",
      "      35             51         4.9024e+03      8.18e-03       8.26e-07       3.86e+04    \n",
      "      36             52         4.9024e+03      8.19e-03       2.07e-07       1.96e+04    \n",
      "      37             53         4.9024e+03      7.91e-03       4.13e-07       1.92e+04    \n",
      "      38             54         4.9024e+03      7.27e-03       4.13e-07       1.44e+04    \n",
      "      39             55         4.9024e+03      6.67e-03       4.13e-07       1.92e+04    \n",
      "      40             56         4.9024e+03      6.31e-03       4.13e-07       1.39e+04    \n",
      "      41             57         4.9024e+03      5.88e-03       4.13e-07       1.92e+04    \n",
      "      42             58         4.9024e+03      5.94e-03       4.13e-07       1.59e+04    \n",
      "      43             59         4.9024e+03      6.11e-03       4.13e-07       1.82e+04    \n",
      "      44             60         4.9024e+03      5.60e-03       4.13e-07       1.48e+04    \n",
      "      45             61         4.9024e+03      5.71e-03       4.13e-07       1.81e+04    \n",
      "      46             62         4.9024e+03      5.55e-03       4.13e-07       1.31e+04    \n",
      "      47             63         4.9024e+03      5.21e-03       4.13e-07       1.77e+04    \n",
      "      48             64         4.9023e+03      5.29e-03       4.13e-07       1.41e+04    \n",
      "      49             65         4.9023e+03      5.44e-03       4.13e-07       1.85e+04    \n",
      "      50             66         4.9023e+03      5.41e-03       4.13e-07       1.47e+04    \n",
      "      51             67         4.9023e+03      5.62e-03       4.13e-07       1.82e+04    \n",
      "      52             68         4.9023e+03      5.48e-03       4.13e-07       1.52e+04    \n",
      "      53             69         4.9023e+03      5.72e-03       4.13e-07       1.76e+04    \n",
      "      54             70         4.9023e+03      5.33e-03       4.13e-07       1.51e+04    \n",
      "      55             71         4.9023e+03      5.68e-03       4.13e-07       1.67e+04    \n",
      "      56             72         4.9023e+03      5.11e-03       4.13e-07       1.36e+04    \n",
      "      57             73         4.9023e+03      5.33e-03       4.13e-07       1.79e+04    \n",
      "      58             74         4.9023e+03      5.37e-03       4.13e-07       1.43e+04    \n",
      "      59             75         4.9023e+03      5.55e-03       4.13e-07       1.81e+04    \n",
      "      60             76         4.9023e+03      5.50e-03       4.13e-07       1.37e+04    \n",
      "      61             77         4.9023e+03      5.33e-03       4.13e-07       1.66e+04    \n",
      "      62             78         4.9023e+03      4.97e-03       4.13e-07       1.48e+04    \n",
      "      63             79         4.9023e+03      5.71e-03       4.13e-07       1.79e+04    \n",
      "      64             80         4.9023e+03      5.31e-03       4.13e-07       1.48e+04    \n",
      "      65             81         4.9023e+03      5.68e-03       4.13e-07       1.74e+04    \n",
      "      66             82         4.9023e+03      5.24e-03       4.13e-07       1.32e+04    \n",
      "      67             83         4.9022e+03      5.20e-03       4.13e-07       1.81e+04    \n",
      "      68             84         4.9022e+03      5.41e-03       4.13e-07       1.38e+04    \n",
      "      69             85         4.9022e+03      5.36e-03       4.13e-07       1.78e+04    \n",
      "      70             86         4.9022e+03      5.27e-03       4.13e-07       1.36e+04    \n",
      "      71             87         4.9022e+03      5.35e-03       4.13e-07       1.79e+04    \n",
      "      72             88         4.9022e+03      5.30e-03       4.13e-07       1.48e+04    \n",
      "      73             89         4.9022e+03      5.67e-03       4.13e-07       1.61e+04    \n",
      "      74             90         4.9022e+03      4.67e-03       4.13e-07       1.46e+04    \n",
      "      75             91         4.9022e+03      5.68e-03       4.13e-07       1.67e+04    \n",
      "      76             92         4.9022e+03      5.18e-03       4.13e-07       1.48e+04    \n",
      "      77             93         4.9022e+03      5.59e-03       4.13e-07       1.77e+04    \n",
      "      78             94         4.9022e+03      5.30e-03       4.13e-07       1.29e+04    \n",
      "      79             95         4.9022e+03      5.14e-03       4.13e-07       1.70e+04    \n",
      "      80             96         4.9022e+03      4.88e-03       4.13e-07       1.42e+04    \n",
      "      81             97         4.9022e+03      5.56e-03       4.13e-07       1.79e+04    \n",
      "      82             98         4.9022e+03      5.37e-03       4.13e-07       1.49e+04    \n",
      "      83             99         4.9022e+03      5.81e-03       4.13e-07       1.57e+04    \n",
      "      84             100        4.9022e+03      4.66e-03       4.13e-07       1.47e+04    \n",
      "      85             101        4.9022e+03      5.68e-03       4.13e-07       1.70e+04    \n",
      "      86             102        4.9021e+03      5.00e-03       4.13e-07       1.36e+04    \n",
      "      87             103        4.9021e+03      5.29e-03       4.13e-07       1.70e+04    \n",
      "      88             104        4.9021e+03      4.97e-03       4.13e-07       1.41e+04    \n",
      "      89             105        4.9021e+03      5.54e-03       4.13e-07       1.77e+04    \n",
      "      90             106        4.9021e+03      5.32e-03       4.13e-07       1.49e+04    \n",
      "      91             107        4.9021e+03      5.67e-03       4.13e-07       1.67e+04    \n",
      "      92             108        4.9021e+03      5.07e-03       4.13e-07       1.37e+04    \n",
      "      93             109        4.9021e+03      5.22e-03       4.13e-07       1.83e+04    \n",
      "      94             110        4.9021e+03      5.35e-03       4.13e-07       1.38e+04    \n",
      "      95             111        4.9021e+03      5.37e-03       4.13e-07       1.81e+04    \n",
      "      96             112        4.9021e+03      5.25e-03       4.13e-07       1.47e+04    \n",
      "      97             113        4.9021e+03      5.66e-03       4.13e-07       1.75e+04    \n",
      "      98             114        4.9021e+03      5.29e-03       4.13e-07       1.49e+04    \n",
      "      99             115        4.9021e+03      5.68e-03       4.13e-07       1.65e+04    \n",
      "      100            116        4.9021e+03      5.01e-03       4.13e-07       1.47e+04    \n",
      "      101            117        4.9021e+03      5.71e-03       4.13e-07       1.82e+04    \n",
      "      102            118        4.9021e+03      5.48e-03       4.13e-07       1.47e+04    \n",
      "      103            119        4.9021e+03      5.60e-03       4.13e-07       1.69e+04    \n",
      "      104            120        4.9020e+03      5.16e-03       4.13e-07       1.50e+04    \n",
      "      105            121        4.9020e+03      5.66e-03       4.13e-07       1.66e+04    \n",
      "      106            122        4.9020e+03      4.76e-03       4.13e-07       1.44e+04    \n",
      "      107            123        4.9020e+03      5.33e-03       4.13e-07       1.67e+04    \n",
      "      108            124        4.9020e+03      4.79e-03       4.13e-07       1.43e+04    \n",
      "      109            125        4.9020e+03      5.49e-03       4.13e-07       1.79e+04    \n",
      "      110            126        4.9020e+03      5.34e-03       4.13e-07       1.46e+04    \n",
      "      111            127        4.9020e+03      5.57e-03       4.13e-07       1.80e+04    \n",
      "      112            128        4.9020e+03      5.42e-03       4.13e-07       1.47e+04    \n",
      "      113            129        4.9020e+03      5.55e-03       4.13e-07       1.80e+04    \n",
      "      114            130        4.9020e+03      5.31e-03       4.13e-07       1.34e+04    \n",
      "      115            131        4.9020e+03      5.27e-03       4.13e-07       1.78e+04    \n",
      "      116            132        4.9020e+03      5.23e-03       4.13e-07       1.38e+04    \n",
      "      117            133        4.9020e+03      5.38e-03       4.13e-07       1.83e+04    \n",
      "      118            134        4.9020e+03      5.50e-03       4.13e-07       1.51e+04    \n",
      "      119            135        4.9020e+03      5.70e-03       4.13e-07       1.66e+04    \n",
      "      120            136        4.9020e+03      5.17e-03       4.13e-07       1.37e+04    \n",
      "      121            137        4.9020e+03      5.32e-03       4.13e-07       1.83e+04    \n",
      "      122            138        4.9020e+03      5.48e-03       4.13e-07       1.49e+04    \n",
      "      123            139        4.9019e+03      5.68e-03       4.13e-07       1.81e+04    \n",
      "      124            140        4.9019e+03      5.50e-03       4.13e-07       1.46e+04    \n",
      "      125            141        4.9019e+03      5.49e-03       4.13e-07       1.81e+04    \n",
      "      126            142        4.9019e+03      5.40e-03       4.13e-07       1.46e+04    \n",
      "      127            143        4.9019e+03      5.50e-03       4.13e-07       1.70e+04    \n",
      "      128            144        4.9019e+03      5.25e-03       4.13e-07       1.42e+04    \n",
      "      129            145        4.9019e+03      5.08e-03       4.13e-07       1.70e+04    \n",
      "      130            146        4.9019e+03      4.99e-03       4.13e-07       1.41e+04    \n",
      "      131            147        4.9019e+03      5.42e-03       4.13e-07       1.79e+04    \n",
      "      132            148        4.9019e+03      5.24e-03       4.13e-07       1.35e+04    \n",
      "      133            149        4.9019e+03      5.33e-03       4.13e-07       1.78e+04    \n",
      "      134            150        4.9019e+03      5.26e-03       4.13e-07       1.34e+04    \n",
      "      135            151        4.9019e+03      5.26e-03       4.13e-07       1.82e+04    \n",
      "      136            152        4.9019e+03      5.29e-03       4.13e-07       1.49e+04    \n",
      "      137            153        4.9019e+03      5.68e-03       4.13e-07       1.65e+04    \n",
      "      138            154        4.9019e+03      4.77e-03       4.13e-07       1.45e+04    \n",
      "      139            155        4.9019e+03      5.64e-03       4.13e-07       1.68e+04    \n",
      "      140            156        4.9019e+03      5.29e-03       4.13e-07       1.48e+04    \n",
      "      141            157        4.9019e+03      5.62e-03       4.13e-07       1.77e+04    \n",
      "      142            158        4.9018e+03      5.38e-03       4.13e-07       1.39e+04    \n",
      "      143            159        4.9018e+03      5.10e-03       4.13e-07       1.70e+04    \n",
      "      144            160        4.9018e+03      4.85e-03       4.13e-07       1.43e+04    \n",
      "      145            161        4.9018e+03      5.63e-03       4.13e-07       1.77e+04    \n",
      "      146            162        4.9018e+03      5.31e-03       4.13e-07       1.36e+04    \n",
      "      147            163        4.9018e+03      5.31e-03       4.13e-07       1.82e+04    \n",
      "      148            164        4.9018e+03      5.26e-03       4.13e-07       1.48e+04    \n",
      "      149            165        4.9018e+03      5.65e-03       4.13e-07       1.62e+04    \n",
      "      150            166        4.9018e+03      4.68e-03       4.13e-07       1.45e+04    \n",
      "      151            167        4.9018e+03      5.58e-03       4.13e-07       1.79e+04    \n",
      "      152            168        4.9018e+03      5.39e-03       4.13e-07       1.39e+04    \n",
      "      153            169        4.9018e+03      5.47e-03       4.13e-07       1.70e+04    \n",
      "      154            170        4.9018e+03      5.14e-03       4.13e-07       1.49e+04    \n",
      "      155            171        4.9018e+03      5.62e-03       4.13e-07       1.74e+04    \n",
      "      156            172        4.9018e+03      5.20e-03       4.13e-07       1.41e+04    \n",
      "      157            173        4.9018e+03      5.45e-03       4.13e-07       1.69e+04    \n",
      "      158            174        4.9018e+03      5.16e-03       4.13e-07       1.36e+04    \n",
      "      159            175        4.9018e+03      5.33e-03       4.13e-07       1.78e+04    \n",
      "      160            176        4.9018e+03      5.26e-03       4.13e-07       1.44e+04    \n",
      "      161            177        4.9017e+03      5.58e-03       4.13e-07       1.55e+04    \n",
      "      162            178        4.9017e+03      4.44e-03       4.13e-07       1.50e+04    \n",
      "      163            179        4.9017e+03      5.43e-03       4.13e-07       1.81e+04    \n",
      "      164            180        4.9017e+03      5.38e-03       4.13e-07       1.46e+04    \n",
      "      165            181        4.9017e+03      5.60e-03       4.13e-07       1.68e+04    \n",
      "      166            182        4.9017e+03      5.09e-03       4.13e-07       1.47e+04    \n",
      "      167            183        4.9017e+03      5.60e-03       4.13e-07       1.74e+04    \n",
      "      168            184        4.9017e+03      5.29e-03       4.13e-07       1.36e+04    \n",
      "      169            185        4.9017e+03      5.48e-03       4.13e-07       1.72e+04    \n",
      "      170            186        4.9017e+03      4.97e-03       4.13e-07       1.45e+04    \n",
      "      171            187        4.9017e+03      5.60e-03       4.13e-07       1.75e+04    \n",
      "      172            188        4.9017e+03      5.37e-03       4.13e-07       1.49e+04    \n",
      "      173            189        4.9017e+03      5.74e-03       4.13e-07       1.57e+04    \n",
      "      174            190        4.9017e+03      4.26e-03       4.13e-07       1.56e+04    \n",
      "      175            191        4.9017e+03      5.24e-03       4.13e-07       1.73e+04    \n",
      "      176            192        4.9017e+03      5.17e-03       4.13e-07       1.36e+04    \n",
      "      177            193        4.9017e+03      5.37e-03       4.13e-07       1.80e+04    \n",
      "      178            194        4.9017e+03      5.34e-03       4.13e-07       1.27e+04    \n",
      "      179            195        4.9017e+03      5.05e-03       4.13e-07       1.78e+04    \n",
      "      180            196        4.9016e+03      5.34e-03       4.13e-07       1.57e+04    \n",
      "      181            197        4.9016e+03      5.83e-03       4.13e-07       1.79e+04    \n",
      "      182            198        4.9016e+03      5.44e-03       4.13e-07       1.39e+04    \n",
      "      183            199        4.9016e+03      5.48e-03       4.13e-07       1.79e+04    \n",
      "      184            200        4.9016e+03      5.28e-03       4.13e-07       1.39e+04    \n",
      "      185            201        4.9016e+03      5.40e-03       4.13e-07       1.84e+04    \n",
      "      186            202        4.9016e+03      5.39e-03       4.13e-07       1.48e+04    \n",
      "      187            203        4.9016e+03      5.68e-03       4.13e-07       1.81e+04    \n",
      "      188            204        4.9016e+03      5.46e-03       4.13e-07       1.47e+04    \n",
      "      189            205        4.9016e+03      5.61e-03       4.13e-07       1.82e+04    \n",
      "      190            206        4.9016e+03      5.46e-03       4.13e-07       1.48e+04    \n",
      "      191            207        4.9016e+03      5.62e-03       4.13e-07       1.65e+04    \n",
      "      192            208        4.9016e+03      5.13e-03       4.13e-07       1.47e+04    \n",
      "      193            209        4.9016e+03      5.57e-03       4.13e-07       1.81e+04    \n",
      "      194            210        4.9016e+03      5.49e-03       4.13e-07       1.33e+04    \n",
      "      195            211        4.9016e+03      5.19e-03       4.13e-07       1.67e+04    \n",
      "      196            212        4.9016e+03      4.79e-03       4.13e-07       1.43e+04    \n",
      "      197            213        4.9016e+03      5.29e-03       4.13e-07       1.82e+04    \n",
      "      198            214        4.9015e+03      5.28e-03       4.13e-07       1.38e+04    \n",
      "      199            215        4.9015e+03      5.39e-03       4.13e-07       1.81e+04    \n",
      "      200            216        4.9015e+03      5.30e-03       4.13e-07       1.50e+04    \n",
      "      201            217        4.9015e+03      5.68e-03       4.13e-07       1.78e+04    \n",
      "      202            218        4.9015e+03      5.30e-03       4.13e-07       1.48e+04    \n",
      "      203            219        4.9015e+03      5.72e-03       4.13e-07       1.66e+04    \n",
      "      204            220        4.9015e+03      4.76e-03       4.13e-07       1.43e+04    \n",
      "      205            221        4.9015e+03      5.36e-03       4.13e-07       1.78e+04    \n",
      "      206            222        4.9015e+03      5.28e-03       4.13e-07       1.27e+04    \n",
      "      207            223        4.9015e+03      5.12e-03       4.13e-07       1.73e+04    \n",
      "      208            224        4.9015e+03      5.16e-03       4.13e-07       1.47e+04    \n",
      "      209            225        4.9015e+03      5.63e-03       4.13e-07       1.68e+04    \n",
      "      210            226        4.9015e+03      4.91e-03       4.13e-07       1.52e+04    \n",
      "      211            227        4.9015e+03      5.78e-03       4.13e-07       1.79e+04    \n",
      "      212            228        4.9015e+03      5.33e-03       4.13e-07       1.50e+04    \n",
      "      213            229        4.9015e+03      5.63e-03       4.13e-07       1.66e+04    \n",
      "      214            230        4.9015e+03      4.89e-03       4.13e-07       1.40e+04    \n",
      "      215            231        4.9015e+03      5.25e-03       4.13e-07       1.66e+04    \n",
      "      216            232        4.9015e+03      5.08e-03       4.13e-07       1.47e+04    \n",
      "      217            233        4.9014e+03      5.60e-03       4.13e-07       1.67e+04    \n",
      "      218            234        4.9014e+03      4.77e-03       4.13e-07       1.45e+04    \n",
      "      219            235        4.9014e+03      5.42e-03       4.13e-07       1.62e+04    \n",
      "      220            236        4.9014e+03      4.76e-03       4.13e-07       1.46e+04    \n",
      "      221            237        4.9014e+03      5.21e-03       4.13e-07       1.62e+04    \n",
      "      222            238        4.9014e+03      4.32e-03       4.13e-07       1.52e+04    \n",
      "      223            239        4.9014e+03      5.41e-03       4.13e-07       1.69e+04    \n",
      "      224            240        4.9014e+03      4.90e-03       4.13e-07       1.44e+04    \n",
      "      225            241        4.9014e+03      5.62e-03       4.13e-07       1.77e+04    \n",
      "      226            242        4.9014e+03      5.25e-03       4.13e-07       1.41e+04    \n",
      "      227            243        4.9014e+03      5.45e-03       4.13e-07       1.80e+04    \n",
      "      228            244        4.9014e+03      5.37e-03       4.13e-07       1.48e+04    \n",
      "      229            245        4.9014e+03      5.60e-03       4.13e-07       1.58e+04    \n",
      "      230            246        4.9014e+03      4.63e-03       4.13e-07       1.48e+04    \n",
      "      231            247        4.9014e+03      5.35e-03       4.13e-07       1.79e+04    \n",
      "      232            248        4.9014e+03      5.39e-03       4.13e-07       1.28e+04    \n",
      "      233            249        4.9014e+03      5.08e-03       4.13e-07       1.71e+04    \n",
      "      234            250        4.9014e+03      4.87e-03       4.13e-07       1.42e+04    \n",
      "      235            251        4.9014e+03      5.42e-03       4.13e-07       1.82e+04    \n",
      "      236            252        4.9013e+03      5.31e-03       4.13e-07       1.48e+04    \n",
      "      237            253        4.9013e+03      5.64e-03       4.13e-07       1.80e+04    \n",
      "      238            254        4.9013e+03      5.43e-03       4.13e-07       1.47e+04    \n",
      "      239            255        4.9013e+03      5.59e-03       4.13e-07       1.70e+04    \n",
      "      240            256        4.9013e+03      4.99e-03       4.13e-07       1.44e+04    \n",
      "      241            257        4.9013e+03      5.54e-03       4.13e-07       1.67e+04    \n",
      "      242            258        4.9013e+03      5.09e-03       4.13e-07       1.43e+04    \n",
      "      243            259        4.9013e+03      5.64e-03       4.13e-07       1.77e+04    \n",
      "      244            260        4.9013e+03      5.33e-03       4.13e-07       1.49e+04    \n",
      "      245            261        4.9013e+03      5.63e-03       4.13e-07       1.74e+04    \n",
      "      246            262        4.9013e+03      5.23e-03       4.13e-07       1.32e+04    \n",
      "      247            263        4.9013e+03      5.22e-03       4.13e-07       1.82e+04    \n",
      "      248            264        4.9013e+03      5.34e-03       4.13e-07       1.50e+04    \n",
      "      249            265        4.9013e+03      5.78e-03       4.13e-07       1.79e+04    \n",
      "      250            266        4.9013e+03      5.43e-03       4.13e-07       1.57e+04    \n",
      "      251            267        4.9013e+03      5.89e-03       4.13e-07       1.82e+04    \n",
      "      252            268        4.9013e+03      5.48e-03       4.13e-07       1.39e+04    \n",
      "      253            269        4.9013e+03      5.36e-03       4.13e-07       1.59e+04    \n",
      "      254            270        4.9013e+03      4.90e-03       4.13e-07       1.49e+04    \n",
      "      255            271        4.9012e+03      5.66e-03       4.13e-07       1.79e+04    \n",
      "      256            272        4.9012e+03      5.31e-03       4.13e-07       1.46e+04    \n",
      "      257            273        4.9012e+03      5.61e-03       4.13e-07       1.60e+04    \n",
      "      258            274        4.9012e+03      4.74e-03       4.13e-07       1.45e+04    \n",
      "      259            275        4.9012e+03      5.30e-03       4.13e-07       1.79e+04    \n",
      "      260            276        4.9012e+03      5.27e-03       4.13e-07       1.40e+04    \n",
      "      261            277        4.9012e+03      5.02e-03       4.13e-07       1.72e+04    \n",
      "      262            278        4.9012e+03      4.96e-03       4.13e-07       1.44e+04    \n",
      "      263            279        4.9012e+03      5.66e-03       4.13e-07       1.80e+04    \n",
      "      264            280        4.9012e+03      5.46e-03       4.13e-07       1.50e+04    \n",
      "      265            281        4.9012e+03      5.83e-03       4.13e-07       1.69e+04    \n",
      "      266            282        4.9012e+03      5.25e-03       4.13e-07       1.44e+04    \n",
      "      267            283        4.9012e+03      5.59e-03       4.13e-07       1.70e+04    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mleast_squares\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# jac=gradient,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m p0\u001b[38;5;241m.\u001b[39mloglike(result\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:941\u001b[0m, in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m     result \u001b[38;5;241m=\u001b[39m call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,\n\u001b[1;32m    938\u001b[0m                           max_nfev, x_scale, diff_step)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 941\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdogbox\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tr_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsmr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularize\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tr_options:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/trf.py:119\u001b[0m, in \u001b[0;36mtrf\u001b[0;34m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrf\u001b[39m(fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[1;32m    113\u001b[0m         loss_function, tr_solver, tr_options, verbose):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# For efficiency, it makes sense to run the simplified version of the\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# algorithm when no bounds are imposed. We decided to write the two\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# separate functions. It violates the DRY principle, but the individual\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# functions are kept the most readable.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(lb \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(ub \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf):\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrf_no_bounds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nfev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_solver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trf_bounds(\n\u001b[1;32m    124\u001b[0m             fun, jac, x0, f0, J0, lb, ub, ftol, xtol, gtol, max_nfev, x_scale,\n\u001b[1;32m    125\u001b[0m             loss_function, tr_solver, tr_options, verbose)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/trf.py:536\u001b[0m, in \u001b[0;36mtrf_no_bounds\u001b[0;34m(fun, jac, x0, f0, J0, ftol, xtol, gtol, max_nfev, x_scale, loss_function, tr_solver, tr_options, verbose)\u001b[0m\n\u001b[1;32m    532\u001b[0m f_true \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    534\u001b[0m cost \u001b[38;5;241m=\u001b[39m cost_new\n\u001b[0;32m--> 536\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mjac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m njev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lsq/least_squares.py:898\u001b[0m, in \u001b[0;36mleast_squares.<locals>.jac_wrapped\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjac_wrapped\u001b[39m(x, f):\n\u001b[0;32m--> 898\u001b[0m     J \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac_sparsity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m J\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# J is guaranteed not sparse.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m         J \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(J)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:519\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:592\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    590\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[1;32m    591\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:470\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    468\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 470\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/numpy/_core/shape_base.py:64\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mConvert inputs to arrays with at least one dimension.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    # jac=gradient,\n",
    "    x0=x1,\n",
    "    verbose=2,\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ -415941.92979457,  -739901.15981344,  -648959.72258677,\n",
       "        -721214.59992895, -1277773.22303356, -1121110.99923105],      dtype=float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacfwd(p0.loglike)(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/runpy.py:198\u001b[0m, in \u001b[0;36m_run_module_as_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m mod_spec\u001b[38;5;241m.\u001b[39morigin\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _run_code(code, main_globals, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod_spec)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/runpy.py:88\u001b[0m, in \u001b[0;36m_run_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m run_globals\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m mod_name,\n\u001b[1;32m     82\u001b[0m                    \u001b[38;5;18m__file__\u001b[39m \u001b[38;5;241m=\u001b[39m fname,\n\u001b[1;32m     83\u001b[0m                    __cached__ \u001b[38;5;241m=\u001b[39m cached,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m                    __package__ \u001b[38;5;241m=\u001b[39m pkg_name,\n\u001b[1;32m     87\u001b[0m                    __spec__ \u001b[38;5;241m=\u001b[39m mod_spec)\n\u001b[0;32m---> 88\u001b[0m exec(code, run_globals)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_globals\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel_launcher.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipykernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[0;32m---> 18\u001b[0m app\u001b[38;5;241m.\u001b[39mlaunch_new_instance()\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/traitlets/config/application.py:1075\u001b[0m, in \u001b[0;36mlaunch_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1074\u001b[0m app\u001b[38;5;241m.\u001b[39minitialize(argv)\n\u001b[0;32m-> 1075\u001b[0m app\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelapp.py:739\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_loop\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/tornado/platform/asyncio.py:205\u001b[0m, in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masyncio_loop\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:679\u001b[0m, in \u001b[0;36mrun_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py:2027\u001b[0m, in \u001b[0;36m_run_once\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2027\u001b[0m         handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m   2028\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/events.py:89\u001b[0m, in \u001b[0;36m_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:545\u001b[0m, in \u001b[0;36mdispatch_queue\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_one()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:534\u001b[0m, in \u001b[0;36mprocess_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m dispatch(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:437\u001b[0m, in \u001b[0;36mdispatch_shell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m--> 437\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/ipkernel.py:362\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_associate_new_top_level_threads_with(parent_header)\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mexecute_request(stream, ident, parent)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/kernelbase.py:778\u001b[0m, in \u001b[0;36mexecute_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(reply_content):\n\u001b[0;32m--> 778\u001b[0m     reply_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# Flush output before sending the reply.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/ipkernel.py:449\u001b[0m, in \u001b[0;36mdo_execute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 449\u001b[0m     res \u001b[38;5;241m=\u001b[39m shell\u001b[38;5;241m.\u001b[39mrun_cell(\n\u001b[1;32m    450\u001b[0m         code,\n\u001b[1;32m    451\u001b[0m         store_history\u001b[38;5;241m=\u001b[39mstore_history,\n\u001b[1;32m    452\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[1;32m    453\u001b[0m         cell_id\u001b[38;5;241m=\u001b[39mcell_id,\n\u001b[1;32m    454\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/ipykernel/zmqshell.py:549\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun_cell(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3075\u001b[0m, in \u001b[0;36mrun_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3075\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_cell(\n\u001b[1;32m   3076\u001b[0m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[1;32m   3077\u001b[0m     )\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3130\u001b[0m, in \u001b[0;36m_run_cell\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3130\u001b[0m     result \u001b[38;5;241m=\u001b[39m runner(coro)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3334\u001b[0m, in \u001b[0;36mrun_cell_async\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3331\u001b[0m interactivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mast_node_interactivity\n\u001b[0;32m-> 3334\u001b[0m has_raised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_ast_nodes(code_ast\u001b[38;5;241m.\u001b[39mbody, cell_name,\n\u001b[1;32m   3335\u001b[0m        interactivity\u001b[38;5;241m=\u001b[39minteractivity, compiler\u001b[38;5;241m=\u001b[39mcompiler, result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_execution_succeeded \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3517\u001b[0m, in \u001b[0;36mrun_ast_nodes\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3516\u001b[0m     asy \u001b[38;5;241m=\u001b[39m compare(code)\n\u001b[0;32m-> 3517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_code(code, result, async_\u001b[38;5;241m=\u001b[39masy):\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m, in \u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3577\u001b[0m         exec(code_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_global_ns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   3579\u001b[0m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      4\u001b[0m perturbed\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39mkm\u001b[38;5;241m.\u001b[39mto(u\u001b[38;5;241m.\u001b[39mau)\n\u001b[0;32m----> 5\u001b[0m ll \u001b[38;5;241m=\u001b[39m p0\u001b[38;5;241m.\u001b[39mloglike(jnp\u001b[38;5;241m.\u001b[39mconcatenate([perturbed\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mflatten(), perturbed\u001b[38;5;241m.\u001b[39mv\u001b[38;5;241m.\u001b[39mflatten()]))\n\u001b[1;32m      6\u001b[0m ll\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:157\u001b[0m, in \u001b[0;36m_loglike\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m state \u001b[38;5;241m=\u001b[39m CartesianState(x\u001b[38;5;241m=\u001b[39mx[:\u001b[38;5;241m3\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m,:], v\u001b[38;5;241m=\u001b[39mx[\u001b[38;5;241m3\u001b[39m:][\u001b[38;5;28;01mNone\u001b[39;00m,:], time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time)\n\u001b[0;32m--> 157\u001b[0m ras, decs \u001b[38;5;241m=\u001b[39m _ephem(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mtimes,\n\u001b[1;32m    159\u001b[0m     state,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integrator,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integrator_state,\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mobserver_positions,\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    166\u001b[0m xis_etas \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(tangent_plane_projection)(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mra, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mdec, ras, decs\n\u001b[1;32m    168\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:290\u001b[0m, in \u001b[0;36m_ephem\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (ra, dec)\n\u001b[0;32m--> 290\u001b[0m _, (ras, decs) \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    291\u001b[0m     scan_func,\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    293\u001b[0m     (positions[:, \u001b[38;5;241m0\u001b[39m, :], velocities[:, \u001b[38;5;241m0\u001b[39m, :], times, observer_positions),\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ras, decs\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/particle.py:287\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m position, velocity, time, observer_position \u001b[38;5;241m=\u001b[39m scan_over\n\u001b[0;32m--> 287\u001b[0m ra, dec \u001b[38;5;241m=\u001b[39m on_sky(position, velocity, time, observer_position, acc_func)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (ra, dec)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/astrometry/sky_projection.py:103\u001b[0m, in \u001b[0;36mon_sky\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_system_state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m xz, _ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m    104\u001b[0m     scan_func,\n\u001b[1;32m    105\u001b[0m     state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m     length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m X \u001b[38;5;241m=\u001b[39m xz \u001b[38;5;241m-\u001b[39m observer_position\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/astrometry/sky_projection.py:93\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m light_travel_time \u001b[38;5;241m=\u001b[39m earth_distance \u001b[38;5;241m*\u001b[39m INV_SPEED_OF_LIGHT\n\u001b[1;32m     92\u001b[0m positions, velocities, final_system_state, final_integrator_state \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 93\u001b[0m     ias15_evolve(\n\u001b[1;32m     94\u001b[0m         state,\n\u001b[1;32m     95\u001b[0m         acc_func,\n\u001b[1;32m     96\u001b[0m         jnp\u001b[38;5;241m.\u001b[39marray([state\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m-\u001b[39m light_travel_time]),\n\u001b[1;32m     97\u001b[0m         initial_integrator_state,\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_system_state\u001b[38;5;241m.\u001b[39mtracer_positions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1251\u001b[0m, in \u001b[0;36mias15_evolve\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (system_state, integrator_state), (\n\u001b[1;32m   1236\u001b[0m         jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m   1237\u001b[0m             (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m         ),\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[1;32m   1250\u001b[0m (final_system_state, final_integrator_state), (positions, velocities) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1251\u001b[0m     jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(scan_func, (initial_system_state, initial_integrator_state), times)\n\u001b[1;32m   1252\u001b[0m )\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m positions, velocities, final_system_state, final_integrator_state\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1232\u001b[0m, in \u001b[0;36mscan_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1231\u001b[0m final_time \u001b[38;5;241m=\u001b[39m scan_over\n\u001b[0;32m-> 1232\u001b[0m system_state, integrator_state \u001b[38;5;241m=\u001b[39m evolve(\n\u001b[1;32m   1233\u001b[0m     system_state, acceleration_func, final_time, integrator_state\n\u001b[1;32m   1234\u001b[0m )\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (system_state, integrator_state), (\n\u001b[1;32m   1236\u001b[0m     jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m   1237\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     ),\n\u001b[1;32m   1248\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-bccassese@gmail.com/My Drive/Columbia University/Research/Active_Projects/jorbit_dev/jorbit/src/jorbit/integrators/ias15.py:1212\u001b[0m, in \u001b[0;36mevolve\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (step_length \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (iter_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10_000\u001b[39m)\n\u001b[1;32m   1211\u001b[0m final_system_state, final_integrator_state, last_meaningful_dt, iter_num \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1212\u001b[0m     jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mwhile_loop(\n\u001b[1;32m   1213\u001b[0m         cond_func,\n\u001b[1;32m   1214\u001b[0m         step_needed,\n\u001b[1;32m   1215\u001b[0m         (\n\u001b[1;32m   1216\u001b[0m             initial_system_state,\n\u001b[1;32m   1217\u001b[0m             initial_integrator_state,\n\u001b[1;32m   1218\u001b[0m             initial_integrator_state\u001b[38;5;241m.\u001b[39mdt,\n\u001b[1;32m   1219\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1220\u001b[0m         ),\n\u001b[1;32m   1221\u001b[0m     )\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1224\u001b[0m final_integrator_state\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m=\u001b[39m last_meaningful_dt\n",
      "\u001b[0;31mJaxStackTraceBeforeTransformation\u001b[0m: ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop.\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 72 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/jax/_src/lax/control_flow/loops.py:1657\u001b[0m, in \u001b[0;36m_while_transpose_error\u001b[0;34m(*_, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_while_transpose_error\u001b[39m(\u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1657\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReverse-mode differentiation does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlax.while_loop or lax.fori_loop with dynamic start/stop values. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1659\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry using lax.scan, or using fori_loop with static start/stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop."
     ]
    }
   ],
   "source": [
    "jax.grad(p0.loglike)(jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_23957/1719285525.py:47: OptimizeWarning: Unknown solver options: ftol\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 16.556969\n",
      "         Iterations: 4\n",
      "         Function evaluations: 123\n",
      "         Gradient evaluations: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cassese/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_minimize.py:726: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "x0 = jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def objective(x):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def gradient(x):\n",
    "    return -jax.jacfwd(p0.loglike)(x)\n",
    "\n",
    "\n",
    "def scale_decorator(func, gradient_func, x0):\n",
    "    # Determine parameter scales based on initial values\n",
    "    param_scales = jnp.where(jnp.abs(x0) < 1e-10, 1.0, jnp.abs(x0))\n",
    "\n",
    "    # Compute gradient at initial point to determine gradient scales\n",
    "    initial_grad = gradient_func(x0)\n",
    "    grad_scales = jnp.where(jnp.abs(initial_grad) < 1e-10, 1.0, jnp.abs(initial_grad))\n",
    "\n",
    "    def scaled_func(x_scaled):\n",
    "        x_unscaled = x_scaled * param_scales\n",
    "        return func(x_unscaled)\n",
    "\n",
    "    def scaled_gradient(x_scaled):\n",
    "        x_unscaled = x_scaled * param_scales\n",
    "        raw_gradient = gradient_func(x_unscaled)\n",
    "        return (raw_gradient * param_scales) / grad_scales\n",
    "\n",
    "    x0_scaled = x0 / param_scales\n",
    "\n",
    "    def unscale_result(result):\n",
    "        \"\"\"Helper to unscale optimization result\"\"\"\n",
    "        unscaled_result = result\n",
    "        unscaled_result.x = result.x * param_scales\n",
    "        unscaled_result.jac = result.jac * grad_scales / param_scales\n",
    "        return unscaled_result\n",
    "\n",
    "    return (\n",
    "        scaled_func,\n",
    "        scaled_gradient,\n",
    "        x0_scaled,\n",
    "        unscale_result,\n",
    "        param_scales,\n",
    "        grad_scales,\n",
    "    )\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "scaled_obj, scaled_grad, x0_scaled, unscale_result, param_scales, grad_scales = (\n",
    "    scale_decorator(objective, gradient, x0)\n",
    ")\n",
    "\n",
    "# Run optimizer with scaled functions\n",
    "result = minimize(\n",
    "    fun=scaled_obj,\n",
    "    x0=x0_scaled,\n",
    "    jac=scaled_grad,\n",
    "    method=\"BFGS\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"ftol\": 1e-8,\n",
    "        \"gtol\": 1e-8,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Unscale the result\n",
    "final_result = unscale_result(result)\n",
    "\n",
    "# result = minimize(\n",
    "#     fun=scaled_obj,\n",
    "#     x0=scaled_x0,\n",
    "#     jac=scaled_grad,\n",
    "#     method='L-BFGS-B',\n",
    "#     options={\"disp\": True, \"ftol\": 1e-8, \"gtol\": 1e-8,},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_23957/1958235744.py:2: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.25731D+01    |proj g|=  2.00572D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  1.88198D+01    |proj g|=  8.44737D-01\n",
      "\n",
      "At iterate    2    f=  1.79691D+01    |proj g|=  4.14018D-01\n",
      "\n",
      "At iterate    3    f=  1.76637D+01    |proj g|=  3.67542D-01\n",
      "\n",
      "At iterate    4    f=  1.68003D+01    |proj g|=  4.31605D-01\n",
      "\n",
      "At iterate    5    f=  1.65988D+01    |proj g|=  2.21770D-01\n",
      "\n",
      "At iterate    6    f=  1.65458D+01    |proj g|=  2.39457D-02\n",
      "\n",
      "At iterate    7    f=  1.65413D+01    |proj g|=  1.41109D-02\n",
      "\n",
      "At iterate    8    f=  1.65409D+01    |proj g|=  2.58912D-03\n",
      "\n",
      "At iterate    9    f=  1.65409D+01    |proj g|=  4.25874D-04\n",
      "\n",
      "At iterate   10    f=  1.65409D+01    |proj g|=  1.07829D-04\n",
      "\n",
      "At iterate   11    f=  1.65409D+01    |proj g|=  1.68624D-05\n",
      "\n",
      "At iterate   12    f=  1.65409D+01    |proj g|=  1.34610D-05\n",
      "\n",
      "At iterate   13    f=  1.65409D+01    |proj g|=  2.09287D-04\n",
      "  ys=-9.005E-14  -gs= 1.468E-13 BFGS update SKIPPED\n",
      "\n",
      "At iterate   14    f=  1.65409D+01    |proj g|=  4.41940D-05\n",
      "\n",
      "At iterate   15    f=  1.65409D+01    |proj g|=  4.18094D-05\n",
      "\n",
      "At iterate   16    f=  1.65409D+01    |proj g|=  1.02679D-05\n",
      "\n",
      "At iterate   17    f=  1.65409D+01    |proj g|=  2.35160D-05\n",
      "\n",
      "At iterate   18    f=  1.65409D+01    |proj g|=  6.18211D-05\n",
      "\n",
      "At iterate   19    f=  1.65409D+01    |proj g|=  6.18211D-05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     19     96      1     1     0   6.182D-05   1.654D+01\n",
      "  F =   16.540901896326066     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Warning:  more than 10 function and gradient\n",
      "   evaluations in the last line search.  Termination\n",
      "   may possibly be caused by a bad search direction.\n"
     ]
    }
   ],
   "source": [
    "# Run optimizer with scaled functions\n",
    "result = minimize(\n",
    "    fun=scaled_obj,\n",
    "    x0=x0_scaled,\n",
    "    jac=scaled_grad,\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")\n",
    "\n",
    "# Unscale the result\n",
    "final_result = unscale_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.97e+07    \n",
      "       1             11         2.1755e+02      3.72e+01       9.34e-06       2.48e+07    \n",
      "       2             12         1.7156e+02      4.60e+01       2.34e-06       1.03e+07    \n",
      "       3             13         1.6787e+02      3.69e+00       4.67e-06       1.13e+07    \n",
      "       4             14         1.5527e+02      1.26e+01       1.17e-06       5.48e+06    \n",
      "       5             15         1.4912e+02      6.16e+00       2.34e-06       4.27e+06    \n",
      "       6             16         1.4661e+02      2.51e+00       2.34e-06       4.82e+06    \n",
      "       7             17         1.4370e+02      2.91e+00       5.84e-07       2.32e+06    \n",
      "       8             18         1.4163e+02      2.07e+00       1.17e-06       1.97e+06    \n",
      "       9             19         1.4023e+02      1.41e+00       1.17e-06       1.84e+06    \n",
      "      10             20         1.3944e+02      7.83e-01       1.17e-06       2.52e+06    \n",
      "      11             21         1.3874e+02      7.02e-01       2.92e-07       1.31e+06    \n",
      "      12             22         1.3817e+02      5.73e-01       5.84e-07       1.05e+06    \n",
      "      13             23         1.3777e+02      4.02e-01       5.84e-07       1.02e+06    \n",
      "      14             24         1.3752e+02      2.51e-01       5.84e-07       1.06e+06    \n",
      "      15             25         1.3737e+02      1.47e-01       5.84e-07       1.27e+06    \n",
      "      16             26         1.3719e+02      1.74e-01       1.46e-07       6.51e+05    \n",
      "      17             27         1.3707e+02      1.20e-01       2.92e-07       4.64e+05    \n",
      "      18             28         1.3700e+02      7.02e-02       2.92e-07       5.67e+05    \n",
      "      19             29         1.3695e+02      5.05e-02       2.92e-07       5.67e+05    \n",
      "      20             30         1.3691e+02      4.17e-02       7.30e-08       2.96e+05    \n",
      "      21             31         1.3688e+02      3.04e-02       1.46e-07       2.53e+05    \n",
      "      22             32         1.3686e+02      2.50e-02       1.46e-07       2.29e+05    \n",
      "      23             33         1.3685e+02      1.10e-02       1.46e-07       2.99e+05    \n",
      "      24             34         1.3683e+02      1.15e-02       3.65e-08       1.61e+05    \n",
      "      25             35         1.3682e+02      1.03e-02       7.30e-08       1.39e+05    \n",
      "      26             36         1.3682e+02      5.96e-03       7.30e-08       1.14e+05    \n",
      "      27             37         1.3681e+02      5.33e-03       7.30e-08       1.36e+05    \n",
      "      28             38         1.3681e+02      1.28e-03       7.30e-08       1.47e+05    \n",
      "      29             39         1.3681e+02      3.02e-03       1.83e-08       7.42e+04    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 39, initial cost 2.5477e+02, final cost 1.3681e+02, first-order optimality 7.42e+04.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=scaled_obj,\n",
    "    # jac=scaled_grad,\n",
    "    x0=x0_scaled,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeplerianState(semi=Array([2.37860178], dtype=float64), ecc=Array([0.14924396], dtype=float64), inc=Array([6.73363883], dtype=float64), Omega=Array([183.37296835], dtype=float64), omega=Array([140.26403582], dtype=float64), nu=Array([173.65444973], dtype=float64), time=<Time object: scale='utc' format='jd' value=2460676.791666667>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = unscale_result(result).x\n",
    "c = CartesianState(x=x[:3][None, :], v=x[3:][None, :], time=times[0])\n",
    "c.to_keplerian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.00572658,  1.77860293,  0.51974103, -0.00665991, -0.00662871,\n",
       "       -0.00203885], dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(22.57306906, dtype=float64), Array(16.54132464, dtype=float64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_obj(x0_scaled), objective(\n",
    "    result.x\n",
    ")  # why is least_squares returning something unscaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.88e+07    \n",
      "       1             12         1.8042e+02      7.43e+01       2.60e-06       7.60e+06    \n",
      "       2             13         1.7088e+02      9.54e+00       2.60e-06       9.89e+06    \n",
      "       3             14         1.6386e+02      7.02e+00       2.60e-06       7.29e+06    \n",
      "       4             15         1.5672e+02      7.14e+00       6.51e-07       4.85e+06    \n",
      "       5             16         1.5155e+02      5.17e+00       1.30e-06       3.86e+06    \n",
      "       6             17         1.4793e+02      3.62e+00       1.30e-06       3.63e+06    \n",
      "       7             18         1.4540e+02      2.53e+00       1.30e-06       4.47e+06    \n",
      "       8             19         1.4372e+02      1.68e+00       1.30e-06       3.37e+06    \n",
      "       9             20         1.4201e+02      1.71e+00       3.26e-07       2.39e+06    \n",
      "      10             21         1.4067e+02      1.34e+00       6.51e-07       1.82e+06    \n",
      "      11             22         1.3971e+02      9.54e-01       6.51e-07       1.80e+06    \n",
      "      12             23         1.3901e+02      6.97e-01       6.51e-07       2.15e+06    \n",
      "      13             24         1.3859e+02      4.21e-01       6.51e-07       1.62e+06    \n",
      "      14             25         1.3817e+02      4.26e-01       1.63e-07       1.22e+06    \n",
      "      15             26         1.3781e+02      3.59e-01       3.26e-07       8.85e+05    \n",
      "      16             27         1.3756e+02      2.50e-01       3.26e-07       9.13e+05    \n",
      "      17             28         1.3736e+02      1.97e-01       3.26e-07       1.05e+06    \n",
      "      18             29         1.3726e+02      1.00e-01       3.26e-07       7.97e+05    \n",
      "      19             30         1.3715e+02      1.08e-01       8.14e-08       6.26e+05    \n",
      "      20             31         1.3706e+02      9.69e-02       1.63e-07       4.35e+05    \n",
      "      21             32         1.3699e+02      6.30e-02       1.63e-07       4.67e+05    \n",
      "      22             33         1.3694e+02      5.65e-02       1.63e-07       5.09e+05    \n",
      "      23             34         1.3692e+02      2.10e-02       1.63e-07       4.09e+05    \n",
      "      24             35         1.3689e+02      2.81e-02       4.07e-08       3.24e+05    \n",
      "      25             36         1.3686e+02      2.64e-02       8.14e-08       2.17e+05    \n",
      "      26             37         1.3685e+02      1.44e-02       8.14e-08       2.40e+05    \n",
      "      27             38         1.3683e+02      1.62e-02       8.14e-08       2.47e+05    \n",
      "      28             39         1.3683e+02      2.67e-03       8.14e-08       2.10e+05    \n",
      "      29             40         1.3682e+02      7.42e-03       2.03e-08       1.69e+05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 40, initial cost 2.5477e+02, final cost 1.3682e+02, first-order optimality 1.69e+05.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=objective,\n",
    "    # jac=scaled_grad,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.00572091,  1.77860035,  0.51974064, -0.00665998, -0.00662872,\n",
       "       -0.00203887])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.5477e+02                                    2.88e+07    \n",
      "       1             12         1.8045e+02      7.43e+01       2.60e-06       7.59e+06    \n",
      "       2             13         1.7114e+02      9.31e+00       2.60e-06       9.91e+06    \n",
      "       3             14         1.6391e+02      7.24e+00       2.60e-06       7.30e+06    \n",
      "       4             15         1.5681e+02      7.09e+00       6.51e-07       4.83e+06    \n",
      "       5             16         1.5172e+02      5.09e+00       1.30e-06       3.86e+06    \n",
      "       6             17         1.4803e+02      3.69e+00       1.30e-06       3.62e+06    \n",
      "       7             18         1.4557e+02      2.46e+00       1.30e-06       4.47e+06    \n",
      "       8             19         1.4379e+02      1.78e+00       1.30e-06       3.38e+06    \n",
      "       9             20         1.4210e+02      1.69e+00       3.26e-07       2.38e+06    \n",
      "      10             21         1.4078e+02      1.31e+00       6.51e-07       1.82e+06    \n",
      "      11             22         1.3979e+02      9.97e-01       6.51e-07       1.79e+06    \n",
      "      12             23         1.3912e+02      6.69e-01       6.51e-07       2.15e+06    \n",
      "      13             24         1.3864e+02      4.74e-01       6.51e-07       1.63e+06    \n",
      "      14             25         1.3822e+02      4.18e-01       1.63e-07       1.21e+06    \n",
      "      15             26         1.3788e+02      3.48e-01       3.26e-07       8.80e+05    \n",
      "      16             27         1.3760e+02      2.74e-01       3.26e-07       9.07e+05    \n",
      "      17             28         1.3742e+02      1.87e-01       3.26e-07       1.04e+06    \n",
      "      18             29         1.3729e+02      1.28e-01       3.26e-07       7.92e+05    \n",
      "      19             30         1.3719e+02      9.48e-02       3.26e-07       1.11e+06    \n",
      "      20             31         1.3709e+02      1.03e-01       8.14e-08       6.11e+05    \n",
      "      21             32         1.3702e+02      7.27e-02       1.63e-07       4.66e+05    \n",
      "      22             33         1.3697e+02      5.22e-02       1.63e-07       5.06e+05    \n",
      "      23             34         1.3693e+02      3.52e-02       1.63e-07       4.03e+05    \n",
      "      24             35         1.3691e+02      2.59e-02       1.63e-07       5.55e+05    \n",
      "      25             36         1.3688e+02      2.58e-02       4.07e-08       3.05e+05    \n",
      "      26             37         1.3686e+02      1.95e-02       8.14e-08       2.39e+05    \n",
      "      27             38         1.3685e+02      1.46e-02       8.14e-08       2.45e+05    \n",
      "      28             39         1.3684e+02      9.80e-03       8.14e-08       2.06e+05    \n",
      "      29             40         1.3683e+02      7.06e-03       8.14e-08       2.76e+05    \n",
      "      30             41         1.3682e+02      6.48e-03       2.03e-08       1.53e+05    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 41, initial cost 2.5477e+02, final cost 1.3682e+02, first-order optimality 1.53e+05.\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "result = least_squares(\n",
    "    fun=objective,\n",
    "    jac=gradient,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(16.54218454, dtype=float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.05587732e+09, dtype=float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_obj(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 22.573069057682215, Loss on the last accepted step: 0.0, Step size: 1.0, y: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616324795378832e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.5, y: [ -415943.93551124  -739899.38120539  -648959.20283939  -721214.60658886\n",
      " -1277773.22966227 -1121111.00126991], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061641537689634e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.25, y: [-207972.97061395 -369948.80129867 -324479.341546   -360607.30662438\n",
      " -638886.61814549 -560555.50165438], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616596534629104e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.125, y: [-103987.48816531 -184973.51134531 -162239.41089931 -180303.65664214\n",
      " -319443.3123871  -280277.75184662], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061695882978612e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0625, y: [ -51994.74694099  -92485.86636863  -81119.44557596  -90151.83165102\n",
      " -159721.65950791 -140138.87694274], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0617683345697946e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.03125, y: [-25998.37632882 -46242.04388029 -40559.46291429 -45075.91915547\n",
      " -79860.83306831 -70069.43949079], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0619132127784445e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.015625, y: [-13000.19102274 -23120.13263612 -20279.47158346 -22537.96290769\n",
      " -39930.41984851 -35034.72076482], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0622029036929414e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0078125, y: [ -6501.0983697  -11559.17701403 -10139.47591804 -11268.9847838\n",
      " -19965.21323861 -17517.36140184], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0627822059249875e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00390625, y: [-3251.55204318 -5778.69920299 -5069.47808533 -5634.49572185\n",
      " -9982.60993366 -8758.68172035], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0639392854639366e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.001953125, y: [-1626.77887992 -2888.46029747 -2534.47916897 -2817.25119088\n",
      " -4991.30828119 -4379.3418796 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0662545377147533e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0009765625, y: [ -814.39229829 -1443.34084471 -1266.9797108  -1408.62892539\n",
      " -2495.65745495 -2189.67195923], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0707941370813786e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00048828125, y: [ -408.19900748  -720.78111833  -633.22998171  -704.31779265\n",
      " -1247.83204183 -1094.83699904], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.066223626450892e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.000244140625, y: [-205.10236207 -359.50125514 -316.35511716 -352.16222628 -623.91933527\n",
      " -547.41951895], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1.741263176329129e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0001220703125, y: [-103.55403937 -178.86132354 -157.91768489 -176.08444309 -311.96298199\n",
      " -273.7107789 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1210058266610485.2, Loss on the last accepted step: 22.573069057682215, Step size: 6.103515625e-05, y: [ -52.77987802  -88.54135775  -78.69896875  -88.0455515  -155.98480535\n",
      " -136.85640888], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1557258003248586.5, Loss on the last accepted step: 22.573069057682215, Step size: 3.0517578125e-05, y: [-27.39279734 -43.38137485 -39.08961069 -44.0261057  -77.99571703\n",
      " -68.42922386], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 802058121937971.2, Loss on the last accepted step: 22.573069057682215, Step size: 1.52587890625e-05, y: [-14.699257   -20.8013834  -19.28493165 -22.0163828  -39.00117287\n",
      " -34.21563136], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 248592818236005.0, Loss on the last accepted step: 22.573069057682215, Step size: 7.62939453125e-06, y: [ -8.35248683  -9.51138767  -9.38259214 -11.01152135 -19.50390079\n",
      " -17.10883511], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 69273677478740.26, Loss on the last accepted step: 22.573069057682215, Step size: 3.814697265625e-06, y: [-5.17910175 -3.86638981 -4.43142238 -5.50909063 -9.75526475 -8.55543698], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18329591762755.668, Loss on the last accepted step: 22.573069057682215, Step size: 1.9073486328125e-06, y: [-3.59240921 -1.04389088 -1.9558375  -2.75787527 -4.88094673 -4.27873792], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4717159011385.629, Loss on the last accepted step: 22.573069057682215, Step size: 9.5367431640625e-07, y: [-2.79906293  0.36735859 -0.71804506 -1.38226759 -2.44378772 -2.14038838], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1196347948988.4683, Loss on the last accepted step: 22.573069057682215, Step size: 4.76837158203125e-07, y: [-2.4023898   1.07298332 -0.09914884 -0.69446375 -1.22520822 -1.07121362], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 301099069639.34674, Loss on the last accepted step: 22.573069057682215, Step size: 2.384185791015625e-07, y: [-2.20405323  1.42579568  0.21029927 -0.35056183 -0.61591847 -0.53662624], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 75478377806.22424, Loss on the last accepted step: 22.573069057682215, Step size: 1.1920928955078125e-07, y: [-2.10488495  1.60220187  0.36502333 -0.17861087 -0.31127359 -0.26933254], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18884195537.268745, Loss on the last accepted step: 22.573069057682215, Step size: 5.960464477539063e-08, y: [-2.05530081  1.69040496  0.44238535 -0.09263539 -0.15895115 -0.1356857 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4721389919.197352, Loss on the last accepted step: 22.573069057682215, Step size: 2.9802322387695312e-08, y: [-2.03050873  1.73450651  0.48106637 -0.04964765 -0.08278993 -0.06886228], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1180222493.5438774, Loss on the last accepted step: 22.573069057682215, Step size: 1.4901161193847656e-08, y: [-2.0181127   1.75655728  0.50040687 -0.02815378 -0.04470932 -0.03545056], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 295006572.2998562, Loss on the last accepted step: 22.573069057682215, Step size: 7.450580596923828e-09, y: [-2.01191468  1.76758266  0.51007713 -0.01740684 -0.02566902 -0.01874471], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 73732297.54740189, Loss on the last accepted step: 22.573069057682215, Step size: 3.725290298461914e-09, y: [-2.00881567  1.77309536  0.51491225 -0.01203337 -0.01614887 -0.01039178], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18424280.514657617, Loss on the last accepted step: 22.573069057682215, Step size: 1.862645149230957e-09, y: [-2.00726617  1.7758517   0.51732982 -0.00934664 -0.01138879 -0.00621532], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4601804.604893329, Loss on the last accepted step: 22.573069057682215, Step size: 9.313225746154785e-10, y: [-2.00649142  1.77722988  0.5185386  -0.00800327 -0.00900875 -0.00412709], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1148343.1054827906, Loss on the last accepted step: 22.573069057682215, Step size: 4.656612873077393e-10, y: [-2.00610404  1.77791896  0.51914299 -0.00733159 -0.00781873 -0.00308297], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 286042.3052457798, Loss on the last accepted step: 22.573069057682215, Step size: 2.3283064365386963e-10, y: [-2.00591035  1.77826351  0.51944519 -0.00699575 -0.00722372 -0.00256091], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 70997.56991652744, Loss on the last accepted step: 22.573069057682215, Step size: 1.1641532182693481e-10, y: [-2.00581351  1.77843578  0.51959628 -0.00682783 -0.00692622 -0.00229988], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 17501.387473158673, Loss on the last accepted step: 22.573069057682215, Step size: 5.820766091346741e-11, y: [-2.00576509  1.77852192  0.51967183 -0.00674387 -0.00677747 -0.00216937], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4259.81340728817, Loss on the last accepted step: 22.573069057682215, Step size: 2.9103830456733704e-11, y: [-2.00574087  1.77856498  0.51970961 -0.00670189 -0.00670309 -0.00210411], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1015.6520619330806, Loss on the last accepted step: 22.573069057682215, Step size: 1.4551915228366852e-11, y: [-2.00572877  1.77858652  0.51972849 -0.0066809  -0.0066659  -0.00207148], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 237.72732879440176, Loss on the last accepted step: 22.573069057682215, Step size: 7.275957614183426e-12, y: [-2.00572272  1.77859728  0.51973794 -0.0066704  -0.00664731 -0.00205517], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 59.80389829475062, Loss on the last accepted step: 22.573069057682215, Step size: 3.637978807091713e-12, y: [-2.00571969  1.77860267  0.51974266 -0.00666515 -0.00663801 -0.00204701], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 23.60190956181135, Loss on the last accepted step: 22.573069057682215, Step size: 1.8189894035458565e-12, y: [-2.00571818  1.77860536  0.51974502 -0.00666253 -0.00663336 -0.00204293], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18.69084593908376, Loss on the last accepted step: 22.573069057682215, Step size: 1.0, y: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712423596641361.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.5, y: [-164091.019947   -293283.07549806 -257138.43638475   55793.39307797\n",
      "   97525.82686763   85840.38200621], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712480206610871.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.25, y: [ -82046.51283221 -146640.64844568 -128568.95831927   27896.69320838\n",
      "   48762.9101183    42920.18998266], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712597410940664.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.125, y: [-41024.25927482 -73319.43491949 -64284.21928654  13948.34327358\n",
      "  24381.45174363  21460.09397088], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6712863729759803.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0625, y: [-20513.13249612 -36658.82815639 -32141.84977017   6974.16830618\n",
      "  12190.7225563   10730.045965  ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6713652404484542.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.03125, y: [-10257.56910677 -18328.52477484 -16070.66501198   3487.08082248\n",
      "   6095.35796263   5365.02196205], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6717293985420048.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.015625, y: [-5129.78741209 -9163.37308407 -8035.07263289  1743.53708063\n",
      "  3047.67566579  2682.50996058], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6741501953156261.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0078125, y: [-2565.89656476 -4580.79723868 -4017.27644335   871.76520971\n",
      "  1523.83451738  1341.25395984], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6940865313257104.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00390625, y: [-1283.95114109 -2289.50931599 -2008.37834857   435.87927424\n",
      "   761.91394317   670.62595948], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1.0078867560568664e+16, Loss on the last accepted step: 18.69084593908376, Step size: 0.001953125, y: [ -642.97842925 -1143.86535464 -1003.92930119   217.93630651\n",
      "   380.95365607   335.31195929], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3566118524490609.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0009765625, y: [-322.49207334 -571.04337397 -501.70477749  108.96482265  190.47351251\n",
      "  167.6549592 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4121714667306713.5, Loss on the last accepted step: 18.69084593908376, Step size: 0.00048828125, y: [-162.24889538 -284.63238363 -250.59251565   54.47908072   95.23344074\n",
      "   83.82645915], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 5402129976275411.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.000244140625, y: [ -82.1273064  -141.42688846 -125.03638472   27.23620975   47.61340485\n",
      "   41.91220913], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 412858984188575.94, Loss on the last accepted step: 18.69084593908376, Step size: 0.0001220703125, y: [-42.06651191 -69.82414088 -62.25831926  13.61477427  23.80338691\n",
      "  20.95508412], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 81851767735781.38, Loss on the last accepted step: 18.69084593908376, Step size: 6.103515625e-05, y: [-22.03611467 -34.02276709 -30.86928653   6.80405652  11.89837793\n",
      "  10.47652161], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 18050294594262.91, Loss on the last accepted step: 18.69084593908376, Step size: 3.0517578125e-05, y: [-12.02091604 -16.12208019 -15.17477016   3.39869765   5.94587345\n",
      "   5.23724036], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4228206146111.887, Loss on the last accepted step: 18.69084593908376, Step size: 1.52587890625e-05, y: [-7.01331673 -7.17173674 -7.32751198  1.69601822  2.9696212   2.61759973], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1022977564452.0562, Loss on the last accepted step: 18.69084593908376, Step size: 7.62939453125e-06, y: [-4.50951708 -2.69656502 -3.40388289  0.8446785   1.48149508  1.30777942], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 251731407653.32993, Loss on the last accepted step: 18.69084593908376, Step size: 3.814697265625e-06, y: [-3.25761725 -0.45897916 -1.44206834  0.41900864  0.73743202  0.65286926], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 62492595560.648544, Loss on the last accepted step: 18.69084593908376, Step size: 1.9073486328125e-06, y: [-2.63166733  0.65981377 -0.46116107  0.20617371  0.36540049  0.32541419], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15578567914.51824, Loss on the last accepted step: 18.69084593908376, Step size: 9.5367431640625e-07, y: [-2.31869238  1.21921024  0.02929256  0.09975625  0.17938473  0.16168665], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3890104800.188278, Loss on the last accepted step: 18.69084593908376, Step size: 4.76837158203125e-07, y: [-2.1622049   1.49890847  0.27451938  0.04654751  0.08637684  0.07982288], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 972008146.5688134, Loss on the last accepted step: 18.69084593908376, Step size: 2.384185791015625e-07, y: [-2.08396116  1.63875759  0.39713279  0.01994315  0.0398729   0.03889099], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 242925710.08179784, Loss on the last accepted step: 18.69084593908376, Step size: 1.1920928955078125e-07, y: [-2.04483929  1.70868215  0.4584395   0.00664097  0.01662093  0.01842505], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 60713707.81796045, Loss on the last accepted step: 18.69084593908376, Step size: 5.960464477539063e-08, y: [-2.02527836e+00  1.74364443e+00  4.89092849e-01 -1.01262134e-05\n",
      "  4.99494659e-03  8.19207873e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15171976.190410793, Loss on the last accepted step: 18.69084593908376, Step size: 2.9802322387695312e-08, y: [-2.01549789e+00  1.76112557e+00  5.04419525e-01 -3.33567199e-03\n",
      " -8.18046069e-04  3.07559314e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3790066.3595114155, Loss on the last accepted step: 18.69084593908376, Step size: 1.4901161193847656e-08, y: [-2.01060765e+00  1.76986614e+00  5.12082863e-01 -4.99844487e-03\n",
      " -3.72454240e-03  5.17350337e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 946095.484585892, Loss on the last accepted step: 18.69084593908376, Step size: 7.450580596923828e-09, y: [-2.00816254e+00  1.77423642e+00  5.15914532e-01 -5.82983131e-03\n",
      " -5.17779057e-03 -7.61771063e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 235824.76051909928, Loss on the last accepted step: 18.69084593908376, Step size: 3.725290298461914e-09, y: [-2.00693998e+00  1.77642156e+00  5.17830366e-01 -6.24552454e-03\n",
      " -5.90441465e-03 -1.40133176e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 58614.1952424372, Loss on the last accepted step: 18.69084593908376, Step size: 1.862645149230957e-09, y: [-2.00632870e+00  1.77751413e+00  5.18788284e-01 -6.45337115e-03\n",
      " -6.26772669e-03 -1.72111211e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 14489.629241776953, Loss on the last accepted step: 18.69084593908376, Step size: 9.313225746154785e-10, y: [-2.00602306e+00  1.77806042e+00  5.19267242e-01 -6.55729445e-03\n",
      " -6.44938271e-03 -1.88100229e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3547.4651612417106, Loss on the last accepted step: 18.69084593908376, Step size: 4.656612873077393e-10, y: [-2.00587024e+00  1.77833356e+00  5.19506722e-01 -6.60925611e-03\n",
      " -6.54021072e-03 -1.96094738e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 856.4053607312029, Loss on the last accepted step: 18.69084593908376, Step size: 2.3283064365386963e-10, y: [-2.00579383e+00  1.77847013e+00  5.19626461e-01 -6.63523693e-03\n",
      " -6.58562473e-03 -2.00091992e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 205.88007521941597, Loss on the last accepted step: 18.69084593908376, Step size: 1.1641532182693481e-10, y: [-2.00575563  1.77853842  0.51968633 -0.00664823 -0.00660833 -0.00202091], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 54.3684702234115, Loss on the last accepted step: 18.69084593908376, Step size: 5.820766091346741e-11, y: [-2.00573652  1.77857256  0.51971627 -0.00665472 -0.00661969 -0.0020309 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 22.050412017517964, Loss on the last accepted step: 18.69084593908376, Step size: 2.9103830456733704e-11, y: [-2.00572697  1.77858963  0.51973123 -0.00665797 -0.00662536 -0.0020359 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 16.750817717121407, Loss on the last accepted step: 18.69084593908376, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 131523100784635.75, Loss on the last accepted step: 16.750817717121407, Step size: 0.5, y: [ -7.84415605 -10.72565593  18.49561315 -17.99848853 -43.15785489\n",
      "  60.76072742], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 86567744527131.1, Loss on the last accepted step: 16.750817717121407, Step size: 0.25, y: [ -4.92493912  -4.47352888   9.50767594  -9.00257406 -21.58224154\n",
      "  30.37934451], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 43008345261396.5, Loss on the last accepted step: 16.750817717121407, Step size: 0.125, y: [ -3.46533066  -1.34746536   5.01370733  -4.50461683 -10.79443487\n",
      "  15.18865306], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16941324528617.014, Loss on the last accepted step: 16.750817717121407, Step size: 0.0625, y: [-2.73552643  0.21556641  2.76672302 -2.25563821 -5.40053154  7.59330733], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 5567731542010.476, Loss on the last accepted step: 16.750817717121407, Step size: 0.03125, y: [-2.37062431  0.99708229  1.64323087 -1.1311489  -2.70357987  3.79563447], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1621028872905.0005, Loss on the last accepted step: 16.750817717121407, Step size: 0.015625, y: [-2.18817325  1.38784023  1.08148479 -0.56890425 -1.35510403  1.89679804], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 439222250849.9029, Loss on the last accepted step: 16.750817717121407, Step size: 0.0078125, y: [-2.09694772  1.5832192   0.80061176 -0.28778192 -0.68086612  0.94737982], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 114392382418.5628, Loss on the last accepted step: 16.750817717121407, Step size: 0.00390625, y: [-2.05133496  1.68090868  0.66017524 -0.14722076 -0.34374716  0.47267071], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 29180371886.50014, Loss on the last accepted step: 16.750817717121407, Step size: 0.001953125, y: [-2.02852858  1.72975343  0.58995698 -0.07694018 -0.17518768  0.23531616], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7366575042.67023, Loss on the last accepted step: 16.750817717121407, Step size: 0.0009765625, y: [-2.01712539  1.7541758   0.55484785 -0.04179988 -0.09090794  0.11663888], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1850385017.0490952, Loss on the last accepted step: 16.750817717121407, Step size: 0.00048828125, y: [-2.01142379  1.76638698  0.53729328 -0.02422974 -0.04876807  0.05730024], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 463673321.75153077, Loss on the last accepted step: 16.750817717121407, Step size: 0.000244140625, y: [-2.00857299  1.77249258  0.528516   -0.01544467 -0.02769814  0.02763093], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 116051921.96936578, Loss on the last accepted step: 16.750817717121407, Step size: 0.0001220703125, y: [-2.00714759  1.77554537  0.52412736 -0.01105213 -0.01716317  0.01279627], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 29029622.769996047, Loss on the last accepted step: 16.750817717121407, Step size: 6.103515625e-05, y: [-2.0064349   1.77707177  0.52193304 -0.00885586 -0.01189568  0.00537894], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7259492.578199817, Loss on the last accepted step: 16.750817717121407, Step size: 3.0517578125e-05, y: [-2.00607855e+00  1.77783497e+00  5.20835877e-01 -7.75772805e-03\n",
      " -9.26194222e-03  1.67027073e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1815144.6584349484, Loss on the last accepted step: 16.750817717121407, Step size: 1.52587890625e-05, y: [-2.00590037e+00  1.77821657e+00  5.20287297e-01 -7.20866100e-03\n",
      " -7.94507129e-03 -1.84061726e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 453831.0432556629, Loss on the last accepted step: 16.750817717121407, Step size: 7.62939453125e-06, y: [-2.00581128e+00  1.77840737e+00  5.20013007e-01 -6.93412748e-03\n",
      " -7.28663582e-03 -1.11122795e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 113474.34408761517, Loss on the last accepted step: 16.750817717121407, Step size: 3.814697265625e-06, y: [-2.00576674e+00  1.77850277e+00  5.19875862e-01 -6.79686072e-03\n",
      " -6.95741809e-03 -1.57481107e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 28381.6424259518, Loss on the last accepted step: 16.750817717121407, Step size: 1.9073486328125e-06, y: [-2.00574447e+00  1.77855047e+00  5.19807290e-01 -6.72822734e-03\n",
      " -6.79280922e-03 -1.80660262e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7108.030810167587, Loss on the last accepted step: 16.750817717121407, Step size: 9.5367431640625e-07, y: [-2.00573333e+00  1.77857432e+00  5.19773003e-01 -6.69391065e-03\n",
      " -6.71050479e-03 -1.92249840e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1789.5756604726666, Loss on the last accepted step: 16.750817717121407, Step size: 4.76837158203125e-07, y: [-2.00572776e+00  1.77858624e+00  5.19755860e-01 -6.67675230e-03\n",
      " -6.66935257e-03 -1.98044629e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 459.9564923332625, Loss on the last accepted step: 16.750817717121407, Step size: 2.384185791015625e-07, y: [-2.00572498  1.77859221  0.51974729 -0.00666817 -0.00664878 -0.00200942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 127.55159883155568, Loss on the last accepted step: 16.750817717121407, Step size: 1.1920928955078125e-07, y: [-2.00572359  1.77859519  0.519743   -0.00666388 -0.00663849 -0.00202391], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44.450647194213346, Loss on the last accepted step: 16.750817717121407, Step size: 5.960464477539063e-08, y: [-2.00572289  1.77859668  0.51974086 -0.00666174 -0.00663334 -0.00203115], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23.675586679764415, Loss on the last accepted step: 16.750817717121407, Step size: 2.9802322387695312e-08, y: [-2.00572254  1.77859742  0.51973979 -0.00666067 -0.00663077 -0.00203477], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18.481915022049037, Loss on the last accepted step: 16.750817717121407, Step size: 1.4901161193847656e-08, y: [-2.00572237  1.7785978   0.51973925 -0.00666013 -0.00662949 -0.00203658], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.183544363163342, Loss on the last accepted step: 16.750817717121407, Step size: 7.450580596923828e-09, y: [-2.00572228  1.77859798  0.51973899 -0.00665986 -0.00662884 -0.00203749], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.85897569213404, Loss on the last accepted step: 16.750817717121407, Step size: 3.725290298461914e-09, y: [-2.00572224  1.77859808  0.51973885 -0.00665973 -0.00662852 -0.00203794], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.777845317274387, Loss on the last accepted step: 16.750817717121407, Step size: 1.862645149230957e-09, y: [-2.00572222  1.77859812  0.51973878 -0.00665966 -0.00662836 -0.00203817], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.757568678489104, Loss on the last accepted step: 16.750817717121407, Step size: 9.313225746154785e-10, y: [-2.00572221  1.77859815  0.51973875 -0.00665963 -0.00662828 -0.00203828], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.752502486003525, Loss on the last accepted step: 16.750817717121407, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859816  0.51973873 -0.00665961 -0.00662824 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751237423462015, Loss on the last accepted step: 16.750817717121407, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859816  0.51973873 -0.0066596  -0.00662822 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75092190092924, Loss on the last accepted step: 16.750817717121407, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750843391542276, Loss on the last accepted step: 16.750817717121407, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.7508239500109, Loss on the last accepted step: 16.750817717121407, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750819182552362, Loss on the last accepted step: 16.750817717121407, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750818037101286, Loss on the last accepted step: 16.750817717121407, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817773858405, Loss on the last accepted step: 16.750817717121407, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75081771961047, Loss on the last accepted step: 16.750817717121407, Step size: 1.8189894035458565e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817711967763, Loss on the last accepted step: 16.750817717121407, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18363996951.55463, Loss on the last accepted step: 16.750817711967763, Step size: 0.5, y: [-2.48294375  2.96607241 -0.53189293 -1.00310846  0.18991806  0.41706305], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 6756271546.493962, Loss on the last accepted step: 16.750817711967763, Step size: 0.25, y: [-2.24433297  2.37233529 -0.0060771  -0.50488403  0.09164493  0.20751233], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2146876769.4546375, Loss on the last accepted step: 16.750817711967763, Step size: 0.125, y: [-2.12502759  2.07546673  0.25683081 -0.25577181  0.04250837  0.10273697], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 617037836.1087826, Loss on the last accepted step: 16.750817711967763, Step size: 0.0625, y: [-2.06537489  1.92703245  0.38828476 -0.1312157   0.01794008  0.05034929], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 166608057.54618162, Loss on the last accepted step: 16.750817711967763, Step size: 0.03125, y: [-2.03554854  1.85281531  0.45401174 -0.06893765  0.00565594  0.02415545], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 43391001.33768293, Loss on the last accepted step: 16.750817711967763, Step size: 0.015625, y: [-2.02063537e+00  1.81570674e+00  4.86875228e-01 -3.77986209e-02\n",
      " -4.86129721e-04  1.10585260e-02], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 11079760.041092493, Loss on the last accepted step: 16.750817711967763, Step size: 0.0078125, y: [-2.01317878  1.79715245  0.50330697 -0.02222911 -0.00355717  0.00451007], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2799962.4532937957, Loss on the last accepted step: 16.750817711967763, Step size: 0.00390625, y: [-2.00945049e+00  1.78787531e+00  5.11522845e-01 -1.44443507e-02\n",
      " -5.09268276e-03  1.23583595e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 703820.9772255587, Loss on the last accepted step: 16.750817711967763, Step size: 0.001953125, y: [-2.00758634e+00  1.78323674e+00  5.15630781e-01 -1.05519724e-02\n",
      " -5.86044160e-03 -4.01279058e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 176449.30590998745, Loss on the last accepted step: 16.750817711967763, Step size: 0.0009765625, y: [-2.00665427e+00  1.78091746e+00  5.17684749e-01 -8.60578317e-03\n",
      " -6.24432101e-03 -1.21983656e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44185.34626821333, Loss on the last accepted step: 16.750817711967763, Step size: 0.00048828125, y: [-2.00618823e+00  1.77975781e+00  5.18711733e-01 -7.63268858e-03\n",
      " -6.43626072e-03 -1.62911532e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 11066.473009255686, Loss on the last accepted step: 16.750817711967763, Step size: 0.000244140625, y: [-2.00595521e+00  1.77917799e+00  5.19225225e-01 -7.14614129e-03\n",
      " -6.53223058e-03 -1.83375469e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2780.1287031015477, Loss on the last accepted step: 16.750817711967763, Step size: 0.0001220703125, y: [-2.00583871e+00  1.77888808e+00  5.19481971e-01 -6.90286764e-03\n",
      " -6.58021551e-03 -1.93607438e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 707.7135717177605, Loss on the last accepted step: 16.750817711967763, Step size: 6.103515625e-05, y: [-2.00578045e+00  1.77874313e+00  5.19610344e-01 -6.78123081e-03\n",
      " -6.60420797e-03 -1.98723422e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 189.50619648032514, Loss on the last accepted step: 16.750817711967763, Step size: 3.0517578125e-05, y: [-2.00575132  1.77867065  0.51967453 -0.00672041 -0.0066162  -0.00201281], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 59.9414478351904, Loss on the last accepted step: 16.750817711967763, Step size: 1.52587890625e-05, y: [-2.00573676  1.77863441  0.51970662 -0.00669    -0.0066222  -0.0020256 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 27.548673603864408, Loss on the last accepted step: 16.750817711967763, Step size: 7.62939453125e-06, y: [-2.00572948  1.77861629  0.51972267 -0.0066748  -0.0066252  -0.002032  ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.450294040702488, Loss on the last accepted step: 16.750817711967763, Step size: 3.814697265625e-06, y: [-2.00572584  1.77860723  0.51973069 -0.0066672  -0.0066267  -0.0020352 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.425682053821195, Loss on the last accepted step: 16.750817711967763, Step size: 1.9073486328125e-06, y: [-2.00572402  1.7786027   0.51973471 -0.0066634  -0.00662745 -0.0020368 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.91953015673455, Loss on the last accepted step: 16.750817711967763, Step size: 9.5367431640625e-07, y: [-2.00572311  1.77860043  0.51973671 -0.00666149 -0.00662783 -0.00203759], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.79299390837364, Loss on the last accepted step: 16.750817711967763, Step size: 4.76837158203125e-07, y: [-2.00572265  1.7785993   0.51973771 -0.00666054 -0.00662801 -0.00203799], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.76136082076422, Loss on the last accepted step: 16.750817711967763, Step size: 2.384185791015625e-07, y: [-2.00572242  1.77859874  0.51973822 -0.00666007 -0.00662811 -0.00203819], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75345308797457, Loss on the last accepted step: 16.750817711967763, Step size: 1.1920928955078125e-07, y: [-2.00572231  1.77859845  0.51973847 -0.00665983 -0.00662815 -0.00203829], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751476336636447, Loss on the last accepted step: 16.750817711967763, Step size: 5.960464477539063e-08, y: [-2.00572225  1.77859831  0.51973859 -0.00665971 -0.00662818 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75098238989647, Loss on the last accepted step: 16.750817711967763, Step size: 2.9802322387695312e-08, y: [-2.00572222  1.77859824  0.51973865 -0.00665965 -0.00662819 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750858816951485, Loss on the last accepted step: 16.750817711967763, Step size: 1.4901161193847656e-08, y: [-2.00572221  1.77859821  0.51973869 -0.00665962 -0.00662819 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75082792186553, Loss on the last accepted step: 16.750817711967763, Step size: 7.450580596923828e-09, y: [-2.0057222   1.77859819  0.5197387  -0.00665961 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750820333891976, Loss on the last accepted step: 16.750817711967763, Step size: 3.725290298461914e-09, y: [-2.0057222   1.77859818  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75081838074599, Loss on the last accepted step: 16.750817711967763, Step size: 1.862645149230957e-09, y: [-2.0057222   1.77859817  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817882985412, Loss on the last accepted step: 16.750817711967763, Step size: 9.313225746154785e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817757323993, Loss on the last accepted step: 16.750817711967763, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817724620212, Loss on the last accepted step: 16.750817711967763, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817715789637, Loss on the last accepted step: 16.750817711967763, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817713246494, Loss on the last accepted step: 16.750817711967763, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712338936, Loss on the last accepted step: 16.750817711967763, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712170505, Loss on the last accepted step: 16.750817711967763, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712047162, Loss on the last accepted step: 16.750817711967763, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817712051596, Loss on the last accepted step: 16.750817711967763, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750817711896932, Loss on the last accepted step: 16.750817711967763, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 122247.18229677227, Loss on the last accepted step: 16.750817711896932, Step size: 0.5, y: [-5.55581302  3.52387244  0.7868994  -4.04584331  2.01188892  0.30655285], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 72213.8255023129, Loss on the last accepted step: 16.750817711896932, Step size: 0.25, y: [-3.78076761  2.65123531  0.65331906 -2.02625145  1.00263036  0.15225723], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 35763.522801532694, Loss on the last accepted step: 16.750817711896932, Step size: 0.125, y: [-2.8932449   2.21491674  0.58652889 -1.01645552  0.49800108  0.07510942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 14598.339188521817, Loss on the last accepted step: 16.750817711896932, Step size: 0.0625, y: [-2.44948355  1.99675745  0.5531338  -0.51155756  0.24568644  0.03653551], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 5018.307460901265, Loss on the last accepted step: 16.750817711896932, Step size: 0.03125, y: [-2.22760287  1.88767781  0.53643626 -0.25910858  0.11952912  0.01724856], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1523.2909212981372, Loss on the last accepted step: 16.750817711896932, Step size: 0.015625, y: [-2.11666253  1.83313799  0.52808749 -0.13288409  0.05645046  0.00760508], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 433.97210686816834, Loss on the last accepted step: 16.750817711896932, Step size: 0.0078125, y: [-2.06119237  1.80586808  0.5239131  -0.06977184  0.02491113  0.00278334], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 126.76368929532103, Loss on the last accepted step: 16.750817711896932, Step size: 0.00390625, y: [-2.03345728e+00  1.79223312e+00  5.21825910e-01 -3.82157168e-02\n",
      "  9.14146457e-03  3.72475019e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 44.977445191646794, Loss on the last accepted step: 16.750817711896932, Step size: 0.001953125, y: [-2.01958974e+00  1.78541565e+00  5.20782314e-01 -2.24376554e-02\n",
      "  1.25663207e-03 -8.32959524e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23.880306578305532, Loss on the last accepted step: 16.750817711896932, Step size: 0.0009765625, y: [-2.01265597e+00  1.78200691e+00  5.20260515e-01 -1.45486247e-02\n",
      " -2.68578418e-03 -1.43567680e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 18.531962204523392, Loss on the last accepted step: 16.750817711896932, Step size: 0.00048828125, y: [-2.00918908e+00  1.78030254e+00  5.19999616e-01 -1.06041093e-02\n",
      " -4.65699231e-03 -1.73703543e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.19068430677233, Loss on the last accepted step: 16.750817711896932, Step size: 0.000244140625, y: [-2.00745564e+00  1.77945035e+00  5.19869167e-01 -8.63185167e-03\n",
      " -5.64259637e-03 -1.88771475e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.857466963444338, Loss on the last accepted step: 16.750817711896932, Step size: 0.0001220703125, y: [-2.00658892e+00  1.77902426e+00  5.19803942e-01 -7.64572283e-03\n",
      " -6.13539840e-03 -1.96305441e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.77574520488109, Loss on the last accepted step: 16.750817711896932, Step size: 6.103515625e-05, y: [-2.00615556e+00  1.77881122e+00  5.19771330e-01 -7.15265841e-03\n",
      " -6.38179942e-03 -2.00072424e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75617269907901, Loss on the last accepted step: 16.750817711896932, Step size: 3.0517578125e-05, y: [-2.00593888  1.77870469  0.51975502 -0.00690613 -0.006505   -0.00201956], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75171681690975, Loss on the last accepted step: 16.750817711896932, Step size: 1.52587890625e-05, y: [-2.00583054  1.77865143  0.51974687 -0.00678286 -0.0065666  -0.00202898], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750822485870852, Loss on the last accepted step: 16.750817711896932, Step size: 7.62939453125e-06, y: [-2.00577637  1.7786248   0.51974279 -0.00672123 -0.0065974  -0.00203369], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75070899419128, Loss on the last accepted step: 16.750817711896932, Step size: 1.0, y: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 224.62754792706178, Loss on the last accepted step: 16.75070899419128, Step size: 0.5, y: [-6.26535506  3.88206434  0.8417004   0.2307619  -0.12013257 -0.01951041], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 100.82484958016879, Loss on the last accepted step: 16.75070899419128, Step size: 0.25, y: [-4.13555217  2.83033791  0.68072058  0.11203575 -0.06337268 -0.01077322], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 49.75759425604515, Loss on the last accepted step: 16.75070899419128, Step size: 0.125, y: [-3.07065072  2.3044747   0.60023067  0.05267267 -0.03499274 -0.00640463], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 29.10252300326593, Loss on the last accepted step: 16.75070899419128, Step size: 0.0625, y: [-2.5382      2.04154309  0.55998571  0.02299113 -0.02080277 -0.00422034], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 20.615770620524625, Loss on the last accepted step: 16.75070899419128, Step size: 0.03125, y: [-2.27197464  1.91007729  0.53986323  0.00815036 -0.01370779 -0.00312819], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 17.62360062464468, Loss on the last accepted step: 16.75070899419128, Step size: 0.015625, y: [-2.13886196e+00  1.84434439e+00  5.29801994e-01  7.29974286e-04\n",
      " -1.01602931e-02 -2.58211375e-03], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 16.810958430674155, Loss on the last accepted step: 16.75070899419128, Step size: 0.0078125, y: [-2.07230562  1.81147794  0.52477137 -0.00298022 -0.00838655 -0.00230908], y on the last accepted step: [-2.00574928  1.77861149  0.51974076 -0.00669041 -0.0066128  -0.00203604]\n",
      "Loss on this step: 16.667565468019887, Loss on the last accepted step: 16.75070899419128, Step size: 1.0, y: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 60.345487249090766, Loss on the last accepted step: 16.667565468019887, Step size: 0.5, y: [-1.43658772e+00  1.49758209e+00  4.76739940e-01 -3.76857997e-02\n",
      "  8.19311683e-03  2.37237244e-04], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 19.808296337302217, Loss on the last accepted step: 16.667565468019887, Step size: 0.25, y: [-1.73780759e+00  1.64631340e+00  4.99498002e-01 -2.12605570e-02\n",
      "  3.46721645e-04 -9.67660486e-04], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 17.146001891809924, Loss on the last accepted step: 16.667565468019887, Step size: 0.125, y: [-1.88841752e+00  1.72067906e+00  5.10877034e-01 -1.30479357e-02\n",
      " -3.57647595e-03 -1.57010935e-03], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 16.714983595230724, Loss on the last accepted step: 16.667565468019887, Step size: 0.0625, y: [-1.96372249e+00  1.75786188e+00  5.16566549e-01 -8.94162499e-03\n",
      " -5.53807474e-03 -1.87133378e-03], y on the last accepted step: [-2.03902745  1.79504471  0.52225607 -0.00483531 -0.00749967 -0.00217256]\n",
      "Loss on this step: 16.64579281615707, Loss on the last accepted step: 16.667565468019887, Step size: 1.0, y: [-2.00137497  1.7764533   0.51941131 -0.00688847 -0.00651887 -0.00202195], y on the last accepted step: [-2.00137497  1.7764533   0.51941131 -0.00688847 -0.00651887 -0.00202195]\n",
      "Loss on this step: 16.548062851891693, Loss on the last accepted step: 16.64579281615707, Step size: 1.0, y: [-1.9996816   1.77561838  0.51928441 -0.00699265 -0.0064699  -0.00201447], y on the last accepted step: [-1.9996816   1.77561838  0.51928441 -0.00699265 -0.0064699  -0.00201447]\n",
      "Loss on this step: 16.540977129124727, Loss on the last accepted step: 16.548062851891693, Step size: 1.0, y: [-2.00742049  1.77943937  0.51986893 -0.00656737 -0.00667292 -0.00204564], y on the last accepted step: [-2.00742049  1.77943937  0.51986893 -0.00656737 -0.00667292 -0.00204564]\n",
      "Loss on this step: 16.54089465193331, Loss on the last accepted step: 16.540977129124727, Step size: 1.0, y: [-2.00558462  1.77853286  0.51973021 -0.0066675  -0.00662509 -0.0020383 ], y on the last accepted step: [-2.00558462  1.77853286  0.51973021 -0.0066675  -0.00662509 -0.0020383 ]\n",
      "Loss on this step: 16.540893598080693, Loss on the last accepted step: 16.54089465193331, Step size: 1.0, y: [-2.00572347  1.77860143  0.51974071 -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572347  1.77860143  0.51974071 -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.5408935976883, Loss on the last accepted step: 16.540893598080693, Step size: 1.0, y: [-2.00572342  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572342  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597688108, Loss on the last accepted step: 16.5408935976883, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597688004, Loss on the last accepted step: 16.540893597688108, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687983, Loss on the last accepted step: 16.540893597688004, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687983, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.5, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.25, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.54089359768798, Step size: 0.0625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.54089359768798, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.5, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.25, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.0625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.03125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.015625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0078125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00390625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.001953125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0009765625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00048828125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.000244140625, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0001220703125, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 6.103515625e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.0517578125e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.52587890625e-05, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 7.62939453125e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.814697265625e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.9073486328125e-06, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 9.5367431640625e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 4.76837158203125e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.384185791015625e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.1920928955078125e-07, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 5.960464477539063e-08, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.9802322387695312e-08, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.0, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n"
     ]
    }
   ],
   "source": [
    "import optimistix\n",
    "\n",
    "\n",
    "# def func(x, args):\n",
    "#     c = CartesianState(x=x[:3][None,:], v=x[3:][None,:], time=times[0].tdb.jd)\n",
    "#     return -p0.loglike(c)\n",
    "def func(x, args):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "solver = optimistix.BFGS(\n",
    "    rtol=1e-12,\n",
    "    atol=1e-12,\n",
    "    verbose=frozenset({\"step_size\", \"loss\", \"y\"}),\n",
    "    use_inverse=False,\n",
    ")\n",
    "res = optimistix.minimise(\n",
    "    fn=func,\n",
    "    solver=solver,\n",
    "    y0=jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on this step: 22.573069057682215, Loss on the last accepted step: 0.0, Step size: 1.0, y: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616324795378832e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.5, y: [ -415943.93551124  -739899.38120539  -648959.20283939  -721214.60658886\n",
      " -1277773.22966227 -1121111.00126991], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061641537689634e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.25, y: [-207972.97061395 -369948.80129867 -324479.341546   -360607.30662438\n",
      " -638886.61814549 -560555.50165438], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0616596534629104e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.125, y: [-103987.48816531 -184973.51134531 -162239.41089931 -180303.65664214\n",
      " -319443.3123871  -280277.75184662], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.061695882978612e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0625, y: [ -51994.74694099  -92485.86636863  -81119.44557596  -90151.83165102\n",
      " -159721.65950791 -140138.87694274], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0617683345697946e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.03125, y: [-25998.37632882 -46242.04388029 -40559.46291429 -45075.91915547\n",
      " -79860.83306831 -70069.43949079], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0619132127784445e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.015625, y: [-13000.19102274 -23120.13263612 -20279.47158346 -22537.96290769\n",
      " -39930.41984851 -35034.72076482], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0622029036929414e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0078125, y: [ -6501.0983697  -11559.17701403 -10139.47591804 -11268.9847838\n",
      " -19965.21323861 -17517.36140184], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0627822059249875e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00390625, y: [-3251.55204318 -5778.69920299 -5069.47808533 -5634.49572185\n",
      " -9982.60993366 -8758.68172035], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0639392854639366e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.001953125, y: [-1626.77887992 -2888.46029747 -2534.47916897 -2817.25119088\n",
      " -4991.30828119 -4379.3418796 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0662545377147533e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0009765625, y: [ -814.39229829 -1443.34084471 -1266.9797108  -1408.62892539\n",
      " -2495.65745495 -2189.67195923], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.0707941370813786e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.00048828125, y: [ -408.19900748  -720.78111833  -633.22998171  -704.31779265\n",
      " -1247.83204183 -1094.83699904], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 2.066223626450892e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.000244140625, y: [-205.10236207 -359.50125514 -316.35511716 -352.16222628 -623.91933527\n",
      " -547.41951895], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1.741263176329129e+17, Loss on the last accepted step: 22.573069057682215, Step size: 0.0001220703125, y: [-103.55403937 -178.86132354 -157.91768489 -176.08444309 -311.96298199\n",
      " -273.7107789 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1210058266610485.2, Loss on the last accepted step: 22.573069057682215, Step size: 6.103515625e-05, y: [ -52.77987802  -88.54135775  -78.69896875  -88.0455515  -155.98480535\n",
      " -136.85640888], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1557258003248586.5, Loss on the last accepted step: 22.573069057682215, Step size: 3.0517578125e-05, y: [-27.39279734 -43.38137485 -39.08961069 -44.0261057  -77.99571703\n",
      " -68.42922386], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 802058121937971.2, Loss on the last accepted step: 22.573069057682215, Step size: 1.52587890625e-05, y: [-14.699257   -20.8013834  -19.28493165 -22.0163828  -39.00117287\n",
      " -34.21563136], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 248592818236005.0, Loss on the last accepted step: 22.573069057682215, Step size: 7.62939453125e-06, y: [ -8.35248683  -9.51138767  -9.38259214 -11.01152135 -19.50390079\n",
      " -17.10883511], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 69273677478740.26, Loss on the last accepted step: 22.573069057682215, Step size: 3.814697265625e-06, y: [-5.17910175 -3.86638981 -4.43142238 -5.50909063 -9.75526475 -8.55543698], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18329591762755.668, Loss on the last accepted step: 22.573069057682215, Step size: 1.9073486328125e-06, y: [-3.59240921 -1.04389088 -1.9558375  -2.75787527 -4.88094673 -4.27873792], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4717159011385.629, Loss on the last accepted step: 22.573069057682215, Step size: 9.5367431640625e-07, y: [-2.79906293  0.36735859 -0.71804506 -1.38226759 -2.44378772 -2.14038838], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1196347948988.4683, Loss on the last accepted step: 22.573069057682215, Step size: 4.76837158203125e-07, y: [-2.4023898   1.07298332 -0.09914884 -0.69446375 -1.22520822 -1.07121362], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 301099069639.34674, Loss on the last accepted step: 22.573069057682215, Step size: 2.384185791015625e-07, y: [-2.20405323  1.42579568  0.21029927 -0.35056183 -0.61591847 -0.53662624], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 75478377806.22424, Loss on the last accepted step: 22.573069057682215, Step size: 1.1920928955078125e-07, y: [-2.10488495  1.60220187  0.36502333 -0.17861087 -0.31127359 -0.26933254], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18884195537.268745, Loss on the last accepted step: 22.573069057682215, Step size: 5.960464477539063e-08, y: [-2.05530081  1.69040496  0.44238535 -0.09263539 -0.15895115 -0.1356857 ], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4721389919.197352, Loss on the last accepted step: 22.573069057682215, Step size: 2.9802322387695312e-08, y: [-2.03050873  1.73450651  0.48106637 -0.04964765 -0.08278993 -0.06886228], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1180222493.5438774, Loss on the last accepted step: 22.573069057682215, Step size: 1.4901161193847656e-08, y: [-2.0181127   1.75655728  0.50040687 -0.02815378 -0.04470932 -0.03545056], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 295006572.2998562, Loss on the last accepted step: 22.573069057682215, Step size: 7.450580596923828e-09, y: [-2.01191468  1.76758266  0.51007713 -0.01740684 -0.02566902 -0.01874471], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 73732297.54740189, Loss on the last accepted step: 22.573069057682215, Step size: 3.725290298461914e-09, y: [-2.00881567  1.77309536  0.51491225 -0.01203337 -0.01614887 -0.01039178], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18424280.514657617, Loss on the last accepted step: 22.573069057682215, Step size: 1.862645149230957e-09, y: [-2.00726617  1.7758517   0.51732982 -0.00934664 -0.01138879 -0.00621532], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4601804.604893329, Loss on the last accepted step: 22.573069057682215, Step size: 9.313225746154785e-10, y: [-2.00649142  1.77722988  0.5185386  -0.00800327 -0.00900875 -0.00412709], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1148343.1054827906, Loss on the last accepted step: 22.573069057682215, Step size: 4.656612873077393e-10, y: [-2.00610404  1.77791896  0.51914299 -0.00733159 -0.00781873 -0.00308297], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 286042.3052457798, Loss on the last accepted step: 22.573069057682215, Step size: 2.3283064365386963e-10, y: [-2.00591035  1.77826351  0.51944519 -0.00699575 -0.00722372 -0.00256091], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 70997.56991652744, Loss on the last accepted step: 22.573069057682215, Step size: 1.1641532182693481e-10, y: [-2.00581351  1.77843578  0.51959628 -0.00682783 -0.00692622 -0.00229988], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 17501.387473158673, Loss on the last accepted step: 22.573069057682215, Step size: 5.820766091346741e-11, y: [-2.00576509  1.77852192  0.51967183 -0.00674387 -0.00677747 -0.00216937], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 4259.81340728817, Loss on the last accepted step: 22.573069057682215, Step size: 2.9103830456733704e-11, y: [-2.00574087  1.77856498  0.51970961 -0.00670189 -0.00670309 -0.00210411], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 1015.6520619330806, Loss on the last accepted step: 22.573069057682215, Step size: 1.4551915228366852e-11, y: [-2.00572877  1.77858652  0.51972849 -0.0066809  -0.0066659  -0.00207148], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 237.72732879440176, Loss on the last accepted step: 22.573069057682215, Step size: 7.275957614183426e-12, y: [-2.00572272  1.77859728  0.51973794 -0.0066704  -0.00664731 -0.00205517], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 59.80389829475062, Loss on the last accepted step: 22.573069057682215, Step size: 3.637978807091713e-12, y: [-2.00571969  1.77860267  0.51974266 -0.00666515 -0.00663801 -0.00204701], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 23.60190956181135, Loss on the last accepted step: 22.573069057682215, Step size: 1.8189894035458565e-12, y: [-2.00571818  1.77860536  0.51974502 -0.00666253 -0.00663336 -0.00204293], y on the last accepted step: [-2.00571666  1.77860805  0.51974738 -0.00665991 -0.00662871 -0.00203885]\n",
      "Loss on this step: 18.69084593908376, Loss on the last accepted step: 22.573069057682215, Step size: 1.0, y: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656163577790677.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.5, y: [-164090.84447962 -293279.46017563 -257137.284282     55795.69437065\n",
      "   97524.25599093   85838.89068364], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656219432660415.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.25, y: [ -82046.42509852 -146638.84078446 -128568.3822679    27897.84385472\n",
      "   48762.12467995   42919.44432137], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656335079169682.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.125, y: [-41024.21540797 -73318.53108888 -64283.93126085  13948.91859675\n",
      "  24381.05902445  21459.72114024], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6656597900918513.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0625, y: [-20513.1105627  -36658.37624109 -32141.70575732   6974.45596777\n",
      "  12190.52619671  10729.85954967], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6657376519418923.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.03125, y: [-10257.55814006 -18328.29881719 -16070.59300556   3487.22465327\n",
      "   6095.25978283   5364.92875439], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6660973268893504.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.015625, y: [-5129.78192874 -9163.26010524 -8035.03662968  1743.60899603\n",
      "  3047.6265759   2682.46335675], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6684888181703251.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.0078125, y: [-2565.89382308 -4580.74074927 -4017.25844174   871.80116741\n",
      "  1523.80997243  1341.23065793], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 6881824413960419.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00390625, y: [-1283.94977025 -2289.48107128 -2008.36934777   435.89725309\n",
      "   761.9016707    670.61430852], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 9977300397153084.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.001953125, y: [ -642.97774384 -1143.85123229 -1003.92480078   217.94529594\n",
      "   380.94751983   335.30613381], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3545060847167142.5, Loss on the last accepted step: 18.69084593908376, Step size: 0.0009765625, y: [-322.49173063 -571.03631279 -501.70252729  108.96931736  190.47044439\n",
      "  167.65204646], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4102477456645055.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.00048828125, y: [-162.24872402 -284.62885304 -250.59139055   54.48132807   95.23190668\n",
      "   83.82500278], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 5559598981456660.0, Loss on the last accepted step: 18.69084593908376, Step size: 0.000244140625, y: [ -82.12722072 -141.42512317 -125.03582217   27.23733343   47.61263782\n",
      "   41.91148095], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 412895572872776.2, Loss on the last accepted step: 18.69084593908376, Step size: 0.0001220703125, y: [-42.06646907 -69.82325823 -62.25803799  13.6153361   23.80300339\n",
      "  20.95472003], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 81851111819179.84, Loss on the last accepted step: 18.69084593908376, Step size: 6.103515625e-05, y: [-22.03609325 -34.02232576 -30.86914589   6.80433744  11.89818618\n",
      "  10.47633957], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 18050209225659.055, Loss on the last accepted step: 18.69084593908376, Step size: 3.0517578125e-05, y: [-12.02090533 -16.12185953 -15.17469985   3.39883811   5.94577757\n",
      "   5.23714934], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 4228174375397.174, Loss on the last accepted step: 18.69084593908376, Step size: 1.52587890625e-05, y: [-7.01331138 -7.17162641 -7.32747682  1.69608845  2.96957326  2.61755422], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 1022966504134.654, Loss on the last accepted step: 18.69084593908376, Step size: 7.62939453125e-06, y: [-4.5095144  -2.69650985 -3.40386531  0.84471361  1.48147111  1.30775666], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 251728138122.1346, Loss on the last accepted step: 18.69084593908376, Step size: 3.814697265625e-06, y: [-3.25761591 -0.45895157 -1.44205955  0.4190262   0.73742004  0.65285789], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 62491708224.42246, Loss on the last accepted step: 18.69084593908376, Step size: 1.9073486328125e-06, y: [-2.63166666  0.65982757 -0.46115668  0.20618249  0.3653945   0.3254085 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15578336950.830612, Loss on the last accepted step: 18.69084593908376, Step size: 9.5367431640625e-07, y: [-2.31869204  1.21921714  0.02929476  0.09976064  0.17938173  0.1616838 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3890045900.1679, Loss on the last accepted step: 18.69084593908376, Step size: 4.76837158203125e-07, y: [-2.16220473  1.49891192  0.27452048  0.04654971  0.08637535  0.07982145], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 971993276.5072057, Loss on the last accepted step: 18.69084593908376, Step size: 2.384185791015625e-07, y: [-2.08396108  1.63875931  0.39713334  0.01994425  0.03987215  0.03889028], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 242921974.5233047, Loss on the last accepted step: 18.69084593908376, Step size: 1.1920928955078125e-07, y: [-2.04483925  1.70868301  0.45843977  0.00664151  0.01662056  0.01842469], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 60712771.75720936, Loss on the last accepted step: 18.69084593908376, Step size: 5.960464477539063e-08, y: [-2.02527833e+00  1.74364486e+00  4.89092986e-01 -9.85187798e-06\n",
      "  4.99475933e-03  8.19190096e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 15171741.93509069, Loss on the last accepted step: 18.69084593908376, Step size: 2.9802322387695312e-08, y: [-2.01549788e+00  1.76112578e+00  5.04419593e-01 -3.33553482e-03\n",
      " -8.18139701e-04  3.07550425e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3790007.78187058, Loss on the last accepted step: 18.69084593908376, Step size: 1.4901161193847656e-08, y: [-2.01060765e+00  1.76986624e+00  5.12082897e-01 -4.99837629e-03\n",
      " -3.72458922e-03  5.17305892e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 946080.8469193219, Loss on the last accepted step: 18.69084593908376, Step size: 7.450580596923828e-09, y: [-2.00816253e+00  1.77423647e+00  5.15914549e-01 -5.82979702e-03\n",
      " -5.17781397e-03 -7.61793285e-04], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 235821.10599754317, Loss on the last accepted step: 18.69084593908376, Step size: 3.725290298461914e-09, y: [-2.00693998e+00  1.77642159e+00  5.17830375e-01 -6.24550739e-03\n",
      " -5.90442635e-03 -1.40134287e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 58613.28431119843, Loss on the last accepted step: 18.69084593908376, Step size: 1.862645149230957e-09, y: [-2.00632870e+00  1.77751415e+00  5.18788288e-01 -6.45336257e-03\n",
      " -6.26773254e-03 -1.72111767e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 14489.40284052151, Loss on the last accepted step: 18.69084593908376, Step size: 9.313225746154785e-10, y: [-2.00602306e+00  1.77806043e+00  5.19267244e-01 -6.55729017e-03\n",
      " -6.44938564e-03 -1.88100507e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 3547.4092573621597, Loss on the last accepted step: 18.69084593908376, Step size: 4.656612873077393e-10, y: [-2.00587024e+00  1.77833357e+00  5.19506723e-01 -6.60925396e-03\n",
      " -6.54021218e-03 -1.96094876e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 856.3917254968645, Loss on the last accepted step: 18.69084593908376, Step size: 2.3283064365386963e-10, y: [-2.00579383e+00  1.77847014e+00  5.19626462e-01 -6.63523586e-03\n",
      " -6.58562546e-03 -2.00092061e-03], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 205.8768389403899, Loss on the last accepted step: 18.69084593908376, Step size: 1.1641532182693481e-10, y: [-2.00575563  1.77853842  0.51968633 -0.00664823 -0.00660833 -0.00202091], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 54.367747074627246, Loss on the last accepted step: 18.69084593908376, Step size: 5.820766091346741e-11, y: [-2.00573652  1.77857256  0.51971627 -0.00665472 -0.00661969 -0.0020309 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 22.050274190831995, Loss on the last accepted step: 18.69084593908376, Step size: 2.9103830456733704e-11, y: [-2.00572697  1.77858963  0.51973123 -0.00665797 -0.00662536 -0.0020359 ], y on the last accepted step: [-2.00571742  1.77860671  0.5197462  -0.00666122 -0.00663104 -0.00204089]\n",
      "Loss on this step: 16.75080474090859, Loss on the last accepted step: 18.69084593908376, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 169644603169826.8, Loss on the last accepted step: 16.75080474090859, Step size: 0.5, y: [ -9.05464439 -13.81993504  22.79009627 -21.1900384  -51.30125272\n",
      "  72.09817681], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 118164505879627.69, Loss on the last accepted step: 16.75080474090859, Step size: 0.25, y: [ -5.53018329  -6.02066844  11.65491749 -10.598349   -25.65394046\n",
      "  36.04806921], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 59610682135552.16, Loss on the last accepted step: 16.75080474090859, Step size: 0.125, y: [ -3.76795274  -2.12103513   6.08732811  -5.3025043  -12.83028433\n",
      "  18.02301541], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 23706295099936.734, Loss on the last accepted step: 16.75080474090859, Step size: 0.0625, y: [-2.88683747 -0.17121848  3.30353341 -2.65458194 -6.41845627  9.01048851], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 7839821118969.897, Loss on the last accepted step: 16.75080474090859, Step size: 0.03125, y: [-2.44627983  0.80368984  1.91163606 -1.33062077 -3.21254223  4.50422506], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2291126953191.8936, Loss on the last accepted step: 16.75080474090859, Step size: 0.015625, y: [-2.22600101  1.29114401  1.21568739 -0.66864018 -1.60958522  2.25109333], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 622104027015.105, Loss on the last accepted step: 16.75080474090859, Step size: 0.0078125, y: [-2.11586161  1.53487109  0.86771305 -0.33764989 -0.80810671  1.12452747], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 162209252307.20477, Loss on the last accepted step: 16.75080474090859, Step size: 0.00390625, y: [-2.0607919   1.65673463  0.69372589 -0.17215474 -0.40736745  0.56124454], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 41401575908.518974, Loss on the last accepted step: 16.75080474090859, Step size: 0.001953125, y: [-2.03325705  1.7176664   0.6067323  -0.08940717 -0.20699783  0.27960307], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10454068085.194855, Loss on the last accepted step: 16.75080474090859, Step size: 0.0009765625, y: [-2.01948962  1.74813228  0.56323551 -0.04803338 -0.10681301  0.13878234], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2626075854.0582895, Loss on the last accepted step: 16.75080474090859, Step size: 0.00048828125, y: [-2.01260591  1.76336523  0.54148711 -0.02734649 -0.05672061  0.06837197], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 658055204.7769172, Loss on the last accepted step: 16.75080474090859, Step size: 0.000244140625, y: [-2.00916405  1.7709817   0.53061292 -0.01700304 -0.0316744   0.03316679], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 164703543.9706092, Loss on the last accepted step: 16.75080474090859, Step size: 0.0001220703125, y: [-2.00744312  1.77478993  0.52517582 -0.01183132 -0.0191513   0.0155642 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 41199466.63779612, Loss on the last accepted step: 16.75080474090859, Step size: 6.103515625e-05, y: [-2.00658266  1.77669405  0.52245727 -0.00924546 -0.01288975  0.0067629 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10302815.7094152, Loss on the last accepted step: 16.75080474090859, Step size: 3.0517578125e-05, y: [-2.00615243  1.77764611  0.52109799 -0.00795252 -0.00975898  0.00236225], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2576082.8629744216, Loss on the last accepted step: 16.75080474090859, Step size: 1.52587890625e-05, y: [-2.00593731e+00  1.77812214e+00  5.20418355e-01 -7.30605931e-03\n",
      " -8.19358811e-03  1.61929728e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 644078.9871490183, Loss on the last accepted step: 16.75080474090859, Step size: 7.62939453125e-06, y: [-2.00582975e+00  1.77836016e+00  5.20078536e-01 -6.98282660e-03\n",
      " -7.41089426e-03 -9.38232247e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 161037.99518129436, Loss on the last accepted step: 16.75080474090859, Step size: 3.814697265625e-06, y: [-2.00577597e+00  1.77847916e+00  5.19908627e-01 -6.82121024e-03\n",
      " -7.01954733e-03 -1.48831323e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 40272.75941962693, Loss on the last accepted step: 16.75080474090859, Step size: 1.9073486328125e-06, y: [-2.00574909e+00  1.77853867e+00  5.19823672e-01 -6.74040207e-03\n",
      " -6.82387387e-03 -1.76335373e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 10080.833671238892, Loss on the last accepted step: 16.75080474090859, Step size: 9.5367431640625e-07, y: [-2.00573564e+00  1.77856842e+00  5.19781195e-01 -6.69999798e-03\n",
      " -6.72603713e-03 -1.90087398e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2532.778357383092, Loss on the last accepted step: 16.75080474090859, Step size: 4.76837158203125e-07, y: [-2.00572892e+00  1.77858329e+00  5.19759956e-01 -6.67979593e-03\n",
      " -6.67711877e-03 -1.96963410e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 645.7569269823059, Loss on the last accepted step: 16.75080474090859, Step size: 2.384185791015625e-07, y: [-2.00572556e+00  1.77859073e+00  5.19749337e-01 -6.66969491e-03\n",
      " -6.65265958e-03 -2.00401416e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 174.00142949487125, Loss on the last accepted step: 16.75080474090859, Step size: 1.1920928955078125e-07, y: [-2.00572388  1.77859445  0.51974403 -0.00666464 -0.00664043 -0.0020212 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 56.062942112056135, Loss on the last accepted step: 16.75080474090859, Step size: 5.960464477539063e-08, y: [-2.00572304  1.77859631  0.51974137 -0.00666212 -0.00663432 -0.0020298 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26.57857151241832, Loss on the last accepted step: 16.75080474090859, Step size: 2.9802322387695312e-08, y: [-2.00572262  1.77859724  0.51974004 -0.00666086 -0.00663126 -0.0020341 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.20761179275799, Loss on the last accepted step: 16.75080474090859, Step size: 1.4901161193847656e-08, y: [-2.00572241  1.7785977   0.51973938 -0.00666023 -0.00662973 -0.00203625], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.364938889191162, Loss on the last accepted step: 16.75080474090859, Step size: 7.450580596923828e-09, y: [-2.0057223   1.77859794  0.51973905 -0.00665991 -0.00662896 -0.00203732], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.90430463201307, Loss on the last accepted step: 16.75080474090859, Step size: 3.725290298461914e-09, y: [-2.00572225  1.77859805  0.51973888 -0.00665975 -0.00662858 -0.00203786], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.789162834089133, Loss on the last accepted step: 16.75080474090859, Step size: 1.862645149230957e-09, y: [-2.00572222  1.77859811  0.5197388  -0.00665967 -0.00662839 -0.00203813], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.76038583457887, Loss on the last accepted step: 16.75080474090859, Step size: 9.313225746154785e-10, y: [-2.00572221  1.77859814  0.51973876 -0.00665963 -0.0066283  -0.00203826], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.753195796672802, Loss on the last accepted step: 16.75080474090859, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859816  0.51973874 -0.00665961 -0.00662825 -0.00203833], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751400396022706, Loss on the last accepted step: 16.75080474090859, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859816  0.51973873 -0.0066596  -0.00662822 -0.00203836], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750952600210827, Loss on the last accepted step: 16.75080474090859, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75084117851343, Loss on the last accepted step: 16.75080474090859, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.00662821 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750813586596493, Loss on the last accepted step: 16.75080474090859, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750806820486787, Loss on the last accepted step: 16.75080474090859, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750805194843167, Loss on the last accepted step: 16.75080474090859, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804821409293, Loss on the last accepted step: 16.75080474090859, Step size: 3.637978807091713e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804744509423, Loss on the last accepted step: 16.75080474090859, Step size: 1.8189894035458565e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473354687, Loss on the last accepted step: 16.75080474090859, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 3922990983.8954067, Loss on the last accepted step: 16.75080473354687, Step size: 0.5, y: [-2.6501976   2.53635291  0.06497501 -0.94988438  0.32484086  0.22920446], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 1445432804.8535957, Loss on the last accepted step: 16.75080473354687, Step size: 0.25, y: [-2.3279599   2.15747554  0.29235687 -0.47827199  0.15910633  0.11358303], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 459569595.31041497, Loss on the last accepted step: 16.75080473354687, Step size: 0.125, y: [-2.16684105  1.96803685  0.40604779 -0.24246579  0.07623906  0.05577232], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 132130088.26122552, Loss on the last accepted step: 16.75080473354687, Step size: 0.0625, y: [-2.08628162  1.87331751  0.46289325 -0.12456269  0.03480543  0.02686696], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 35684556.50337921, Loss on the last accepted step: 16.75080473354687, Step size: 0.03125, y: [-2.04600191  1.82595784  0.49131599 -0.06561114  0.01408862  0.01241428], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9294833.672423251, Loss on the last accepted step: 16.75080473354687, Step size: 0.015625, y: [-2.02586205  1.80227801  0.50552735 -0.03613537  0.00373021  0.00518795], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2373592.5596803604, Loss on the last accepted step: 16.75080473354687, Step size: 0.0078125, y: [-2.01579212e+00  1.79043809e+00  5.12633034e-01 -2.13974812e-02\n",
      " -1.44899648e-03  1.57477548e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 599863.1821710416, Loss on the last accepted step: 16.75080473354687, Step size: 0.00390625, y: [-2.01075716e+00  1.78451813e+00  5.16185876e-01 -1.40285376e-02\n",
      " -4.03859849e-03 -2.31809306e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 150799.3217221546, Loss on the last accepted step: 16.75080473354687, Step size: 0.001953125, y: [-2.00823968e+00  1.78155815e+00  5.17962297e-01 -1.03440658e-02\n",
      " -5.33339949e-03 -1.13510170e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 37815.93761301712, Loss on the last accepted step: 16.75080473354687, Step size: 0.0009765625, y: [-2.00698094e+00  1.78007816e+00  5.18850507e-01 -8.50182984e-03\n",
      " -5.98079999e-03 -1.58674789e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9479.54819185883, Loss on the last accepted step: 16.75080473354687, Step size: 0.00048828125, y: [-2.00635157e+00  1.77933816e+00  5.19294612e-01 -7.58071188e-03\n",
      " -6.30450024e-03 -1.81257099e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 2384.078443023442, Loss on the last accepted step: 16.75080473354687, Step size: 0.000244140625, y: [-2.00603688e+00  1.77896817e+00  5.19516665e-01 -7.12015291e-03\n",
      " -6.46635037e-03 -1.92548254e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 608.7862317101427, Loss on the last accepted step: 16.75080473354687, Step size: 0.0001220703125, y: [-2.00587954e+00  1.77878317e+00  5.19627691e-01 -6.88987342e-03\n",
      " -6.54727543e-03 -1.98193832e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 164.78499399043434, Loss on the last accepted step: 16.75080473354687, Step size: 6.103515625e-05, y: [-2.00580087  1.77869067  0.5196832  -0.00677473 -0.00658774 -0.00201017], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 53.7624579919134, Loss on the last accepted step: 16.75080473354687, Step size: 3.0517578125e-05, y: [-2.00576153  1.77864442  0.51971096 -0.00671716 -0.00660797 -0.00202428], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26.004075326244468, Loss on the last accepted step: 16.75080473354687, Step size: 1.52587890625e-05, y: [-2.00574186  1.77862129  0.51972484 -0.00668838 -0.00661808 -0.00203134], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 19.064151777016612, Loss on the last accepted step: 16.75080473354687, Step size: 7.62939453125e-06, y: [-2.00573203  1.77860973  0.51973178 -0.00667399 -0.00662314 -0.00203487], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.32913748775177, Loss on the last accepted step: 16.75080473354687, Step size: 3.814697265625e-06, y: [-2.00572711  1.77860395  0.51973525 -0.00666679 -0.00662567 -0.00203663], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.895383545115955, Loss on the last accepted step: 16.75080473354687, Step size: 1.9073486328125e-06, y: [-2.00572465  1.77860106  0.51973698 -0.00666319 -0.00662694 -0.00203751], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.786947073359862, Loss on the last accepted step: 16.75080473354687, Step size: 9.5367431640625e-07, y: [-2.00572343  1.77859962  0.51973785 -0.00666139 -0.00662757 -0.00203795], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75983917349073, Loss on the last accepted step: 16.75080473354687, Step size: 4.76837158203125e-07, y: [-2.00572281  1.77859889  0.51973828 -0.00666049 -0.00662788 -0.00203817], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.753062803541717, Loss on the last accepted step: 16.75080473354687, Step size: 2.384185791015625e-07, y: [-2.0057225   1.77859853  0.5197385  -0.00666004 -0.00662804 -0.00203828], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751369052565106, Loss on the last accepted step: 16.75080473354687, Step size: 1.1920928955078125e-07, y: [-2.00572235  1.77859835  0.51973861 -0.00665982 -0.00662812 -0.00203834], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75094569434059, Loss on the last accepted step: 16.75080473354687, Step size: 5.960464477539063e-08, y: [-2.00572227  1.77859826  0.51973866 -0.00665971 -0.00662816 -0.00203837], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750840046601585, Loss on the last accepted step: 16.75080473354687, Step size: 2.9802322387695312e-08, y: [-2.00572223  1.77859821  0.51973869 -0.00665965 -0.00662818 -0.00203838], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750813522324403, Loss on the last accepted step: 16.75080473354687, Step size: 1.4901161193847656e-08, y: [-2.00572222  1.77859819  0.5197387  -0.00665962 -0.00662819 -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750806876778192, Loss on the last accepted step: 16.75080473354687, Step size: 7.450580596923828e-09, y: [-2.00572221  1.77859818  0.51973871 -0.00665961 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080534286355, Loss on the last accepted step: 16.75080473354687, Step size: 3.725290298461914e-09, y: [-2.0057222   1.77859818  0.51973871 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804902737123, Loss on the last accepted step: 16.75080473354687, Step size: 1.862645149230957e-09, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080478121115, Loss on the last accepted step: 16.75080473354687, Step size: 9.313225746154785e-10, y: [-2.0057222   1.77859817  0.51973872 -0.0066596  -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804748710618, Loss on the last accepted step: 16.75080473354687, Step size: 4.656612873077393e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804738944353, Loss on the last accepted step: 16.75080473354687, Step size: 2.3283064365386963e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473570971, Loss on the last accepted step: 16.75080473354687, Step size: 1.1641532182693481e-10, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804734426474, Loss on the last accepted step: 16.75080473354687, Step size: 5.820766091346741e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804733889535, Loss on the last accepted step: 16.75080473354687, Step size: 2.9103830456733704e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473373192, Loss on the last accepted step: 16.75080473354687, Step size: 1.4551915228366852e-11, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750804733601424, Loss on the last accepted step: 16.75080473354687, Step size: 7.275957614183426e-12, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75080473353612, Loss on the last accepted step: 16.75080473354687, Step size: 1.0, y: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 161642.6237495841, Loss on the last accepted step: 16.75080473353612, Step size: 0.5, y: [-7.46735314  4.46343793  0.93068091 -6.22053221  3.0986233   0.47273257], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 103163.30612481372, Loss on the last accepted step: 16.75080473353612, Step size: 0.25, y: [-4.73653767  3.12101805  0.72520981 -3.1135959   1.54599755  0.23534709], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 57050.3179006556, Loss on the last accepted step: 16.75080473353612, Step size: 0.125, y: [-3.37112993  2.44980811  0.62247427 -1.56012775  0.76968467  0.11665435], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 26155.623737700727, Loss on the last accepted step: 16.75080473353612, Step size: 0.0625, y: [-2.68842606  2.11420314  0.57110649 -0.78339367  0.38152824  0.05730798], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 9935.890076840564, Loss on the last accepted step: 16.75080473353612, Step size: 0.03125, y: [-2.34707413  1.94640065  0.5454226  -0.39502663  0.18745002  0.02763479], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 3233.2974048432666, Loss on the last accepted step: 16.75080473353612, Step size: 0.015625, y: [-2.17639816  1.86249941  0.53258066 -0.20084311  0.09041091  0.0127982 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 950.2824371766724, Loss on the last accepted step: 16.75080473353612, Step size: 0.0078125, y: [-2.09106018  1.82054879  0.52615969 -0.10375135  0.04189135  0.0053799 ], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 269.7200085662807, Loss on the last accepted step: 16.75080473353612, Step size: 0.00390625, y: [-2.04839119e+00  1.79957348e+00  5.22949203e-01 -5.52054738e-02\n",
      "  1.76315768e-02  1.67075410e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 82.68537503572642, Loss on the last accepted step: 16.75080473353612, Step size: 0.001953125, y: [-2.02705669e+00  1.78908583e+00  5.21343960e-01 -3.09325338e-02\n",
      "  5.50168817e-03 -1.83819993e-04], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 33.57653878709546, Loss on the last accepted step: 16.75080473353612, Step size: 0.0009765625, y: [-2.01638944e+00  1.78384200e+00  5.20541339e-01 -1.87960639e-02\n",
      " -5.63256159e-04 -1.11110704e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 20.994485466267555, Loss on the last accepted step: 16.75080473353612, Step size: 0.00048828125, y: [-2.01105582e+00  1.78122008e+00  5.20140028e-01 -1.27278289e-02\n",
      " -3.59572833e-03 -1.57475057e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.81306965053772, Loss on the last accepted step: 16.75080473353612, Step size: 0.000244140625, y: [-2.00838901e+00  1.77990913e+00  5.19939373e-01 -9.69371142e-03\n",
      " -5.11196441e-03 -1.80657233e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 17.014847160561676, Loss on the last accepted step: 16.75080473353612, Step size: 0.0001220703125, y: [-2.00705560e+00  1.77925365e+00  5.19839045e-01 -8.17665268e-03\n",
      " -5.87008245e-03 -1.92248321e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.8157768381781, Loss on the last accepted step: 16.75080473353612, Step size: 6.103515625e-05, y: [-2.00638890e+00  1.77892591e+00  5.19788881e-01 -7.41812331e-03\n",
      " -6.24914147e-03 -1.98043865e-03], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.766493885795505, Loss on the last accepted step: 16.75080473353612, Step size: 3.0517578125e-05, y: [-2.00605555  1.77876204  0.5197638  -0.00703886 -0.00643867 -0.00200942], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.754445648520665, Loss on the last accepted step: 16.75080473353612, Step size: 1.52587890625e-05, y: [-2.00588887  1.7786801   0.51975126 -0.00684923 -0.00653344 -0.00202391], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.751573784889075, Loss on the last accepted step: 16.75080473353612, Step size: 7.62939453125e-06, y: [-2.00580553  1.77863914  0.51974499 -0.00675441 -0.00658082 -0.00203115], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.750926239653406, Loss on the last accepted step: 16.75080473353612, Step size: 3.814697265625e-06, y: [-2.00576386  1.77861865  0.51974185 -0.006707   -0.00660451 -0.00203477], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75079990814405, Loss on the last accepted step: 16.75080473353612, Step size: 1.9073486328125e-06, y: [-2.00574303  1.77860841  0.51974028 -0.0066833  -0.00661635 -0.00203658], y on the last accepted step: [-2.0057222   1.77859817  0.51973872 -0.00665959 -0.0066282  -0.00203839]\n",
      "Loss on this step: 16.75078588270273, Loss on the last accepted step: 16.75080473353612, Step size: 1.0, y: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 107.80584232938871, Loss on the last accepted step: 16.75078588270273, Step size: 0.5, y: [-3.78810545  2.65871305  0.65442978  0.09113483 -0.05335951 -0.00921925], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 38.35965783947903, Loss on the last accepted step: 16.75078588270273, Step size: 0.25, y: [-2.89691903  2.21865817  0.58708464  0.04223169 -0.02999089 -0.00562837], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 21.467829625356504, Loss on the last accepted step: 16.75078588270273, Step size: 0.125, y: [-2.45132582  1.99863073  0.55341207  0.01778012 -0.01830659 -0.00383293], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 17.774846328932956, Loss on the last accepted step: 16.75078588270273, Step size: 0.0625, y: [-2.22852922  1.88861701  0.53657579  0.00555434 -0.01246443 -0.00293521], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.92940743062247, Loss on the last accepted step: 16.75078588270273, Step size: 0.03125, y: [-2.11713092e+00  1.83361015e+00  5.28157644e-01 -5.58553654e-04\n",
      " -9.54335464e-03 -2.48634845e-03], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.748791298705314, Loss on the last accepted step: 16.75078588270273, Step size: 0.015625, y: [-2.06143176  1.80610672  0.52394857 -0.003615   -0.00808282 -0.00226192], y on the last accepted step: [-2.00573261  1.77860329  0.5197395  -0.00667145 -0.00662228 -0.00203749]\n",
      "Loss on this step: 16.724982128951844, Loss on the last accepted step: 16.75078588270273, Step size: 1.0, y: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ], y on the last accepted step: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ]\n",
      "Loss on this step: 17.146241008157222, Loss on the last accepted step: 16.724982128951844, Step size: 0.5, y: [-1.94698309e+00  1.74959975e+00  5.15304570e-01 -9.84998454e-03\n",
      " -5.10605390e-03 -1.80450435e-03], y on the last accepted step: [-2.03358219  1.79235501  0.52184404 -0.00514322 -0.00735255 -0.0021497 ]\n",
      "Loss on this step: 16.605234340886252, Loss on the last accepted step: 16.724982128951844, Step size: 1.0, y: [-1.99028264e+00  1.77097738e+00  5.18574303e-01 -7.49660371e-03\n",
      " -6.22930042e-03 -1.97710393e-03], y on the last accepted step: [-1.99028264e+00  1.77097738e+00  5.18574303e-01 -7.49660371e-03\n",
      " -6.22930042e-03 -1.97710393e-03]\n",
      "Loss on this step: 16.542555673968565, Loss on the last accepted step: 16.605234340886252, Step size: 1.0, y: [-2.00576117  1.77862013  0.51974359 -0.00665737 -0.00662993 -0.00203912], y on the last accepted step: [-2.00576117  1.77862013  0.51974359 -0.00665737 -0.00662993 -0.00203912]\n",
      "Loss on this step: 16.540904042046847, Loss on the last accepted step: 16.542555673968565, Step size: 1.0, y: [-2.0061081   1.77879133  0.51976976 -0.00663889 -0.00663875 -0.00204039], y on the last accepted step: [-2.0061081   1.77879133  0.51976976 -0.00663889 -0.00663875 -0.00204039]\n",
      "Loss on this step: 16.540893651014116, Loss on the last accepted step: 16.540904042046847, Step size: 1.0, y: [-2.00569725  1.77858848  0.51973873 -0.00666135 -0.00662802 -0.00203875], y on the last accepted step: [-2.00569725  1.77858848  0.51973873 -0.00666135 -0.00662802 -0.00203875]\n",
      "Loss on this step: 16.540893597693966, Loss on the last accepted step: 16.540893651014116, Step size: 1.0, y: [-2.00572371  1.77860155  0.51974072 -0.00665989 -0.00662872 -0.00203885], y on the last accepted step: [-2.00572371  1.77860155  0.51974072 -0.00665989 -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768823, Loss on the last accepted step: 16.540893597693966, Step size: 1.0, y: [-2.00572341  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572341  1.7786014   0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768805, Loss on the last accepted step: 16.54089359768823, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687987, Loss on the last accepted step: 16.54089359768805, Step size: 1.0, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687987, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.54089359768798, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.5, y: [-2.00572338  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.54089359768798, Loss on the last accepted step: 16.540893597687976, Step size: 0.25, y: [-2.00572339  1.77860138  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.03125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.015625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0078125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00390625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.001953125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0009765625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.00048828125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.000244140625, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 0.0001220703125, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 6.103515625e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.0517578125e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.52587890625e-05, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 7.62939453125e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 3.814697265625e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.9073486328125e-06, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 9.5367431640625e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 4.76837158203125e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.384185791015625e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.1920928955078125e-07, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 5.960464477539063e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 2.9802322387695312e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.4901161193847656e-08, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n",
      "Loss on this step: 16.540893597687976, Loss on the last accepted step: 16.540893597687976, Step size: 1.0, y: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885], y on the last accepted step: [-2.00572339  1.77860139  0.5197407  -0.0066599  -0.00662872 -0.00203885]\n"
     ]
    }
   ],
   "source": [
    "def func(x, args):\n",
    "    return -p0.loglike(x)\n",
    "\n",
    "\n",
    "solver = optimistix.BFGS(\n",
    "    rtol=1e-12,\n",
    "    atol=1e-12,\n",
    "    verbose=frozenset({\"step_size\", \"loss\", \"y\"}),\n",
    "    use_inverse=True,\n",
    ")\n",
    "res = optimistix.minimise(\n",
    "    fn=func,\n",
    "    solver=solver,\n",
    "    y0=jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 711.1x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from jorbit.astrometry.transformations import icrs_to_horizons_ecliptic\n",
    "\n",
    "Omegas = jnp.linspace(0, 2 * jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "incs = jnp.arccos(jnp.linspace(-1, 1, 11))[:-1] * 180 / jnp.pi\n",
    "# nus = jnp.linspace(0, 2 * jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "# nus = jnp.array([jnp.pi/4, 7*jnp.pi/4]) * 180 / jnp.pi\n",
    "# Omegas = jnp.array([0.0]) * 180 / jnp.pi\n",
    "incs = jnp.linspace(0, jnp.pi, 11)[:-1] * 180 / jnp.pi\n",
    "\n",
    "arrs = []\n",
    "for Omega in Omegas:\n",
    "    for inc in incs:\n",
    "        for _nu in range(10):\n",
    "            arrs.append(\n",
    "                jnp.array([5.0, 0.0, np.random.uniform(0, 360), inc, Omega, 0.0])\n",
    "            )\n",
    "arrs = jnp.array(arrs)\n",
    "\n",
    "k = KeplerianState(\n",
    "    semi=arrs[:, 0],\n",
    "    ecc=arrs[:, 1],\n",
    "    nu=arrs[:, 2],\n",
    "    inc=arrs[:, 3],\n",
    "    Omega=arrs[:, 4],\n",
    "    omega=arrs[:, 5],\n",
    "    time=times[0],\n",
    ")\n",
    "c = k.to_cartesian()\n",
    "x = icrs_to_horizons_ecliptic(c.x)\n",
    "v = icrs_to_horizons_ecliptic(c.v)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(x[:, 0], x[:, 1], x[:, 2])\n",
    "ax.set(aspect=\"equal\")\n",
    "\n",
    "# phi = Omegas * jnp.pi / 180\n",
    "# theta = incs * jnp.pi / 180\n",
    "# x = jnp.outer(jnp.sin(theta), jnp.cos(phi))\n",
    "# y = jnp.outer(jnp.sin(theta), jnp.sin(phi))\n",
    "# z = jnp.outer(jnp.cos(theta), jnp.ones_like(phi))\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# ax.scatter(x, y, z, alpha=0.5)\n",
    "# ax.set(aspect=\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.hstack([x, v])\n",
    "r = jax.vmap(p0.loglike)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = jax.vmap(p0.loglike)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-10947237.04957968, dtype=float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:29, 33.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "j = np.zeros(len(r))\n",
    "for i, c in tqdm(enumerate(x)):\n",
    "    j[i] = p0.loglike(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 0.08682409,  0.49240388, -0.8660254 ]], dtype=float64),\n",
       " Array([ 0.08682409,  0.49240388, -0.8660254 ], dtype=float64))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phi = p0._observations.ra[0]\n",
    "# theta = jnp.pi/2 - p0._observations.dec[0]\n",
    "\n",
    "i = 2\n",
    "j = 3\n",
    "phi = jnp.linspace(0, 2 * jnp.pi, 10)[i]\n",
    "theta = jnp.pi / 2 - jnp.linspace(-jnp.pi, jnp.pi, 10)[j]\n",
    "\n",
    "\n",
    "x = jnp.sin(theta) * jnp.cos(phi)\n",
    "y = jnp.sin(theta) * jnp.sin(phi)\n",
    "z = jnp.cos(theta)\n",
    "\n",
    "\n",
    "x_icrs = jnp.hstack([x, y, z])\n",
    "x = icrs_to_horizons_ecliptic(x_icrs)  # * 3.0\n",
    "\n",
    "\n",
    "# assume we're observing the thing at its highest excursion from the ecliptic:\n",
    "inc = jnp.array([jnp.abs(jnp.arcsin(x[2])) / jnp.linalg.norm(x) * 180 / jnp.pi])\n",
    "\n",
    "# its longitude of ascending node is the angle between the x-axis and the projection of the vector onto the xy-plane:\n",
    "varphi = (jnp.arctan2(x[1], x[0]) * 180 / jnp.pi) % 360\n",
    "Omega = jnp.array([varphi]) - 90 if x[2] > 0 else jnp.array([varphi]) + 90\n",
    "\n",
    "nu = jnp.array([90.0]) if x[2] > 0 else jnp.array([270.0])\n",
    "a = jnp.array([3.0])\n",
    "ecc = jnp.array([0.0])\n",
    "omega = jnp.array([0.0])\n",
    "\n",
    "k = KeplerianState(\n",
    "    semi=a, ecc=ecc, nu=nu, inc=inc, Omega=Omega, omega=omega, time=times[0]\n",
    ")\n",
    "c = k.to_cartesian()\n",
    "c.x / 3.0, x_icrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-2.68373692,  1.32528782,  0.20289981, -0.00432348, -0.00821017,\n",
       "       -0.00355955], dtype=float64)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_initial_guess(ra, dec):\n",
    "    phi = ra\n",
    "    theta = jnp.pi / 2 - dec\n",
    "\n",
    "    x = jnp.sin(theta) * jnp.cos(phi)\n",
    "    y = jnp.sin(theta) * jnp.sin(phi)\n",
    "    z = jnp.cos(theta)\n",
    "\n",
    "    x_icrs = jnp.hstack([x, y, z])\n",
    "    x = icrs_to_horizons_ecliptic(x_icrs)\n",
    "\n",
    "    # assume we're observing the thing at its highest excursion from the ecliptic:\n",
    "    inc = jnp.array([jnp.abs(jnp.arcsin(x[2])) / jnp.linalg.norm(x) * 180 / jnp.pi])\n",
    "\n",
    "    # its longitude of ascending node is the angle between the x-axis and the projection of the vector onto the xy-plane:\n",
    "    varphi = (jnp.arctan2(x[1], x[0]) * 180 / jnp.pi) % 360\n",
    "    Omega = jnp.array([varphi]) - 90 if x[2] > 0 else jnp.array([varphi]) + 90\n",
    "\n",
    "    nu = jnp.array([90.0]) if x[2] > 0 else jnp.array([270.0])\n",
    "    a = jnp.array([3.0])\n",
    "    ecc = jnp.array([0.0])\n",
    "    omega = jnp.array([0.0])\n",
    "\n",
    "    k = KeplerianState(\n",
    "        semi=a, ecc=ecc, nu=nu, inc=inc, Omega=Omega, omega=omega, time=times[0]\n",
    "    )\n",
    "    c = k.to_cartesian()\n",
    "\n",
    "    return jnp.concatenate([c.x.flatten(), c.v.flatten()])\n",
    "\n",
    "\n",
    "generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 267, initial cost 2.0803e+10, final cost 2.0829e+01, first-order optimality 2.81e+05.\n",
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.0829e+01                                    1.51e+03    \n",
      "       1             13         2.0829e+01      1.08e-05       2.01e-08       2.17e+03    \n",
      "`xtol` termination condition is satisfied.\n",
      "Function evaluations 13, initial cost 2.0829e+01, final cost 2.0829e+01, first-order optimality 2.17e+03.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-37.37021373, dtype=float64)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "\n",
    "def tmp(x):\n",
    "    r = p0.residuals(x)\n",
    "    return jnp.linalg.norm(r, axis=1)\n",
    "\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=x0,\n",
    "    verbose=2,\n",
    "    method=\"lm\",\n",
    ")\n",
    "\n",
    "result = least_squares(\n",
    "    fun=tmp,\n",
    "    x0=result.x,\n",
    "    verbose=2,\n",
    ")\n",
    "p0.loglike(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-2.68373692,  1.32528782,  0.20289981, -0.00432348, -0.00821017,\n",
       "        -0.00355955], dtype=float64),\n",
       " Array([-2.00571666,  1.77860805,  0.51974738], dtype=float64))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0, perturbed.x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 60350.47697017, -29348.02941075],\n",
       "        [ 60368.24178112, -29360.07765625],\n",
       "        [ 60386.01812898, -29372.11240986],\n",
       "        [ 60771.72808604, -29636.19788502],\n",
       "        [ 60789.75940909, -29648.39425987],\n",
       "        [ 60807.80023077, -29660.57633796],\n",
       "        [ 62072.27698179, -30521.56535972],\n",
       "        [ 62091.07246654, -30534.19213625],\n",
       "        [ 62109.87083723, -30546.80199541]], dtype=float64),\n",
       " Array([[-0.90962391,  0.69842047],\n",
       "        [-0.90984353,  0.69858176],\n",
       "        [-0.91006152,  0.69873977],\n",
       "        [-0.91479967,  0.70217721],\n",
       "        [-0.9150206 ,  0.70233721],\n",
       "        [-0.91524007,  0.70249453],\n",
       "        [-0.93037155,  0.71334351],\n",
       "        [-0.93059056,  0.71350114],\n",
       "        [-0.93081201,  0.71365595]], dtype=float64))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0.residuals(x0), p0.residuals(\n",
    "    jnp.concatenate([perturbed.x.flatten(), perturbed.v.flatten()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_32615/2677624643.py:3: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.08026D+10    |proj g|=  8.59172D+10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.87312D+09    |proj g|=  1.34391D+10\n",
      "\n",
      "At iterate    2    f=  5.63142D+09    |proj g|=  1.38511D+10\n",
      "\n",
      "At iterate    3    f=  1.72328D+09    |proj g|=  9.57667D+09\n",
      "\n",
      "At iterate    4    f=  2.10574D+08    |proj g|=  4.36231D+09\n",
      "\n",
      "At iterate    5    f=  6.43936D+06    |proj g|=  1.21037D+09\n",
      "\n",
      "At iterate    6    f=  5.98118D+05    |proj g|=  4.08164D+08\n",
      "\n",
      "At iterate    7    f=  3.49487D+04    |proj g|=  3.54526D+07\n",
      "\n",
      "At iterate    8    f=  2.50987D+03    |proj g|=  4.57711D+06\n",
      "\n",
      "At iterate    9    f=  2.08509D+03    |proj g|=  2.21285D+06\n",
      "\n",
      "At iterate   10    f=  2.07084D+03    |proj g|=  1.35289D+05\n",
      "\n",
      "At iterate   11    f=  2.07047D+03    |proj g|=  1.19648D+05\n",
      "\n",
      "At iterate   12    f=  2.07010D+03    |proj g|=  2.10923D+04\n",
      "\n",
      "At iterate   13    f=  2.07008D+03    |proj g|=  1.55474D+04\n",
      "\n",
      "At iterate   14    f=  2.07006D+03    |proj g|=  1.51741D+04\n",
      "\n",
      "At iterate   15    f=  2.06996D+03    |proj g|=  4.51527D+04\n",
      "\n",
      "At iterate   16    f=  2.06976D+03    |proj g|=  8.84213D+04\n",
      "\n",
      "At iterate   17    f=  2.06918D+03    |proj g|=  1.64690D+05\n",
      "\n",
      "At iterate   18    f=  2.06771D+03    |proj g|=  2.84107D+05\n",
      "\n",
      "At iterate   19    f=  2.06381D+03    |proj g|=  4.79257D+05\n",
      "\n",
      "At iterate   20    f=  2.05370D+03    |proj g|=  7.91539D+05\n",
      "\n",
      "At iterate   21    f=  2.02746D+03    |proj g|=  1.28926D+06\n",
      "\n",
      "At iterate   22    f=  1.96060D+03    |proj g|=  2.05884D+06\n",
      "\n",
      "At iterate   23    f=  1.79733D+03    |proj g|=  3.16348D+06\n",
      "\n",
      "At iterate   24    f=  1.44096D+03    |proj g|=  4.43769D+06\n",
      "\n",
      "At iterate   25    f=  8.52306D+02    |proj g|=  5.04203D+06\n",
      "\n",
      "At iterate   26    f=  2.95519D+02    |proj g|=  3.75197D+06\n",
      "\n",
      "At iterate   27    f=  5.45593D+01    |proj g|=  9.65256D+05\n",
      "\n",
      "At iterate   28    f=  2.22937D+01    |proj g|=  3.57105D+05\n",
      "\n",
      "At iterate   29    f=  2.17264D+01    |proj g|=  2.96716D+05\n",
      "\n",
      "At iterate   30    f=  2.16002D+01    |proj g|=  2.69197D+04\n",
      "\n",
      "At iterate   31    f=  2.15979D+01    |proj g|=  7.06181D+02\n",
      "\n",
      "At iterate   32    f=  2.15978D+01    |proj g|=  4.77760D+02\n",
      "\n",
      "At iterate   33    f=  2.15978D+01    |proj g|=  2.74173D+02\n",
      "\n",
      "At iterate   34    f=  2.15978D+01    |proj g|=  1.15808D+01\n",
      "\n",
      "At iterate   35    f=  2.15978D+01    |proj g|=  1.15833D+01\n",
      "\n",
      "At iterate   36    f=  2.15978D+01    |proj g|=  1.54316D+01\n",
      "\n",
      "At iterate   37    f=  2.15978D+01    |proj g|=  2.16900D+01\n",
      "\n",
      "At iterate   38    f=  2.15978D+01    |proj g|=  7.25578D+01\n",
      "\n",
      "At iterate   39    f=  2.15978D+01    |proj g|=  1.14733D+02\n",
      "\n",
      "At iterate   40    f=  2.15978D+01    |proj g|=  2.07047D+02\n",
      "\n",
      "At iterate   41    f=  2.15978D+01    |proj g|=  3.38652D+02\n",
      "\n",
      "At iterate   42    f=  2.15978D+01    |proj g|=  5.62889D+02\n",
      "\n",
      "At iterate   43    f=  2.15978D+01    |proj g|=  9.17398D+02\n",
      "\n",
      "At iterate   44    f=  2.15978D+01    |proj g|=  1.49366D+03\n",
      "\n",
      "At iterate   45    f=  2.15978D+01    |proj g|=  2.42218D+03\n",
      "\n",
      "At iterate   46    f=  2.15977D+01    |proj g|=  3.92222D+03\n",
      "\n",
      "At iterate   47    f=  2.15975D+01    |proj g|=  6.34786D+03\n",
      "\n",
      "At iterate   48    f=  2.15970D+01    |proj g|=  1.02691D+04\n",
      "\n",
      "At iterate   49    f=  2.15956D+01    |proj g|=  1.66084D+04\n",
      "\n",
      "At iterate   50    f=  2.15919D+01    |proj g|=  2.68573D+04\n",
      "\n",
      "At iterate   51    f=  2.15824D+01    |proj g|=  4.34159D+04\n",
      "\n",
      "At iterate   52    f=  2.15575D+01    |proj g|=  7.01218D+04\n",
      "\n",
      "At iterate   53    f=  2.14931D+01    |proj g|=  1.12944D+05\n",
      "\n",
      "At iterate   54    f=  2.13286D+01    |proj g|=  1.80334D+05\n",
      "\n",
      "At iterate   55    f=  2.09274D+01    |proj g|=  2.79451D+05\n",
      "\n",
      "At iterate   56    f=  2.00617D+01    |proj g|=  3.88293D+05\n",
      "\n",
      "At iterate   57    f=  1.86946D+01    |proj g|=  4.48843D+05\n",
      "\n",
      "At iterate   58    f=  1.70426D+01    |proj g|=  3.63513D+05\n",
      "\n",
      "At iterate   59    f=  1.67319D+01    |proj g|=  3.68765D+05\n",
      "\n",
      "At iterate   60    f=  1.65455D+01    |proj g|=  2.42421D+04\n",
      "\n",
      "At iterate   61    f=  1.65417D+01    |proj g|=  1.41932D+04\n",
      "\n",
      "At iterate   62    f=  1.65410D+01    |proj g|=  6.09246D+03\n",
      "\n",
      "At iterate   63    f=  1.65409D+01    |proj g|=  3.66181D+03\n",
      "\n",
      "At iterate   64    f=  1.65409D+01    |proj g|=  6.79236D+02\n",
      "\n",
      "At iterate   65    f=  1.65409D+01    |proj g|=  5.96880D+02\n",
      "\n",
      "At iterate   66    f=  1.65409D+01    |proj g|=  6.23824D+02\n",
      "\n",
      "At iterate   67    f=  1.65409D+01    |proj g|=  6.48640D+01\n",
      "\n",
      "At iterate   68    f=  1.65409D+01    |proj g|=  5.70842D+01\n",
      "\n",
      "At iterate   69    f=  1.65409D+01    |proj g|=  1.51113D+01\n",
      "\n",
      "At iterate   70    f=  1.65409D+01    |proj g|=  1.70456D+01\n",
      "\n",
      "At iterate   71    f=  1.65409D+01    |proj g|=  8.19826D+00\n",
      "\n",
      "At iterate   72    f=  1.65409D+01    |proj g|=  3.06289D-01\n",
      "\n",
      "At iterate   73    f=  1.65409D+01    |proj g|=  5.44969D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     73     85      1     0     0   5.450D-02   1.654D+01\n",
      "  F =   16.540893597687987     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "result = minimize(\n",
    "    fun=lambda x: -p0.loglike(x),\n",
    "    x0=x0,\n",
    "    jac=lambda x: -jax.grad(p0.loglike)(x),\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/qxz5chg95r53_2nlv9f86qhm0000gn/T/ipykernel_32615/2864640065.py:3: OptimizeWarning: Unknown solver options: line_search\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           30\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.08026D+10    |proj g|=  8.59169D+10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.79468D+09    |proj g|=  1.64232D+10\n",
      "\n",
      "At iterate    2    f=  2.49217D+09    |proj g|=  8.37510D+09\n",
      "\n",
      "At iterate    3    f=  1.49127D+08    |proj g|=  3.43455D+09\n",
      "\n",
      "At iterate    4    f=  3.86313D+07    |proj g|=  3.16180D+09\n",
      "\n",
      "At iterate    5    f=  6.95561D+06    |proj g|=  1.00851D+09\n",
      "\n",
      "At iterate    6    f=  2.78564D+04    |proj g|=  6.61831D+07\n",
      "\n",
      "At iterate    7    f=  9.26132D+03    |proj g|=  1.68038D+07\n",
      "\n",
      "At iterate    8    f=  6.74918D+03    |proj g|=  1.10810D+07\n",
      "\n",
      "At iterate    9    f=  4.78945D+03    |proj g|=  7.49951D+06\n",
      "\n",
      "At iterate   10    f=  3.72583D+03    |proj g|=  4.72473D+06\n",
      "\n",
      "At iterate   11    f=  3.56047D+03    |proj g|=  5.15348D+05\n",
      "\n",
      "At iterate   12    f=  3.55960D+03    |proj g|=  7.46747D+04\n",
      "\n",
      "At iterate   13    f=  3.55958D+03    |proj g|=  1.17105D+05\n",
      "\n",
      "At iterate   14    f=  3.55956D+03    |proj g|=  6.12807D+04\n",
      "\n",
      "At iterate   15    f=  3.55953D+03    |proj g|=  1.14185D+05\n",
      "\n",
      "At iterate   16    f=  3.55953D+03    |proj g|=  2.18137D+04\n",
      "\n",
      "At iterate   17    f=  3.55953D+03    |proj g|=  2.14381D+04\n",
      "\n",
      "At iterate   18    f=  3.55952D+03    |proj g|=  2.61977D+04\n",
      "\n",
      "At iterate   19    f=  3.55948D+03    |proj g|=  5.91543D+04\n",
      "\n",
      "At iterate   20    f=  3.55941D+03    |proj g|=  7.07542D+04\n",
      "\n",
      "At iterate   21    f=  3.55875D+03    |proj g|=  1.28844D+05\n",
      "\n",
      "At iterate   22    f=  3.55772D+03    |proj g|=  2.47628D+05\n",
      "\n",
      "At iterate   23    f=  3.55658D+03    |proj g|=  4.25565D+05\n",
      "\n",
      "At iterate   24    f=  3.55543D+03    |proj g|=  3.63503D+05\n",
      "\n",
      "At iterate   25    f=  3.55543D+03    |proj g|=  2.79430D+05\n",
      "\n",
      "At iterate   26    f=  3.55476D+03    |proj g|=  2.91142D+05\n",
      "\n",
      "At iterate   27    f=  3.55419D+03    |proj g|=  2.42803D+05\n",
      "\n",
      "At iterate   28    f=  3.55395D+03    |proj g|=  1.24703D+05\n",
      "\n",
      "At iterate   29    f=  3.55393D+03    |proj g|=  1.02369D+05\n",
      "\n",
      "At iterate   30    f=  3.55390D+03    |proj g|=  1.57473D+05\n",
      "\n",
      "At iterate   31    f=  3.55362D+03    |proj g|=  1.53739D+05\n",
      "\n",
      "At iterate   32    f=  3.55346D+03    |proj g|=  1.14800D+05\n",
      "\n",
      "At iterate   33    f=  3.55343D+03    |proj g|=  1.34013D+05\n",
      "\n",
      "At iterate   34    f=  3.55338D+03    |proj g|=  6.07272D+04\n",
      "\n",
      "At iterate   35    f=  3.55338D+03    |proj g|=  1.22618D+05\n",
      "\n",
      "At iterate   36    f=  3.55336D+03    |proj g|=  8.07065D+04\n",
      "\n",
      "At iterate   37    f=  3.55336D+03    |proj g|=  2.17412D+04\n",
      "\n",
      "At iterate   38    f=  3.55336D+03    |proj g|=  2.06002D+04\n",
      "\n",
      "At iterate   39    f=  3.55336D+03    |proj g|=  2.37179D+04\n",
      "\n",
      "At iterate   40    f=  3.55336D+03    |proj g|=  8.70714D+04\n",
      "\n",
      "At iterate   41    f=  3.55336D+03    |proj g|=  8.61711D+04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x0 \u001b[38;5;241m=\u001b[39m generate_initial_guess(p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mra[\u001b[38;5;241m0\u001b[39m], p0\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mdec[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mp0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# jac=lambda x: -jax.grad(p0.loglike)(x),\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max line search steps (default is 20)\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxcor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase memory storage (default is 10)\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Make function value convergence more lenient\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gradient convergence criterion\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max function evaluations\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase max iterations\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Step size for finite difference (if needed)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mline_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrong_wolfe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use strong Wolfe conditions\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:344\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:306\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:47\u001b[0m, in \u001b[0;36m_wrapper_grad.<locals>.wrapped1\u001b[0;34m(x, f0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped1\u001b[39m(x, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:519\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:592\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    590\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[1;32m    591\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:470\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    468\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 470\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/virtual_envs/jorbit/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:24\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user-provided objective function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return a scalar value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x0 = generate_initial_guess(p0._observations.ra[0], p0._observations.dec[0])\n",
    "\n",
    "result = minimize(\n",
    "    fun=lambda x: -p0.loglike(x),\n",
    "    x0=x0,\n",
    "    # jac=lambda x: -jax.grad(p0.loglike)(x),\n",
    "    method=\"L-BFGS-B\",\n",
    "    options={\n",
    "        \"disp\": True,\n",
    "        \"maxls\": 100,  # Increase max line search steps (default is 20)\n",
    "        \"maxcor\": 30,  # Increase memory storage (default is 10)\n",
    "        \"ftol\": 1e-12,  # Make function value convergence more lenient\n",
    "        \"gtol\": 1e-8,  # Gradient convergence criterion\n",
    "        \"maxfun\": 5000,  # Increase max function evaluations\n",
    "        \"maxiter\": 1000,  # Increase max iterations\n",
    "        \"eps\": 1e-10,  # Step size for finite difference (if needed)\n",
    "        \"line_search\": \"strong_wolfe\",  # Use strong Wolfe conditions\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jorbit",
   "language": "python",
   "name": "jorbit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
